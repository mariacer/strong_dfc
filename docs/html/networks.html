
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Networks &#8212; dfc 0.1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Utilities" href="utils.html" />
    <link rel="prev" title="Data" href="datahandlers.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="networks">
<span id="networks-reference-label"></span><h1><a class="toc-backref" href="#id1">Networks</a><a class="headerlink" href="#networks" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#networks" id="id1">Networks</a></p>
<ul>
<li><p><a class="reference internal" href="#general-notes" id="id2">General Notes</a></p></li>
<li><p><a class="reference internal" href="#module-networks.network_interface" id="id3">API</a></p>
<ul>
<li><p><a class="reference internal" href="#implementation-of-an-interface-to-be-used-for-all-network-implementations" id="id4">Implementation of an interface to be used for all network implementations</a></p></li>
<li><p><a class="reference internal" href="#implementation-of-a-pytorch-layer-to-be-used-within-networks" id="id5">Implementation of a pytorch layer to be used within networks</a></p></li>
<li><p><a class="reference internal" href="#implementation-of-a-simple-network-that-is-trained-with-backpropagation" id="id6">Implementation of a simple network that is trained with backpropagation</a></p></li>
<li><p><a class="reference internal" href="#implementation-of-a-network-for-direct-feedback-alingment" id="id7">Implementation of a network for Direct Feedback Alingment</a></p></li>
<li><p><a class="reference internal" href="#implementation-of-a-layer-for-direct-feedback-alingment" id="id8">Implementation of a layer for Direct Feedback Alingment</a></p></li>
<li><p><a class="reference internal" href="#implementation-of-a-network-for-deep-feedback-control" id="id9">Implementation of a network for Deep Feedback Control</a></p></li>
<li><p><a class="reference internal" href="#implementation-of-a-layer-for-deep-feedback-control" id="id10">Implementation of a layer for Deep Feedback Control</a></p></li>
<li><p><a class="reference internal" href="#implementation-of-a-network-for-deep-feedback-control-that-uses-a-single-phase" id="id11">Implementation of a network for Deep Feedback Control that uses a single phase</a></p></li>
<li><p><a class="reference internal" href="#adding-custom-functions-to-pytorch-s-autograd" id="id12">Adding custom functions to PyTorch’s autograd</a></p></li>
<li><p><a class="reference internal" href="#helper-functions-for-generating-different-networks" id="id13">Helper functions for generating different networks</a></p></li>
<li><p><a class="reference internal" href="#script-with-helper-functions-for-deep-feedback-control-computations" id="id14">Script with helper functions for Deep Feedback Control computations</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="general-notes">
<h2><a class="toc-backref" href="#id2">General Notes</a><a class="headerlink" href="#general-notes" title="Permalink to this headline">¶</a></h2>
<p>Implementation of different types of networks. The common network structure is specified in <a class="reference internal" href="#module-networks.network_interface" title="networks.network_interface"><code class="xref py py-mod docutils literal notranslate"><span class="pre">networks.network_interface</span></code></a>. This is an abstract method from which specific network types should inherit. It uses an abstract layer that is specified in <a class="reference internal" href="#module-networks.layer_interface" title="networks.layer_interface"><code class="xref py py-mod docutils literal notranslate"><span class="pre">networks.layer_interface</span></code></a>.</p>
<p>For example, a backpropagation network inherits from these two abstract classes and uses <code class="xref py py-mod docutils literal notranslate"><span class="pre">networks.bp_networks.bp_layer</span></code> within <code class="xref py py-mod docutils literal notranslate"><span class="pre">networks.bp_networks.bp_network</span></code>.</p>
<p>Important functions for performing credit assignment, such as classes for performing non-linear operations within layers with a specific forward and backward pass, are specified in <a class="reference internal" href="#module-networks.credit_assignment_functions" title="networks.credit_assignment_functions"><code class="xref py py-mod docutils literal notranslate"><span class="pre">networks.credit_assignment_functions</span></code></a>.</p>
</div>
<div class="section" id="module-networks.network_interface">
<span id="api"></span><h2><a class="toc-backref" href="#id3">API</a><a class="headerlink" href="#module-networks.network_interface" title="Permalink to this headline">¶</a></h2>
<div class="section" id="implementation-of-an-interface-to-be-used-for-all-network-implementations">
<h3><a class="toc-backref" href="#id4">Implementation of an interface to be used for all network implementations</a><a class="headerlink" href="#implementation-of-an-interface-to-be-used-for-all-network-implementations" title="Permalink to this headline">¶</a></h3>
<p>A simple network wrapper to be used as a blueprint for all other network
classes.</p>
<dl class="class">
<dt id="networks.network_interface.NetworkInterface">
<em class="property">class </em><code class="sig-prename descclassname">networks.network_interface.</code><code class="sig-name descname">NetworkInterface</code><span class="sig-paren">(</span><em class="sig-param">n_in</em>, <em class="sig-param">n_hidden</em>, <em class="sig-param">n_out</em>, <em class="sig-param">activation='relu'</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">initialization='orthogonal'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <a class="reference external" href="https://docs.python.org/3/library/abc.html#abc.ABC" title="(in Python v3.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></a></p>
<p>Implementation of an interface for networks.</p>
<p>The last layer is always set to linear. For classification tasks, a softmax
will be applied when computing the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_in</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – Number of inputs.</p></li>
<li><p><strong>n_hidden</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – A list of integers, each number denoting the size of a
hidden layer. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, there is no hidden layer.</p></li>
<li><p><strong>n_out</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – Number of outputs.</p></li>
<li><p><strong>activation</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The nonlinearity used in hidden layers.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, no nonlinearity will be applied.</p></li>
<li><p><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether layers may have bias terms.</p></li>
<li><p><strong>initialization</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The type of initialization to be applied.</p></li>
</ul>
</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="networks.network_interface.NetworkInterface.activation">
<em class="property">property </em><code class="sig-name descname">activation</code><a class="headerlink" href="#networks.network_interface.NetworkInterface.activation" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.network_interface.NetworkInterface.activation" title="networks.network_interface.NetworkInterface.activation"><code class="xref py py-attr docutils literal notranslate"><span class="pre">activation</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">targets=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the backward pass.</p>
<p>As this is simple backprop, no special computations are needed here,
autograd does it all.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The loss.</p></li>
<li><p><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The dataset targets. This will usually be
ignored, but will become useful for DFC with strong feedback.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.bias">
<em class="property">property </em><code class="sig-name descname">bias</code><a class="headerlink" href="#networks.network_interface.NetworkInterface.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.network_interface.NetworkInterface.bias" title="networks.network_interface.NetworkInterface.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.clone_params">
<code class="sig-name descname">clone_params</code><span class="sig-paren">(</span><em class="sig-param">params_type='params'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.clone_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.clone_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Clone the parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>params_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The type of params to be returned. For DFC
networks, this can be useful to distinguish between
“forward_params” and “backard_params”.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>With the same structure as <cite>self.params</cite> but cloned values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.contains_nans">
<code class="sig-name descname">contains_nans</code><span class="sig-paren">(</span><em class="sig-param">max_value=1000</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.contains_nans"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.contains_nans" title="Permalink to this definition">¶</a></dt>
<dd><p>Check whether the network parameters contain NaNs or large values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>max_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The maximum value above which parameters are
considered to be diverging.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Flag indicating whether the network contains a NaN.</dt><dd><p>Also returns True if some parameters are above 1000, which
indicates divergence.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)">bool</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.create_layers">
<code class="sig-name descname">create_layers</code><span class="sig-paren">(</span><em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.create_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.create_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Create layers.</p>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.depth">
<em class="property">property </em><code class="sig-name descname">depth</code><a class="headerlink" href="#networks.network_interface.NetworkInterface.depth" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.network_interface.NetworkInterface.depth" title="networks.network_interface.NetworkInterface.depth"><code class="xref py py-attr docutils literal notranslate"><span class="pre">depth</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the output <img class="math" src="_images/math/1b5e577d6216dca3af7d87aa122a0b9b360d6cb3.png" alt="y"/> of this network given the input
<img class="math" src="_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.png" alt="x"/>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The input to the network.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The output of the network.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.forward_params">
<em class="property">abstract </em><code class="sig-name descname">forward_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.forward_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.forward_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Access parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The list of forward parameters.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>params (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.get_forward_parameter_list">
<code class="sig-name descname">get_forward_parameter_list</code><span class="sig-paren">(</span><em class="sig-param">with_bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.get_forward_parameter_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.get_forward_parameter_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of forward parameters with or without biases.</p>
<p>Since <cite>forward_params</cite> might contain sublists, this transforms it into
a flat list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>with_bias</strong> (<em>boolean</em>) – Whether to include biases.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A flat list of parameters.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.get_max_grad">
<code class="sig-name descname">get_max_grad</code><span class="sig-paren">(</span><em class="sig-param">params=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.get_max_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.get_max_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the maximum gradient across the parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – The list of parameters across which to compute the
maximum. If <code class="docutils literal notranslate"><span class="pre">None</span></code> is provided, it is extracted from the
<cite>params</cite> attribute of the class.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The maximum gradient encountered.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)">float</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.get_vectorized_parameter_updates">
<code class="sig-name descname">get_vectorized_parameter_updates</code><span class="sig-paren">(</span><em class="sig-param">with_bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.get_vectorized_parameter_updates"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.get_vectorized_parameter_updates" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a vector with all the vectorized, concatenated parameter updates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether to include biases, if they exist.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The vectorized form.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)">torch.Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.initialization">
<em class="property">property </em><code class="sig-name descname">initialization</code><a class="headerlink" href="#networks.network_interface.NetworkInterface.initialization" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.network_interface.NetworkInterface.initialization" title="networks.network_interface.NetworkInterface.initialization"><code class="xref py py-attr docutils literal notranslate"><span class="pre">initialization</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.layer_class">
<em class="property">property </em><code class="sig-name descname">layer_class</code><a class="headerlink" href="#networks.network_interface.NetworkInterface.layer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the layer type to be used.</p>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.n_hidden">
<em class="property">property </em><code class="sig-name descname">n_hidden</code><a class="headerlink" href="#networks.network_interface.NetworkInterface.n_hidden" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.network_interface.NetworkInterface.n_hidden" title="networks.network_interface.NetworkInterface.n_hidden"><code class="xref py py-attr docutils literal notranslate"><span class="pre">n_hidden</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.n_in">
<em class="property">property </em><code class="sig-name descname">n_in</code><a class="headerlink" href="#networks.network_interface.NetworkInterface.n_in" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.network_interface.NetworkInterface.n_in" title="networks.network_interface.NetworkInterface.n_in"><code class="xref py py-attr docutils literal notranslate"><span class="pre">n_in</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.n_out">
<em class="property">property </em><code class="sig-name descname">n_out</code><a class="headerlink" href="#networks.network_interface.NetworkInterface.n_out" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.network_interface.NetworkInterface.n_out" title="networks.network_interface.NetworkInterface.n_out"><code class="xref py py-attr docutils literal notranslate"><span class="pre">n_out</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#networks.network_interface.NetworkInterface.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.params">
<em class="property">abstract </em><code class="sig-name descname">params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.params" title="Permalink to this definition">¶</a></dt>
<dd><p>Access parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><dl class="simple">
<dt>The list of parameters. It has length equal to the</dt><dd><p>number of layers. If a bias is being used, each element of the
list is a two-length list with first weights and then biases.
For DFC networks, it will also contain the feedback weights, and
thus will have longer length.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>params (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.save_logs">
<code class="sig-name descname">save_logs</code><span class="sig-paren">(</span><em class="sig-param">writer</em>, <em class="sig-param">step</em>, <em class="sig-param">log_weights='forward'</em>, <em class="sig-param">prefix=''</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.save_logs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.save_logs" title="Permalink to this definition">¶</a></dt>
<dd><p>Log the norm of the weights and the gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>writer</strong> – The tensorboard writer.</p></li>
<li><p><strong>step</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The writer iteration.</p></li>
<li><p><strong>log_weights</strong> (<em>True</em>) – Whether to log the forward weights.</p></li>
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The naming prefix.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.network_interface.NetworkInterface.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/network_interface.html#NetworkInterface.zero_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.network_interface.NetworkInterface.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize all the gradients of the network parameters to zero.</p>
<p>This affects both the forward and the backward parameters.</p>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-networks.layer_interface"></span><div class="section" id="implementation-of-a-pytorch-layer-to-be-used-within-networks">
<h3><a class="toc-backref" href="#id5">Implementation of a pytorch layer to be used within networks</a><a class="headerlink" href="#implementation-of-a-pytorch-layer-to-be-used-within-networks" title="Permalink to this headline">¶</a></h3>
<p>Layer module to be used within network classes.</p>
<dl class="class">
<dt id="networks.layer_interface.LayerInterface">
<em class="property">class </em><code class="sig-prename descclassname">networks.layer_interface.</code><code class="sig-name descname">LayerInterface</code><span class="sig-paren">(</span><em class="sig-param">in_features</em>, <em class="sig-param">out_features</em>, <em class="sig-param">last_layer_features=None</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">requires_grad=False</em>, <em class="sig-param">forward_activation='tanh'</em>, <em class="sig-param">initialization='orthogonal'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <a class="reference external" href="https://docs.python.org/3/library/abc.html#abc.ABC" title="(in Python v3.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></a></p>
<p>Implementation of an abstract layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The number of pre-neurons.</p></li>
<li><p><strong>out_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The number of post-neurons.</p></li>
<li><p><strong>last_layer_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The number of neurons in the last layer.
Only provided for direct feedback applications, else <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>bias</strong> (<em>boolean</em>) – Whether the layer has a bias or not.</p></li>
<li><p><strong>requires_grad</strong> (<em>boolean</em>) – Whether the parameters require a gradient.</p></li>
<li><p><strong>forward_activation</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The forward activation to be used.</p></li>
<li><p><strong>initialization</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The initialization to be used.</p></li>
</ul>
</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="networks.layer_interface.LayerInterface.activations">
<em class="property">property </em><code class="sig-name descname">activations</code><a class="headerlink" href="#networks.layer_interface.LayerInterface.activations" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.layer_interface.LayerInterface.activations" title="networks.layer_interface.LayerInterface.activations"><code class="xref py py-attr docutils literal notranslate"><span class="pre">activations</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.bias">
<em class="property">property </em><code class="sig-name descname">bias</code><a class="headerlink" href="#networks.layer_interface.LayerInterface.bias" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.layer_interface.LayerInterface.bias" title="networks.layer_interface.LayerInterface.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.compute_bp_activation_updates">
<code class="sig-name descname">compute_bp_activation_updates</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">retain_graph=False</em>, <em class="sig-param">linear=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.compute_bp_activation_updates"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.compute_bp_activation_updates" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the error backpropagation teaching signal for activations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <a class="reference internal" href="#networks.layer_interface.LayerInterface.compute_bp_update" title="networks.layer_interface.LayerInterface.compute_bp_update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_bp_update()</span></code></a>.</p></li>
<li><p><strong>linear</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating whether the update should be
computed for the linear activations instead of the nonlinear
activations.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A tensor containing the BP updates for the layer</dt><dd><p>activations for the current mini-batch.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)">torch.Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.compute_bp_update">
<code class="sig-name descname">compute_bp_update</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">retain_graph=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.compute_bp_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.compute_bp_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the error backpropagation update for the forward
parameters of this layer, based on the given loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The loss.</p></li>
<li><p><strong>retain_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – flag indicating whether the graph of the
network should be retained after computing the gradients or
jacobians. If the graph will not be used anymore for the current
minibatch afterwards, <cite>retain_graph</cite> should be <cite>False</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.compute_nullspace_relative_norm">
<code class="sig-name descname">compute_nullspace_relative_norm</code><span class="sig-paren">(</span><em class="sig-param">output_activation</em>, <em class="sig-param">retain_graph=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.compute_nullspace_relative_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.compute_nullspace_relative_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the norm of the components of the weight gradients that are
in the nullspace of the jacobian of the output with respect to weights,
relative to the norm of the weight gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_activation</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The outputs of the layer.</p></li>
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <a class="reference internal" href="#networks.layer_interface.LayerInterface.compute_bp_update" title="networks.layer_interface.LayerInterface.compute_bp_update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_bp_update()</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The relative norm.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)">torch.Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.compute_vectorized_jacobian">
<code class="sig-name descname">compute_vectorized_jacobian</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.compute_vectorized_jacobian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.compute_vectorized_jacobian" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the vectorized Jacobian of the forward activation function.</p>
<p>Compute vectorized Jacobian evaluated at the value <cite>x</cite>. The vectorized
Jacobian is the vector with the diagonal elements of the real Jacobian
as it is a diagonal matrix for element-wise functions. As <cite>x</cite> is a
minibatch, the output will also be a mini-batch of vectorized Jacobia
(thus a matrix).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The linear activations for the current mini-batch.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The Jacobian.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)">torch.Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the output of the layer.</p>
<p>This method applies first a linear mapping with the parameters
<code class="docutils literal notranslate"><span class="pre">weights</span></code> and <code class="docutils literal notranslate"><span class="pre">bias</span></code>, after which it applies the forward activation
function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – Mini-batch of size <cite>[B, in_features]</cite> with input
activations from the previous layer or input.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The mini-batch of output activations of the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.forward_activation">
<em class="property">property </em><code class="sig-name descname">forward_activation</code><a class="headerlink" href="#networks.layer_interface.LayerInterface.forward_activation" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.layer_interface.LayerInterface.forward_activation" title="networks.layer_interface.LayerInterface.forward_activation"><code class="xref py py-attr docutils literal notranslate"><span class="pre">forward_activation</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.forward_activation_function">
<code class="sig-name descname">forward_activation_function</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.forward_activation_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.forward_activation_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute element-wise forward activation based on activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The input.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The post-linearity activation.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)">torch.Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.get_forward_gradients">
<code class="sig-name descname">get_forward_gradients</code><span class="sig-paren">(</span><em class="sig-param">with_bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.get_forward_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.get_forward_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list containing the gradients of the forward parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>with_bias</strong> (<em>boolean</em>) – Whether the bias should be returned.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A list with the gradients. It has length of one if only</dt><dd><p>weights are returned, and length two if biases are returned.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.get_forward_parameters">
<code class="sig-name descname">get_forward_parameters</code><span class="sig-paren">(</span><em class="sig-param">with_bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.get_forward_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.get_forward_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list containing the forward parameters.</p>
<p>In previous versions this was also called <cite>get_forward_parameter_list</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>with_bias</strong> (<em>boolean</em>) – Whether the bias should be returned.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A list with the parameters. It has length of one if only</dt><dd><p>weights are returned, and length two if biases are returned.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">weights</em>, <em class="sig-param">bias=None</em>, <em class="sig-param">initialization='xavier'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the network parameters.</p>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#networks.layer_interface.LayerInterface.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.requires_grad">
<code class="sig-name descname">requires_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.requires_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.requires_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Set <cite>require_grad</cite> attribute of the activations to <cite>True</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.save_logs">
<code class="sig-name descname">save_logs</code><span class="sig-paren">(</span><em class="sig-param">writer</em>, <em class="sig-param">step</em>, <em class="sig-param">name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.save_logs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.save_logs" title="Permalink to this definition">¶</a></dt>
<dd><p>Save logs and plots of this layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>writer</strong> (<em>SummaryWriter</em>) – Summary writer.</p></li>
<li><p><strong>step</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The global step used for the x-axis of the plots.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The name of the layer.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.set_layer">
<code class="sig-name descname">set_layer</code><span class="sig-paren">(</span><em class="sig-param">in_features</em>, <em class="sig-param">out_features</em>, <em class="sig-param">use_bias=True</em>, <em class="sig-param">requires_grad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/layer_interface.html#LayerInterface.set_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.layer_interface.LayerInterface.set_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the network parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The number of pre-neurons.</p></li>
<li><p><strong>out_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The number of post-neurons.</p></li>
<li><p><strong>use_bias</strong> (<em>boolean</em>) – Whether a bias should be created.</p></li>
<li><p><strong>requires_grad</strong> (<em>boolean</em>) – Whether the gradient should be computed.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.use_bias">
<em class="property">property </em><code class="sig-name descname">use_bias</code><a class="headerlink" href="#networks.layer_interface.LayerInterface.use_bias" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.layer_interface.LayerInterface.use_bias" title="networks.layer_interface.LayerInterface.use_bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">use_bias</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.layer_interface.LayerInterface.weights">
<em class="property">property </em><code class="sig-name descname">weights</code><a class="headerlink" href="#networks.layer_interface.LayerInterface.weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.layer_interface.LayerInterface.weights" title="networks.layer_interface.LayerInterface.weights"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weights</span></code></a>.</p>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-networks.bp_network"></span><div class="section" id="implementation-of-a-simple-network-that-is-trained-with-backpropagation">
<h3><a class="toc-backref" href="#id6">Implementation of a simple network that is trained with backpropagation</a><a class="headerlink" href="#implementation-of-a-simple-network-that-is-trained-with-backpropagation" title="Permalink to this headline">¶</a></h3>
<p>A simple network that is prepared to be trained with backprop.</p>
<dl class="class">
<dt id="networks.bp_network.BPLayer">
<em class="property">class </em><code class="sig-prename descclassname">networks.bp_network.</code><code class="sig-name descname">BPLayer</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/bp_network.html#BPLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.bp_network.BPLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#networks.layer_interface.LayerInterface" title="networks.layer_interface.LayerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">networks.layer_interface.LayerInterface</span></code></a></p>
<p>Implementation of a backpropagation layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of class <code class="xref py py-class docutils literal notranslate"><span class="pre">layer_interface.LayerInterface</span></code>.</p>
</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="networks.bp_network.BPLayer.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#networks.bp_network.BPLayer.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="networks.bp_network.BPNetwork">
<em class="property">class </em><code class="sig-prename descclassname">networks.bp_network.</code><code class="sig-name descname">BPNetwork</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/bp_network.html#BPNetwork"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.bp_network.BPNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#networks.network_interface.NetworkInterface" title="networks.network_interface.NetworkInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">networks.network_interface.NetworkInterface</span></code></a></p>
<p>Implementation of a Multi-Layer Perceptron (MLP) trained with backprop.</p>
<p>This is a simple fully-connected network, that receives input vector
<img class="math" src="_images/math/5369e693370bbbd19fea43055b8f96596bff42a6.png" alt="\mathbf{x}"/> and outputs a vector <img class="math" src="_images/math/b04f62e21c4cd0ae0183708ab43bd04d6a0579f7.png" alt="\mathbf{y}"/> of real values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of class
<code class="xref py py-class docutils literal notranslate"><span class="pre">network_interface.NetworkInterface</span></code>.</p>
</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="networks.bp_network.BPNetwork.forward_params">
<em class="property">property </em><code class="sig-name descname">forward_params</code><a class="headerlink" href="#networks.bp_network.BPNetwork.forward_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Access forward parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The list of parameters.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>params (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.bp_network.BPNetwork.layer_class">
<em class="property">property </em><code class="sig-name descname">layer_class</code><a class="headerlink" href="#networks.bp_network.BPNetwork.layer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the layer type to be used.</p>
</dd></dl>

<dl class="method">
<dt id="networks.bp_network.BPNetwork.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/bp_network.html#BPNetwork.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.bp_network.BPNetwork.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a state into the network.</p>
<p>This function sets the forward and backward weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)"><em>dict</em></a>) – The state with forward and backward weights.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.bp_network.BPNetwork.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#networks.bp_network.BPNetwork.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="networks.bp_network.BPNetwork.params">
<em class="property">property </em><code class="sig-name descname">params</code><a class="headerlink" href="#networks.bp_network.BPNetwork.params" title="Permalink to this definition">¶</a></dt>
<dd><p>Access parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The list of parameters.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>params (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.bp_network.BPNetwork.state_dict">
<em class="property">property </em><code class="sig-name descname">state_dict</code><a class="headerlink" href="#networks.bp_network.BPNetwork.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing the current state of the network,
incliding forward and backward weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The forward and feedback weights.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)">dict</a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-networks.dfa_network"></span><div class="section" id="implementation-of-a-network-for-direct-feedback-alingment">
<h3><a class="toc-backref" href="#id7">Implementation of a network for Direct Feedback Alingment</a><a class="headerlink" href="#implementation-of-a-network-for-direct-feedback-alingment" title="Permalink to this headline">¶</a></h3>
<p>A network that is prepared to be trained with DFA.</p>
<dl class="class">
<dt id="networks.dfa_network.DFANetwork">
<em class="property">class </em><code class="sig-prename descclassname">networks.dfa_network.</code><code class="sig-name descname">DFANetwork</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfa_network.html#DFANetwork"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfa_network.DFANetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#networks.network_interface.NetworkInterface" title="networks.network_interface.NetworkInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">networks.network_interface.NetworkInterface</span></code></a></p>
<p>Implementation of a network for Direct Feedback Alignment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of class
<code class="xref py py-class docutils literal notranslate"><span class="pre">network_interface.NetworkInterface</span></code>.</p>
</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="networks.dfa_network.DFANetwork.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfa_network.html#DFANetwork.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfa_network.DFANetwork.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the output <img class="math" src="_images/math/1b5e577d6216dca3af7d87aa122a0b9b360d6cb3.png" alt="y"/> of this network given the input
<img class="math" src="_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.png" alt="x"/>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The input to the network.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The output of the network.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfa_network.DFANetwork.forward_params">
<em class="property">property </em><code class="sig-name descname">forward_params</code><a class="headerlink" href="#networks.dfa_network.DFANetwork.forward_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Access forward parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The list of parameters.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>params (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfa_network.DFANetwork.layer_class">
<em class="property">property </em><code class="sig-name descname">layer_class</code><a class="headerlink" href="#networks.dfa_network.DFANetwork.layer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the layer type to be used.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfa_network.DFANetwork.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfa_network.html#DFANetwork.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfa_network.DFANetwork.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a state into the network.</p>
<p>This function sets the forward and backward weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)"><em>dict</em></a>) – The state with forward and backward weights.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfa_network.DFANetwork.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#networks.dfa_network.DFANetwork.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="networks.dfa_network.DFANetwork.params">
<em class="property">property </em><code class="sig-name descname">params</code><a class="headerlink" href="#networks.dfa_network.DFANetwork.params" title="Permalink to this definition">¶</a></dt>
<dd><p>Access parameters.</p>
<p>Only feedforward weights are learned.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The list of parameters.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>params (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfa_network.DFANetwork.state_dict">
<em class="property">property </em><code class="sig-name descname">state_dict</code><a class="headerlink" href="#networks.dfa_network.DFANetwork.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing the current state of the network,
incliding forward and backward weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The forward and feedback weights.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)">dict</a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-networks.dfa_layer"></span><div class="section" id="implementation-of-a-layer-for-direct-feedback-alingment">
<h3><a class="toc-backref" href="#id8">Implementation of a layer for Direct Feedback Alingment</a><a class="headerlink" href="#implementation-of-a-layer-for-direct-feedback-alingment" title="Permalink to this headline">¶</a></h3>
<p>A layer that is prepared to be trained with DFA.</p>
<dl class="class">
<dt id="networks.dfa_layer.DFALayer">
<em class="property">class </em><code class="sig-prename descclassname">networks.dfa_layer.</code><code class="sig-name descname">DFALayer</code><span class="sig-paren">(</span><em class="sig-param">in_features</em>, <em class="sig-param">out_features</em>, <em class="sig-param">last_layer_features</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">requires_grad=False</em>, <em class="sig-param">forward_activation='tanh'</em>, <em class="sig-param">initialization='orthogonal'</em>, <em class="sig-param">initialization_fb=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfa_layer.html#DFALayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfa_layer.DFALayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#networks.layer_interface.LayerInterface" title="networks.layer_interface.LayerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">networks.layer_interface.LayerInterface</span></code></a></p>
<p>Implementation of a Direct Feedback Alignment layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of class <code class="xref py py-class docutils literal notranslate"><span class="pre">layer_interface.LayerInterface</span></code>.</p></li>
<li><p><strong>last_layer_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The size of the output layer.</p></li>
<li><p><strong>initialization_fb</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The initialization to use for the feedback
weights. If <cite>None</cite> is provided, the same initialization function
as for the forward weights will be used.</p></li>
</ul>
</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="networks.dfa_layer.DFALayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">grad_out</em>, <em class="sig-param">is_last_layer=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfa_layer.html#DFALayer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfa_layer.DFALayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the output of the layer.</p>
<p>This method applies first a linear mapping with the parameters
<code class="docutils literal notranslate"><span class="pre">weights</span></code> and <code class="docutils literal notranslate"><span class="pre">bias</span></code>, after which it applies the forward activation
function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – Mini-batch of size <cite>[B, in_features]</cite> with input
activations from the previous layer or input.</p></li>
<li><p><strong>grad_out</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – A tensor that will reference the gradient
of the output, such that it can then be overwritten during the
gradient computation of the last layer, and used to be
projected to earlier layers.</p></li>
<li><p><strong>is_last_layer</strong> (<em>boolean</em>) – Whether this is the last layer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The mini-batch of output activations of the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfa_layer.DFALayer.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#networks.dfa_layer.DFALayer.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="networks.dfa_layer.DFALayer.set_direct_feedback_layer">
<code class="sig-name descname">set_direct_feedback_layer</code><span class="sig-paren">(</span><em class="sig-param">last_features</em>, <em class="sig-param">out_features</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfa_layer.html#DFALayer.set_direct_feedback_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfa_layer.DFALayer.set_direct_feedback_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the network backward parameters.</p>
<p>This layer connects the output layer to a hidden layer. No biases are
used in direct feedback layers. These backward parameters have no
gradient as they are fixed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method
<code class="xref py py-meth docutils literal notranslate"><span class="pre">layer_interface.LayerInterface.set_layer()</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfa_layer.DFALayer.weights_backward">
<em class="property">property </em><code class="sig-name descname">weights_backward</code><a class="headerlink" href="#networks.dfa_layer.DFALayer.weights_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">_weights_backward</span></code>.</p>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-networks.dfc_network"></span><div class="section" id="implementation-of-a-network-for-deep-feedback-control">
<h3><a class="toc-backref" href="#id9">Implementation of a network for Deep Feedback Control</a><a class="headerlink" href="#implementation-of-a-network-for-deep-feedback-control" title="Permalink to this headline">¶</a></h3>
<p>A network that is prepared to be trained with DFC.</p>
<dl class="class">
<dt id="networks.dfc_network.DFCNetwork">
<em class="property">class </em><code class="sig-prename descclassname">networks.dfc_network.</code><code class="sig-name descname">DFCNetwork</code><span class="sig-paren">(</span><em class="sig-param">n_in</em>, <em class="sig-param">n_hidden</em>, <em class="sig-param">n_out</em>, <em class="sig-param">activation='relu'</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">initialization='xavier_normal'</em>, <em class="sig-param">cont_updates=False</em>, <em class="sig-param">initialization_fb='weight_product'</em>, <em class="sig-param">ndi=False</em>, <em class="sig-param">sigma=0.36</em>, <em class="sig-param">sigma_fb=0.01</em>, <em class="sig-param">sigma_output=0.36</em>, <em class="sig-param">sigma_output_fb=0.1</em>, <em class="sig-param">sigma_init=0.001</em>, <em class="sig-param">dt_di=0.02</em>, <em class="sig-param">dt_di_fb=0.001</em>, <em class="sig-param">alpha_di=0.001</em>, <em class="sig-param">alpha_di_fb=0.5</em>, <em class="sig-param">tmax_di=500</em>, <em class="sig-param">tmax_di_fb=10</em>, <em class="sig-param">k_p=2.0</em>, <em class="sig-param">k_p_fb=0.0</em>, <em class="sig-param">epsilon_di=0.3</em>, <em class="sig-param">time_constant_ratio=0.2</em>, <em class="sig-param">time_constant_ratio_fb=0.005</em>, <em class="sig-param">inst_transmission=False</em>, <em class="sig-param">inst_transmission_fb=False</em>, <em class="sig-param">apical_time_constant=-1</em>, <em class="sig-param">apical_time_constant_fb=None</em>, <em class="sig-param">inst_system_dynamics=False</em>, <em class="sig-param">inst_apical_dynamics=False</em>, <em class="sig-param">proactive_controller=False</em>, <em class="sig-param">noisy_dynamics=False</em>, <em class="sig-param">target_stepsize=0.01</em>, <em class="sig-param">tau_f=0.9</em>, <em class="sig-param">tau_noise=0.8</em>, <em class="sig-param">forward_requires_grad=False</em>, <em class="sig-param">include_non_converged_samples=True</em>, <em class="sig-param">compare_with_ndi=False</em>, <em class="sig-param">low_pass_filter_u=False</em>, <em class="sig-param">low_pass_filter_noise=False</em>, <em class="sig-param">use_jacobian_as_fb=False</em>, <em class="sig-param">save_ndi_updates=False</em>, <em class="sig-param">save_df=False</em>, <em class="sig-param">strong_feedback=False</em>, <em class="sig-param">compute_jacobian_at='full_trajectory'</em>, <em class="sig-param">freeze_fb_weights=False</em>, <em class="sig-param">scaling_fb_updates=False</em>, <em class="sig-param">learning_rule='nonlinear_difference'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#networks.network_interface.NetworkInterface" title="networks.network_interface.NetworkInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">networks.network_interface.NetworkInterface</span></code></a></p>
<p>Implementation of a network for Deep Feedback Control.</p>
<p>Note that forward and feedback weights are learned in two different phases
that we call the wake and sleep phases, respectively.</p>
<p>It contains the following important functions:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">forward</span></code>: compute the forward pass across all layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">backward</span></code>: compute the backward phase with controller on, and compute
the gradients of the forward weights.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">compute_feedback_gradients</span></code>: compute the gradients of feedback weights.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">controller</span></code>: a proportional integral controller.</p></li>
</ul>
<p>In this two-phase DFC setting, the following options exist for defining the
target activations. For forward weight learning, the target outputs are
either feedforward activations nudged towards lower loss (default) or set as
the actual supervised targets (if the option <code class="docutils literal notranslate"><span class="pre">strong_feedback</span></code> is active).
For feedback weight learning, the target outputs are either the feedforward
activations (default) or the supervised targets (if the option
<code class="docutils literal notranslate"><span class="pre">strong_feedback</span></code> is active).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of class
<code class="xref py py-class docutils literal notranslate"><span class="pre">network_interface.NetworkInterface</span></code>.</p></li>
<li><p><strong>initialization_fb</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The initialization for feedback weights.</p></li>
<li><p><strong>ndi</strong> (<em>boolean</em>) – Whether to compute the non-dynamical inversion, i.e. the
analytical solution at steady-state instead of simulating the
dynamics.</p></li>
<li><p><strong>cont_updates</strong> (<em>boolean</em>) – Whether the forward weights are updated at
steady-state or not. Feedback weights are always computed
continuously.</p></li>
<li><p><strong>sigma</strong> – Std of Gaussian noise to corrupt activations during controller
dynamics.</p></li>
<li><p><strong>sigma_output</strong> – Same as <cite>sigma</cite> but for output layer.</p></li>
<li><p><strong>sigma_fb</strong> – Same as “sigma” but for the sleep phase.</p></li>
<li><p><strong>sigma_output_fb</strong> – Same as <cite>sigma_fb</cite> but for output layer.</p></li>
<li><p><strong>sigma_init</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Std for assing Gaussian noise to feedback weight
initialization for “weight_product” inits.</p></li>
<li><p><strong>dt_di</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The timestep to be used in the differential equations.</p></li>
<li><p><strong>dt_di_fb</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Same as “dt_di” but for the sleep phase.</p></li>
<li><p><strong>alpha_di</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Leakage gain of the feedback controller.</p></li>
<li><p><strong>alpha_di_fb</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Same as “alpha” but for the sleep phase.</p></li>
<li><p><strong>tmax_di</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Maximum number of iterations (timesteps) performed in
the dynamical inversion of targets.</p></li>
<li><p><strong>tmax_di_fb</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Same as “tmax_di” but for the sleep phase.</p></li>
<li><p><strong>epsilon_di</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Constant to check for convergence.</p></li>
<li><p><strong>k_p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The gain factor of the proportional control.</p></li>
<li><p><strong>k_p_fb</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Same as “k_p” but for the sleep phase.</p></li>
<li><p><strong>time_constant_ratio</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Ratio of the time constant of the voltage
dynamics w.r.t. the controller dynamics.</p></li>
<li><p><strong>time_constant_ratio_fb</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Same as “time_constant_ratio” but for
the sleep phase.</p></li>
<li><p><strong>inst_transmission</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether to assume an instantaneous
transmission between layers.</p></li>
<li><p><strong>inst_transmission_fb</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Same as “inst_transmission” but for the
sleep phase.</p></li>
<li><p><strong>apical_time_constant</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Time constant of the apical compartment.</p></li>
<li><p><strong>apical_time_constant_fb</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Same as “apical_time_constant” but for
the sleep phase.</p></li>
<li><p><strong>inst_system_dynamics</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether the dynamics of the somatic
compartments, should be approximated by their instantaneous
counterparts.</p></li>
<li><p><strong>inst_apical_dynamics</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether the dyamics of the apical
compartiment should be instantneous.</p></li>
<li><p><strong>proactive_controller</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether to use the teaching signal of the
next time step for simulating the controller dynamics.</p></li>
<li><p><strong>noisy_dynamics</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether dynamics should be corrupted with noise
with std “sigma” or “sigma_fb”.</p></li>
<li><p><strong>target_stepsize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Step size for computing the output target based
on the output gradient.</p></li>
<li><p><strong>tau_f</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The time constant for filtering the dynamics and the
control signal.</p></li>
<li><p><strong>tau_noise</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The time constant to filter the noise.</p></li>
<li><p><strong>forward_requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether the forward pass requires autograd
to compute gradients.</p></li>
<li><p><strong>include_non_converged_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether samples that have not
converged should be excluded.</p></li>
<li><p><strong>compare_with_ndi</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether the dynamical inversion results should
be compared to the analytical solution. Should only ever be active
if <cite>ndi</cite> is <cite>False</cite>.</p></li>
<li><p><strong>low_pass_filter_u</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether the control signal should be low-pass
filtered.</p></li>
<li><p><strong>low_pass_filter_noise</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether the noise should be low-pass
filtered. Only relevant if <cite>noisy_dynamics==True</cite>.</p></li>
<li><p><strong>use_jacobian_as_fb</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether to use the Jacobian for the
feedback weights.</p></li>
<li><p><strong>save_ndi_updates</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether angle with the analytical updates
should be computed. Causes a minor increase in computational load.</p></li>
<li><p><strong>save_df</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether angles should be stored in a dataframe.</p></li>
<li><p><strong>strong_feedback</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether the feedback should be strong. In this
case, the outputs are not simply nudged but clamped to the desired
values. In this setting, the linearization becomes very inaccurate,
so it does not make sense to use an analytical solution, so
<cite>ndi</cite> should be <cite>False</cite>.</p></li>
<li><p><strong>compute_jacobian_at</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – How to compute the Jacobian.</p></li>
<li><p><strong>scaling_fb_updates</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether to scale the feedback updates
differently for different layers.</p></li>
<li><p><strong>learning_rule</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The type of learning rule to use for the forward
weights.</p></li>
</ul>
</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.alpha_di">
<em class="property">property </em><code class="sig-name descname">alpha_di</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.alpha_di" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.alpha_di" title="networks.dfc_network.DFCNetwork.alpha_di"><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha_di</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.alpha_di_fb">
<em class="property">property </em><code class="sig-name descname">alpha_di_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.alpha_di_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.alpha_di_fb" title="networks.dfc_network.DFCNetwork.alpha_di_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha_di_fb</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.apical_time_constant">
<em class="property">property </em><code class="sig-name descname">apical_time_constant</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.apical_time_constant" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.apical_time_constant" title="networks.dfc_network.DFCNetwork.apical_time_constant"><code class="xref py py-attr docutils literal notranslate"><span class="pre">apical_time_constant</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.apical_time_constant_fb">
<em class="property">property </em><code class="sig-name descname">apical_time_constant_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.apical_time_constant_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.apical_time_constant_fb" title="networks.dfc_network.DFCNetwork.apical_time_constant_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">apical_time_constant_fb</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">targets=None</em>, <em class="sig-param">return_for_fb=False</em>, <em class="sig-param">verbose=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the feedback phase of the network.</p>
<p>In this phase, the network is pushed to the output target by the
controller. Compute the update of the forward weights of the network
accordingly and save it in <code class="docutils literal notranslate"><span class="pre">self.layers[i].weights.grad</span></code> for each
layer <code class="docutils literal notranslate"><span class="pre">i</span></code>.</p>
<p>Note: <code class="docutils literal notranslate"><span class="pre">backward</span></code> is implemented at the network-level, as it performs
simultaneous inversion and control of all targets</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – Mean output loss for current mini-batch.</p></li>
<li><p><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The dataset targets. This will usually be
ignored, as the targets will be taken to be the activations
nudged towards lower loss, unless we use strong feedback.</p></li>
<li><p><strong>return_for_fb</strong> (<em>boolean</em>) – Whether to return the control signal and
the aplical voltages. This is useful when implementing DFC in
a single phase, as these values will be needed to compute the
feedback gradients.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether to display warnings.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(Optionally) the controller signal.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)">torch.Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.check_convergence">
<code class="sig-name descname">check_convergence</code><span class="sig-paren">(</span><em class="sig-param">r</em>, <em class="sig-param">r_feedforward</em>, <em class="sig-param">output_target</em>, <em class="sig-param">u</em>, <em class="sig-param">sample_error</em>, <em class="sig-param">batch_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.check_convergence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.check_convergence" title="Permalink to this definition">¶</a></dt>
<dd><p>Check whether the dynamics of the network have converged.</p>
<p>This function computes whether individual samples have converged to a
small output error. Like this, the ones that have not converged can be
exclulded from the mini-batch update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>r</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The target activations across layers.</p></li>
<li><p><strong>r_feedforward</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The forward activations across layers.</p></li>
<li><p><strong>output_target</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The output target.</p></li>
<li><p><strong>u</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The control signal.</p></li>
<li><p><strong>sample_error</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The L2 norm of the error <img class="math" src="_images/math/76887f07dbc08b29e8296f7b5da8177a022303b3.png" alt="e(t)"/>
at each timestep, computed by the controller.</p></li>
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The batch-size.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Tuple containing:</p>
<ul class="simple">
<li><p><strong>converged</strong>: List indicating if individual samples converged.</p></li>
<li><p><strong>diverged</strong>: List indicating if individual samples diverged.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(…)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.compute_H_update">
<code class="sig-name descname">compute_H_update</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">u</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.compute_H_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.compute_H_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the negative weight updates from the <img class="math" src="_images/math/f6debfa5617fe793770015b967c5d5ab70ad9182.png" alt="\mathcal{H}"/>
loss.</p>
<p>This computes the gradients w.r.t. the post-nonlinearity activations
using the full Jacobian of the network:</p>
<div class="math">
<p><img src="_images/math/cad3924e91985e2770e93b01172d7a655bbdefb6.png" alt="\frac{1}{2} \frac{d \lVert Q\mathbf{u}_{ss} \rVert^2_2}{d \
    \bm{\theta}} = -\mathbf{u}_{ss}^{T}Q^{T}Q(J_{ss}Q + \
    \alpha I)^{-1}J_{ss}R_{ss}^{T}"/></p>
</div><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_bp_update()</span></code>.</p></li>
<li><p><strong>u</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The controller signal.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Tuple containing:</p>
<ul class="simple">
<li><p><strong>lu_updates_W</strong> (list): The weight updates for each layer
according to loss <img class="math" src="_images/math/f6debfa5617fe793770015b967c5d5ab70ad9182.png" alt="\mathcal{H}"/>.</p></li>
<li><p><strong>lu_updates_b</strong> (list): The bias updates for each layer
according to loss <img class="math" src="_images/math/f6debfa5617fe793770015b967c5d5ab70ad9182.png" alt="\mathcal{H}"/>. <code class="docutils literal notranslate"><span class="pre">None</span></code> if no biases
exist.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(…)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.compute_bp_angles">
<code class="sig-name descname">compute_bp_angles</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">i</em>, <em class="sig-param">retain_graph=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.compute_bp_angles"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.compute_bp_angles" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the angles of the current forward parameter updates of layer
<cite>i</cite> with the backprop update for those parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Output loss of the network.</p></li>
<li><p><strong>i</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – Layer index.</p></li>
<li><p><strong>retain_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – flag indicating whether the graph of the
network should be retained after computing the gradients or
jacobians. If the graph will not be used anymore for the current
minibatch afterwards, retain_graph should be False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Tuple containing:</p>
<ul class="simple">
<li><p><strong>weights_angle</strong>: The angle in degrees between the updates for
the forward weights.</p></li>
<li><p><strong>bias_angle</strong>: (Optionally) the angle in degrees for the bias.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(…)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.compute_condition_two">
<code class="sig-name descname">compute_condition_two</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.compute_condition_two"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.compute_condition_two" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the Gauss-Newton condition on the feedback weights.</p>
<div class="math">
<p><img src="_images/math/705d0378cdd73ae6fe95aece9b98e67d6182c929.png" alt="\frac{\|\tilde{J}_2\|_F}{\|\tilde{J}\|_F}"/></p>
</div><p>to keep track whether condition 2 is (approximately) satisfied.
If the minibatch size is bigger than 1, the mean over the minibatch
is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The condition value.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.compute_error">
<code class="sig-name descname">compute_error</code><span class="sig-paren">(</span><em class="sig-param">output_target</em>, <em class="sig-param">r</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.compute_error"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.compute_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the error <img class="math" src="_images/math/912d7183c64b672df9f23d95b376cc519009af6c.png" alt="\mathbf{e}(t)"/> in the predictions.</p>
<p>By default this error is computed as in the DFC paper according to:</p>
<div class="math">
<p><img src="_images/math/d2d34c270410aacee74a261fa7f4dcb2b33fb338.png" alt="\mathbf{e}(t) = \mathbf{r}_L^* - \mathbf{r}_L(t)"/></p>
</div><p>For a mean-squared error (MSE) loss
<img class="math" src="_images/math/6981c56044111d90b757638b5b3c0ffff9baf526.png" alt="\mathcal{L} = \frac{1}{2}\lVert {\mathbf{r}_L^* - \
\mathbf{r}_L(t)} \rVert_{2}^{2}"/>, this can be seen as the gradient of
the loss with respect to the output activations <img class="math" src="_images/math/d6d71463c8d21494e564279d8c51145cd3fe60bd.png" alt="\mathbf{r}_L(t)"/>.
This notion can be generalized to other losses, and we can instead
write the error as:</p>
<div class="math">
<p><img src="_images/math/74413d8dad55a13e0b65770ae6002039bb90bc7b.png" alt="\mathbf{e}(t) = - \frac{\partial \mathcal{L}}{\partial \
    \mathbf{r}_L} \biggr\rvert^T_{\mathbf{r}_L=\mathbf{r}_L(t)}"/></p>
</div><p>In this function we hard-code the solution of this equation for the
MSE loss mentioned above as well as for the cross-entropy loss. Which
one of these is used will be determined by the attribute
<code class="docutils literal notranslate"><span class="pre">loss_function_name</span></code>, which by default is the MSE loss.</p>
<p>So for cross-entropy loss, we return the following error:</p>
<div class="math">
<p><img src="_images/math/5d806c4bcd004b22174f87511d1261ba6de5c60a.png" alt="\mathbf{e}(t) = \mathbf{r}_L^* - \text{softmax}(\mathbf{r}_L(t))"/></p>
</div><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_target</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The desired output
<img class="math" src="_images/math/410680b0f6dbbe715405476fb295cc638abbb612.png" alt="\mathbf{r}_L^*"/>.</p></li>
<li><p><strong>r</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The current output <img class="math" src="_images/math/d6d71463c8d21494e564279d8c51145cd3fe60bd.png" alt="\mathbf{r}_L(t)"/>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The error <img class="math" src="_images/math/912d7183c64b672df9f23d95b376cc519009af6c.png" alt="\mathbf{e}(t)"/>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)">torch.Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.compute_feedback_gradients">
<code class="sig-name descname">compute_feedback_gradients</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">targets</em>, <em class="sig-param">init=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.compute_feedback_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.compute_feedback_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the gradients of the feedback weights for each layer.</p>
<p>The updates are computed according to the following update rule:</p>
<div class="math">
<p><img src="_images/math/8b6e3964d292dc60278c503977f4c4bb6f0e6aa0.png" alt="\frac{d Q_i}{dt} = - k \frac{1}{\sigma^2} \
    \mathbf{v}^{\text{fb}}_i(t) \mathbf{u}(t)^T - \beta Q_i"/></p>
</div><p>where the output target is equal to the feedforward output of the
network (i.e. without feedback) and where noise is applied to the
network dynamics. Hence, the controller will try to ‘counter’ the
noisy dynamics, such that the output is equal to the unnoisy output.</p>
<p>The scaling <img class="math" src="_images/math/9630132210b904754c9ab272b61cb527d12263ca.png" alt="k"/> is either 1 or, if the option
<code class="docutils literal notranslate"><span class="pre">scaling_fb_updates</span></code> is active,
<img class="math" src="_images/math/cdb12961fec6c53b69e851112949ac43b5760357.png" alt="(1 + \frac{\tau_v}{\tau_{\epsilon}})^{L-i}"/>.</p>
<p>No inputs are required to this function since all activations are
expected to be saved within the object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The loss.</p></li>
<li><p><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The dataset targets. This will usually be
ignored, as the targets will be taken to be the activations
nudged towards lower loss, unless we use strong feedback.</p></li>
<li><p><strong>init</strong> (<em>boolean</em>) – Indicates that this is a pre-train of the weights.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.compute_full_jacobian">
<code class="sig-name descname">compute_full_jacobian</code><span class="sig-paren">(</span><em class="sig-param">linear=True</em>, <em class="sig-param">noisy_dynamics=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.compute_full_jacobian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.compute_full_jacobian" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the Jacobian of the network output.</p>
<p>Compute the Jacobian of the network output (post-nonlinearity)
with respect to either the concatenated pre-nonlinearity activations of
all layers (including the output layer!)
(i.e. <code class="docutils literal notranslate"><span class="pre">self.layers[i].linear_activations</span></code>) if <code class="docutils literal notranslate"><span class="pre">linear=True</span></code> or the
concatenated post-nonlinearity activations of all layers if
<code class="docutils literal notranslate"><span class="pre">linear=False</span></code> (i.e. <code class="docutils literal notranslate"><span class="pre">self.layers[i].activations</span></code>).</p>
<p>If there is noise being used in the dynamics, then a low-passed version
of the activations will be used to compute the Jacobian.</p>
<p>Note that this implementation does not use autograd.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>linear</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating whether the Jacobian with respect to
pre-nonlinearity activations (<code class="docutils literal notranslate"><span class="pre">linear=True</span></code>) should be taken
or with respect to the post-nonlinearity activations
(<code class="docutils literal notranslate"><span class="pre">linear=False</span></code>).</p></li>
<li><p><strong>noisy_dynamics</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether the dynamics are noisy.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A <img class="math" src="_images/math/a4820c21798c9244d5b44960f2281211eaa84d60.png" alt="B \times n_L \times \sum_{l=1}^L n_l"/></dt><dd><p>dimensional tensor, with <img class="math" src="_images/math/4bc3e94a67870b41b7c20179693e889251e2c136.png" alt="B"/> the minibatch size and
<img class="math" src="_images/math/85e302ee545da3eaa4eb692462a3a77da256a71d.png" alt="n_l"/> the dimension of layer <img class="math" src="_images/math/470aa65888a2971c9346e573f12b37ea406b8ec9.png" alt="l"/>,
containing the Jacobian of the network output w.r.t. the
concatenated activations (pre or post-nonlinearity) of all
layers, for each minibatch sample.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)">torch.Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.compute_jacobian_at">
<em class="property">property </em><code class="sig-name descname">compute_jacobian_at</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.compute_jacobian_at" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.compute_jacobian_at" title="networks.dfc_network.DFCNetwork.compute_jacobian_at"><code class="xref py py-attr docutils literal notranslate"><span class="pre">compute_jacobian_at</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param">output_target</em>, <em class="sig-param">r</em>, <em class="sig-param">axis=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss in the predictions.</p>
<p>This function is mostly used to check for convergence.
By default this error is computed as in the DFC paper for each sample
according to:</p>
<div class="math">
<p><img src="_images/math/613afac293204da48aadb8e2b26d3c9e2027e015.png" alt="\mathcal{L} = \frac{1}{2}\ \lVert {\mathbf{r}_L^* - \
    \mathbf{r}_L(t)} \rVert_{2}^{2}"/></p>
</div><p>However, if <code class="docutils literal notranslate"><span class="pre">loss_function_name==cross_entropy</span></code> we compute the
following:</p>
<div class="math">
<p><img src="_images/math/9e1862529dd378fc829912ebd3621e80dfa1cf67.png" alt="\mathcal{L} = - (\mathbf{r}_L^* * \log \
    \text{softmax}(\mathbf{r}_L(t))"/></p>
</div><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_target</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The desired output
<img class="math" src="_images/math/410680b0f6dbbe715405476fb295cc638abbb612.png" alt="\mathbf{r}_L^*"/>.</p></li>
<li><p><strong>r</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The current output <img class="math" src="_images/math/d6d71463c8d21494e564279d8c51145cd3fe60bd.png" alt="\mathbf{r}_L(t)"/>.</p></li>
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The axis across which to compute the norm.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of loss values in the mini-batch.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)">torch.Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.compute_output_target">
<code class="sig-name descname">compute_output_target</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">targets=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.compute_output_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.compute_output_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the output target.</p>
<p>The target can be computed in one of two ways:</p>
<ul class="simple">
<li><p>Nudged activations towards lower loss:</p></li>
</ul>
<div class="math">
<p><img src="_images/math/689f49dbd56feecaddbb5b1f82f59f74917beb61.png" alt="\mathbf{r}_L^* = \mathbf{r}_L^- - \lambda \frac{\partial \
    \mathcal{L}}{\partial \mathbf{r}_L} \
    \bigg\rvert^T_{\mathbf{r}_L = \mathbf{r}_L^-}"/></p>
</div><ul class="simple">
<li><p>As the actual targets (if <code class="docutils literal notranslate"><span class="pre">strong_feedback==True</span></code>)</p></li>
</ul>
<div class="math">
<p><img src="_images/math/c7c03b522618e9dd8e5e8d0e8285e14cd8335d8b.png" alt="\mathbf{r}_L^* = \mathbf{r}^\text{true}"/></p>
</div><p>We assume the loss is averaged across the mini-batch, so here we need
to multiply by the batch size to use the total loss over the mini-batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – Mean output loss for current mini-batch.</p></li>
<li><p><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The dataset targets. This will usually be
ignored, as the targets will be taken to be the activations
nudged towards lower loss, unless we use strong feedback.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Mini-batch of output targets</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)">torch.Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.compute_ratio_ff_fb">
<code class="sig-name descname">compute_ratio_ff_fb</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">u</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.compute_ratio_ff_fb"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.compute_ratio_ff_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the ratio of the current feedforward and feedback stimulus.</p>
<p>It is computed as:</p>
<div class="math">
<p><img src="_images/math/262d3d6199700e680464c7433e6b390ef27348b3.png" alt="\frac{||Q_{i}\mathbf{u}(t)||}{||W_{i}\mathbf{r}_{i-1}(t)||}"/></p>
</div><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – Output loss of the network.</p></li>
<li><p><strong>u</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – Controller signal.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list with the ratios for each layer.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.cont_updates">
<em class="property">property </em><code class="sig-name descname">cont_updates</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.cont_updates" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.cont_updates" title="networks.dfc_network.DFCNetwork.cont_updates"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cont_updates</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.controller">
<code class="sig-name descname">controller</code><span class="sig-paren">(</span><em class="sig-param">output_target</em>, <em class="sig-param">alpha</em>, <em class="sig-param">dt</em>, <em class="sig-param">tmax</em>, <em class="sig-param">k_p=0.0</em>, <em class="sig-param">noisy_dynamics=False</em>, <em class="sig-param">inst_transmission=False</em>, <em class="sig-param">time_constant_ratio=1.0</em>, <em class="sig-param">apical_time_constant=-1</em>, <em class="sig-param">proactive_controller=False</em>, <em class="sig-param">sigma=0.01</em>, <em class="sig-param">sigma_output=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.controller"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.controller" title="Permalink to this definition">¶</a></dt>
<dd><p>Simulate the feedback control loop for several timesteps.</p>
<p>The following continuous time ODEs are simulated with time interval
<code class="docutils literal notranslate"><span class="pre">dt</span></code>. The following equation is used for the voltage:</p>
<div class="math">
<p><img src="_images/math/1edeb43d3862199210f5c5a5c6439948406998fd.png" alt="\frac{\tau_v}{\tau_u}\frac{d \mathbf{v}_i(t)}{dt} = \
    -\mathbf{v}_i(t) + W_i \mathbf{r}_{i-1}(t) + b_i + \
    Q_i \mathbf{u}(t)"/></p>
</div><p>And the following for the control signal:</p>
<div class="math">
<p><img src="_images/math/47c1d58cacc30cf02935703e94c3c0247e0585a5.png" alt="\mathbf{u}(t) = \mathbf{u}^{\text{int}}(t) + k \mathbf{e}(t)"/></p>
</div><div class="math">
<p><img src="_images/math/c080b422905d5a6ac4186a0bf2b52ad16939413f.png" alt="\tau_u \frac{d \mathbf{u}^{\text{int}}(t)}{dt} = \mathbf{e}(t) - \
    \alpha \mathbf{u}^{\text{int}}(t)"/></p>
</div><p>Note that we use a ratio <img class="math" src="_images/math/907074e936f2c97483ebb34e99ebe5544d6bc331.png" alt="\frac{\tau_v}{\tau_u}"/> instead of two
separate time constants for <img class="math" src="_images/math/c605e79d60f7b21de44f33ea934c70ef6cc1541d.png" alt="\mathbf{v}"/> and <img class="math" src="_images/math/352f74d71ad9144677bed76f6a9deb39593f2916.png" alt="\mathbf{u}"/>,
as a scaling of both time constants can be absorbed in the simulation
timestep <code class="docutils literal notranslate"><span class="pre">dt</span></code>.
IMPORTANT: <code class="docutils literal notranslate"><span class="pre">time_constant_ratio</span></code> should never be taken smaller than
<code class="docutils literal notranslate"><span class="pre">dt</span></code>, as then the forward Euler method will become unstable by
default (the simulation steps will start to ‘overshoot’).</p>
<p>If <code class="docutils literal notranslate"><span class="pre">inst_transmission=False</span></code>, the forward Euler method is used to
simulate the differential equation. If <code class="docutils literal notranslate"><span class="pre">inst_transmission=True</span></code>, a
slight modification is made to the forward Euler method, assuming that
we have instant transmission from one layer to the next: the basal
voltage of layer <code class="docutils literal notranslate"><span class="pre">i</span></code> at timestep <code class="docutils literal notranslate"><span class="pre">t</span></code> will already be based on the
forward propagation of the somatic voltage of layer <code class="docutils literal notranslate"><span class="pre">i-1</span></code> at timestep
<code class="docutils literal notranslate"><span class="pre">t</span></code>, hence including the feedback of layer <code class="docutils literal notranslate"><span class="pre">i-1</span></code> at timestep <code class="docutils literal notranslate"><span class="pre">t</span></code>.
It is recommended to put <code class="docutils literal notranslate"><span class="pre">inst_transmission=True</span></code> when the
<code class="docutils literal notranslate"><span class="pre">time_constant_ratio</span></code> is approaching <code class="docutils literal notranslate"><span class="pre">dt</span></code>, as then we are
approaching the limit of instantaneous system dynamics in the simulation
where <code class="docutils literal notranslate"><span class="pre">inst_transmission</span></code> is always used (See below).</p>
<p>If <code class="docutils literal notranslate"><span class="pre">inst_system_dynamics=True</span></code>, we assume that the time constant of
the system (i.e. the network) is much smaller than that of the
controller and we approximate this by replacing the dynamical equations
for <img class="math" src="_images/math/9db50f96634681eef80940fdd247ba6a579572e9.png" alt="\mathbf{v}_i"/> by their instantaneous equivalents:</p>
<div class="math">
<p><img src="_images/math/ae2036aac3294e904d4f1e0849f1f8b7aed26535.png" alt="\mathbf{v}_i(t) = W_i \mathbf{r}_{i-1}(t) + b_i + Q_i \mathbf{u}(t)"/></p>
</div><p>Note that <code class="docutils literal notranslate"><span class="pre">inst_transmission</span></code> will always be put on <code class="docutils literal notranslate"><span class="pre">True</span></code>
(overridden) in combination with <code class="docutils literal notranslate"><span class="pre">inst_system_dynamics</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">proactive_controller=True</span></code>, the control input <code class="docutils literal notranslate"><span class="pre">u[k+1]</span></code> will be
used to compute the apical voltages <code class="docutils literal notranslate"><span class="pre">v^\text{fb}[k+1]</span></code>, instead of the
control input <code class="docutils literal notranslate"><span class="pre">u[k]</span></code>. This is a slight variation on the forward Euler
method and corresponds to the conventional discretized control schemes.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">noisy_dynamics=True</span></code>, noise is added to the apical compartment of
the neurons. We now simulate the apical compartment with its own
dynamics, as the feedback learning rule needs access to the noisy apical
compartment. We use the following stochastic differential equation for
the apical compartment:</p>
<div class="math">
<p><img src="_images/math/07a8d97ddee65c267fe15a3a0b1f0c473fcb19e9.png" alt="\tau_{\text{fb}} d \mathbf{v}_i^{\text{fb}}(t) = \
    (-\mathbf{v}_i^{\text{fb}}(t) + Q_i \mathbf{u}(t))dt + \sigma \
    \bm{\epsilon}_i(t)"/></p>
</div><p>with <img class="math" src="_images/math/a63cf4e988cc8f205689566f8db7a7f7a2624a39.png" alt="\bm{\epsilon}"/> the Wiener process (Brownian motion) with
covariance matrix <img class="math" src="_images/math/015755a22b6219b345c36a9a47b091dc56007486.png" alt="I"/>.</p>
<p>This is simulated with the Euler-Maruyama method:</p>
<div class="math">
<p><img src="_images/math/6fc40a88f9d90c3c9c4a26ab92818eb5a9d832fa.png" alt="v_i^\text{fb}[k+1] = v_i^\text{fb}[k] + \Delta t / \tau_\text{fb} \
    (-v_i^\text{fb}[k] + Q_i u[k]) + \sigma / \sqrt{\Delta t / \
    \tau_\text{fb}} \Delta \beta"/></p>
</div><p>with <img class="math" src="_images/math/c6a83888bdfb239145f479d0eb2e54a5bdb1b536.png" alt="\Delta \beta"/> drawn from the zero-mean Gaussian distribution
with covariance <img class="math" src="_images/math/015755a22b6219b345c36a9a47b091dc56007486.png" alt="I"/>. The other dynamical equations in the system
remain the same, except that <img class="math" src="_images/math/e79753013d91bd09a880bcb400483d57d1e45746.png" alt="Q_i \mathbf{u}"/> is replaced by
<img class="math" src="_images/math/8bb7ae13ca2b80ef7c92443af0e53f166980ddcb.png" alt="\mathbf{v}_i^\text{fb}"/>:</p>
<div class="math">
<p><img src="_images/math/314708ba2074a95fdc8e34bb33429fdd5c5a7ce5.png" alt="\tau_v \frac{d \mathbf{v}_i(t)}{dt} = -\mathbf{v}_i(t) + W_i \
    \mathbf{r}_{i-1}(t) + b_i + \mathbf{v}_i^\text{fb}"/></p>
</div><p>One can opt for instantaneous apical compartment dynamics by putting
its time constant <img class="math" src="_images/math/3f1575de766ed3c07136f8c53dec71047e41b74a.png" alt="\tau_\text{fb}"/> (<code class="docutils literal notranslate"><span class="pre">apical_time_constant</span></code>) equal
to <code class="docutils literal notranslate"><span class="pre">dt</span></code>. This is not encouraged for training the feedback weights, but
can be used for simulating noisy system dynamics for training the
forward weights, resulting in:</p>
<div class="math">
<p><img src="_images/math/d90723cfbe9939a20b4b764bf12f26b9144e1cb7.png" alt="\tau_v d \mathbf{v}_i(t) = (-\mathbf{v}_i(t) + W_i \
    \mathbf{r}_{i-1}(t) + b_i + Q_i \mathbf{u}(t) )dt + \
    \sigma \bm{\epsilon}_i(t)"/></p>
</div><p>which can again be similarly discretized with the Euler-Maruyama method.</p>
<p>Note that for training the feedback weights, it is recommended to put
<code class="docutils literal notranslate"><span class="pre">inst_transmission=True</span></code>, such that the noise of all layers can
influence the output at the current timestep, instead of having to wait
for a couple of timesteps, depending on the layer depth.</p>
<p>Note that in the current implementation, we interpret that the noise is
added in the apical compartment, and that the basal and somatic
compartments are not noisy. At some point we might want to also add
noise in the somatic and basal compartments for physical realism.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_target</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The output target
<img class="math" src="_images/math/410680b0f6dbbe715405476fb295cc638abbb612.png" alt="\mathbf{r}_L^*"/> that is used by the controller to compute
the control error <img class="math" src="_images/math/912d7183c64b672df9f23d95b376cc519009af6c.png" alt="\mathbf{e}(t)"/>.</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The leakage term of the controller.</p></li>
<li><p><strong>dt</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The time interval used in the forward Euler method.</p></li>
<li><p><strong>tmax</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The maximum number of timesteps.</p></li>
<li><p><strong>k_p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The positive gain parameter for the proportional part
of the controller. If it is equal to zero (by default),
no proportional control will be used, only integral control.</p></li>
<li><p><strong>noisy_dynamics</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating whether noise should be
added to the dynamics.</p></li>
<li><p><strong>inst_transmission</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating whether the modified
version of the forward Euler method should be used, where it is
assumed that there is instant transmission between layers (but
not necessarily instant voltage dynamics). See the docstring
above for more information.</p></li>
<li><p><strong>time_constant_ratio</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Ratio of the time constant of the
voltage dynamics w.r.t. the controller dynamics.</p></li>
<li><p><strong>apical_time_constant</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Time constant of the apical
compartment. If <code class="docutils literal notranslate"><span class="pre">-1</span></code>, we assume that the user does not want
to model the apical compartment dynamics, but assumes instant
transmission to the somatic compartment instead (i.e. apical
time constant of zero).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Tuple containing:</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>r</strong> (list): A list with at index <code class="docutils literal notranslate"><span class="pre">i</span></code> a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>of dimension <img class="math" src="_images/math/71c582acd65f6994657d391562ae32e51d59e479.png" alt="t_{max}\times B \times n_i"/> containing the
firing rates of layer <code class="docutils literal notranslate"><span class="pre">i</span></code> for each timestep.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>u</strong> (torch.Tensor): A tensor of dimension</dt><dd><p><img class="math" src="_images/math/c8dd7f3a82e3e23f79698e3c99bd6f44ef34cabb.png" alt="t_{max}\times B \times n_L"/> containing the control input
for each timestep.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>(v_fb, v_ff, v)</strong> (tuple): A tuple with 3 elements, each</dt><dd><p>containing a list with at index <code class="docutils literal notranslate"><span class="pre">i</span></code> a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> of
dimension <img class="math" src="_images/math/71c582acd65f6994657d391562ae32e51d59e479.png" alt="t_{max}\times B \times n_i"/> containing the
voltage levels of the apical, basal or somatic compartments
respectively.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>sample_error</strong> (torch.Tensor): A tensor of dimension</dt><dd><p><img class="math" src="_images/math/996605aaaa14ddb3bdfe21cd669ece0e9e337869.png" alt="t_{max} \times B"/> containing the L2 norm of the error
<img class="math" src="_images/math/912d7183c64b672df9f23d95b376cc519009af6c.png" alt="\mathbf{e}(t)"/> at each timestep.</p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(…)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.dt_di">
<em class="property">property </em><code class="sig-name descname">dt_di</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.dt_di" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.dt_di" title="networks.dfc_network.DFCNetwork.dt_di"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dt_di</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.dt_di_fb">
<em class="property">property </em><code class="sig-name descname">dt_di_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.dt_di_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.dt_di_fb" title="networks.dfc_network.DFCNetwork.dt_di_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dt_di_fb</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.dynamical_inversion">
<code class="sig-name descname">dynamical_inversion</code><span class="sig-paren">(</span><em class="sig-param">output_target</em>, <em class="sig-param">verbose=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.dynamical_inversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.dynamical_inversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the dynamical (simulated) inversion of the targets.</p>
<p>It does the inversion in real time, controlling all hidden layers
simultaneously.</p>
<p>This function calls <code class="docutils literal notranslate"><span class="pre">self.controller()</span></code> as a subroutine, which
returns values for <img class="math" src="_images/math/352f74d71ad9144677bed76f6a9deb39593f2916.png" alt="\mathbf{u}"/>, <img class="math" src="_images/math/ab3e69b5976e455e35ecc039531e1178711ddcc2.png" alt="\mathbf{v}^\text{fb}"/>,
<img class="math" src="_images/math/c605e79d60f7b21de44f33ea934c70ef6cc1541d.png" alt="\mathbf{v}"/>, <img class="math" src="_images/math/056185c97afba9ed168d64acdb0005a8f5837a0a.png" alt="\mathbf{v}^\text{ff}"/> and <img class="math" src="_images/math/76d48b0f4e9b13740882d287d9df4666be491d00.png" alt="\mathbf{r}"/>
for every simulated time step. The last values of these arrays are taken
to represent the steady state. However, convergence is not guaranteed.
If <code class="docutils literal notranslate"><span class="pre">self.include_non_converged_samples</span></code> is set to  <code class="docutils literal notranslate"><span class="pre">False</span></code>,
the values of batch elements that did not converge are set to
their feedforward mode values, which includes <img class="math" src="_images/math/352f74d71ad9144677bed76f6a9deb39593f2916.png" alt="\mathbf{u}"/> and
<img class="math" src="_images/math/ab3e69b5976e455e35ecc039531e1178711ddcc2.png" alt="\mathbf{v}^\text{fb}"/> being set to 0.
If <code class="docutils literal notranslate"><span class="pre">self.include_non_converged_samples</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, some of
the returned values with <code class="docutils literal notranslate"><span class="pre">_ss</span></code> suffix may in fact not represent the
steady state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_target</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The output targets.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether to display warnings.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>An ordered tuple containing:</p>
<ul class="simple">
<li><p><strong>u_ss</strong> (torch.Tensor): <img class="math" src="_images/math/352f74d71ad9144677bed76f6a9deb39593f2916.png" alt="\mathbf{u}"/>, the final control
input.</p></li>
<li><p><strong>v_ss</strong> (list):
<img class="math" src="_images/math/b8186687701d25f48ff27b57c64afd91f395f7a5.png" alt="\mathbf{v}_{ss} = \mathbf{v}^- + \
\Delta_{\mathbf{v}}"/>
The final voltage activations of the somatic
compartments, split in a list that contains
<img class="math" src="_images/math/33ecb00f6df1be169e5f6d572a03a94f9c69db43.png" alt="\mathbf{v}_{ss}"/> for each layer.</p></li>
<li><p><strong>r_ss</strong> (list): <img class="math" src="_images/math/c29fe7c9a0795ab68ad69e9948e19da27a63e35a.png" alt="\mathbf{r}_{ss} = \
\phi(\mathbf{v}_{ss})"/>. The final firing rates of the
neurons, split in a list that contains <img class="math" src="_images/math/bf0669a4a55cb2318f57da05461de0bc5bed0623.png" alt="\mathbf{r}_{ss}"/>
for each layer.</p></li>
<li><p><strong>r_out_ss</strong> (torch.Tensor): The finaloutput activation
of the network.</p></li>
<li><p><strong>delta_v_ss</strong> (list): A list containing the final
<img class="math" src="_images/math/c77c0878a19a1f0aa54787865d41ff5aee4b8259.png" alt="\Delta \mathbf{v}_i"/> for each layer.</p></li>
<li><p><strong>(u, v_fb, v, v_ff, r)</strong> (tuple): A tuple with 5 elements.
<img class="math" src="_images/math/352f74d71ad9144677bed76f6a9deb39593f2916.png" alt="\mathbf{u}"/> represents a tensor of dimension
<img class="math" src="_images/math/c8dd7f3a82e3e23f79698e3c99bd6f44ef34cabb.png" alt="t_{max}\times B \times n_L"/> containing the control
input for each timestep.
<img class="math" src="_images/math/ab3e69b5976e455e35ecc039531e1178711ddcc2.png" alt="\mathbf{v}^\text{fb}"/>, <img class="math" src="_images/math/c605e79d60f7b21de44f33ea934c70ef6cc1541d.png" alt="\mathbf{v}"/>,
<img class="math" src="_images/math/056185c97afba9ed168d64acdb0005a8f5837a0a.png" alt="\mathbf{v}^\text{ff}"/> each contain a list with at
index <code class="docutils literal notranslate"><span class="pre">i</span></code> a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> of dimension
<img class="math" src="_images/math/71c582acd65f6994657d391562ae32e51d59e479.png" alt="t_{max}\times B \times n_i"/>.
<img class="math" src="_images/math/76d48b0f4e9b13740882d287d9df4666be491d00.png" alt="\mathbf{r}"/> is a list with at index <code class="docutils literal notranslate"><span class="pre">i</span></code> a
<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> of dimension <img class="math" src="_images/math/71c582acd65f6994657d391562ae32e51d59e479.png" alt="t_{max}\times B \times n_i"/>
containing the firing rates of layer <code class="docutils literal notranslate"><span class="pre">i</span></code> for each timestep.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(…)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.epsilon_di">
<em class="property">property </em><code class="sig-name descname">epsilon_di</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.epsilon_di" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.epsilon_di" title="networks.dfc_network.DFCNetwork.epsilon_di"><code class="xref py py-attr docutils literal notranslate"><span class="pre">epsilon_di</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.feedback_params">
<em class="property">property </em><code class="sig-name descname">feedback_params</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.feedback_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Access feedback parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The list of feedback parameters.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>params (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the output <img class="math" src="_images/math/1b5e577d6216dca3af7d87aa122a0b9b360d6cb3.png" alt="y"/> of this network given the input
<img class="math" src="_images/math/888f7c323ac0341871e867220ae2d76467d74d6e.png" alt="x"/>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The input to the network.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The output of the network.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.forward_params">
<em class="property">property </em><code class="sig-name descname">forward_params</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.forward_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Access forward parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The list of forward parameters.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>params (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.forward_params_grad">
<em class="property">property </em><code class="sig-name descname">forward_params_grad</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.forward_params_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a structure identical to forward params but with gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The gradients.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.forward_requires_grad">
<em class="property">property </em><code class="sig-name descname">forward_requires_grad</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.forward_requires_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute forward_requires_grad</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.freeze_fb_weights">
<em class="property">property </em><code class="sig-name descname">freeze_fb_weights</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.freeze_fb_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.freeze_fb_weights" title="networks.dfc_network.DFCNetwork.freeze_fb_weights"><code class="xref py py-attr docutils literal notranslate"><span class="pre">freeze_fb_weights</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.full_Q">
<em class="property">property </em><code class="sig-name descname">full_Q</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.full_Q" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for matrix <img class="math" src="_images/math/e48985136024b7cb7f95b251348694279b61fe54.png" alt="\bar{Q}"/> containing the concatenated
feedback weights.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.get_max_grad">
<code class="sig-name descname">get_max_grad</code><span class="sig-paren">(</span><em class="sig-param">params_type='both'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.get_max_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.get_max_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the maximum gradient across the parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>params_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Whether to compute across the entire set of
parameters or only forward or only feedback.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The maximum gradient encountered.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)">float</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.include_non_converged_samples">
<em class="property">property </em><code class="sig-name descname">include_non_converged_samples</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.include_non_converged_samples" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute
<a class="reference internal" href="#networks.dfc_network.DFCNetwork.include_non_converged_samples" title="networks.dfc_network.DFCNetwork.include_non_converged_samples"><code class="xref py py-attr docutils literal notranslate"><span class="pre">include_non_converged_samples</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.init_feedback_layers_weight_product">
<code class="sig-name descname">init_feedback_layers_weight_product</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.init_feedback_layers_weight_product"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.init_feedback_layers_weight_product" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the feedback weights to the inversion matrices.</p>
<p>The feedback weights will be initializaed to the product of the forward
weights (transposed) of subsequent layers.
Note that this function can’t be called at the level of inidividual
layers since it needs information about the entire network.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.initialization_fb">
<em class="property">property </em><code class="sig-name descname">initialization_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.initialization_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.initialization_fb" title="networks.dfc_network.DFCNetwork.initialization_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">initialization_fb</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.input">
<em class="property">property </em><code class="sig-name descname">input</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.input" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for attribute input.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.inst_apical_dynamics">
<em class="property">property </em><code class="sig-name descname">inst_apical_dynamics</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.inst_apical_dynamics" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.inst_apical_dynamics" title="networks.dfc_network.DFCNetwork.inst_apical_dynamics"><code class="xref py py-attr docutils literal notranslate"><span class="pre">inst_apical_dynamics</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.inst_system_dynamics">
<em class="property">property </em><code class="sig-name descname">inst_system_dynamics</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.inst_system_dynamics" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.inst_system_dynamics" title="networks.dfc_network.DFCNetwork.inst_system_dynamics"><code class="xref py py-attr docutils literal notranslate"><span class="pre">inst_system_dynamics</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.inst_transmission">
<em class="property">property </em><code class="sig-name descname">inst_transmission</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.inst_transmission" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.inst_transmission" title="networks.dfc_network.DFCNetwork.inst_transmission"><code class="xref py py-attr docutils literal notranslate"><span class="pre">inst_transmission</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.inst_transmission_fb">
<em class="property">property </em><code class="sig-name descname">inst_transmission_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.inst_transmission_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.inst_transmission_fb" title="networks.dfc_network.DFCNetwork.inst_transmission_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">inst_transmission_fb</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.k_p">
<em class="property">property </em><code class="sig-name descname">k_p</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.k_p" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.k_p" title="networks.dfc_network.DFCNetwork.k_p"><code class="xref py py-attr docutils literal notranslate"><span class="pre">k_p</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.k_p_fb">
<em class="property">property </em><code class="sig-name descname">k_p_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.k_p_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.k_p_fb" title="networks.dfc_network.DFCNetwork.k_p_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">k_p_fb</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.layer_class">
<em class="property">property </em><code class="sig-name descname">layer_class</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.layer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the layer type to be used.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.learning_rule">
<em class="property">property </em><code class="sig-name descname">learning_rule</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.learning_rule" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.learning_rule" title="networks.dfc_network.DFCNetwork.learning_rule"><code class="xref py py-attr docutils literal notranslate"><span class="pre">learning_rule</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a state into the network.</p>
<p>This function sets the forward and backward weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)"><em>dict</em></a>) – The state with forward and backward weights.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.loss_function_name">
<em class="property">property </em><code class="sig-name descname">loss_function_name</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.loss_function_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.loss_function_name" title="networks.dfc_network.DFCNetwork.loss_function_name"><code class="xref py py-attr docutils literal notranslate"><span class="pre">loss_function_name</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.low_pass_filter_noise">
<em class="property">property </em><code class="sig-name descname">low_pass_filter_noise</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.low_pass_filter_noise" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">_low_pass_filter_noise</span></code></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.low_pass_filter_u">
<em class="property">property </em><code class="sig-name descname">low_pass_filter_u</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.low_pass_filter_u" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.low_pass_filter_u" title="networks.dfc_network.DFCNetwork.low_pass_filter_u"><code class="xref py py-attr docutils literal notranslate"><span class="pre">low_pass_filter_u</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.ndi">
<em class="property">property </em><code class="sig-name descname">ndi</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.ndi" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.ndi" title="networks.dfc_network.DFCNetwork.ndi"><code class="xref py py-attr docutils literal notranslate"><span class="pre">ndi</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.noisy_dynamics">
<em class="property">property </em><code class="sig-name descname">noisy_dynamics</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.noisy_dynamics" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.noisy_dynamics" title="networks.dfc_network.DFCNetwork.noisy_dynamics"><code class="xref py py-attr docutils literal notranslate"><span class="pre">noisy_dynamics</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.non_dynamical_inversion">
<code class="sig-name descname">non_dynamical_inversion</code><span class="sig-paren">(</span><em class="sig-param">output_target</em>, <em class="sig-param">retain_graph=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.non_dynamical_inversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.non_dynamical_inversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the analytical solution for the network activations in the
feedback phase, when the controller pushes the network to reach the
output target. The following formulas are used:</p>
<div class="math">
<p><img src="_images/math/14e4b69d0384e47a0e352eb1ed179f807ad2ac1a.png" alt="\mathbf{u}_{ss} &amp;= (JQ + \alpha I)^{-1} \delta_L \\
\Delta \mathbf{v}_{ss} &amp;= Q \mathbf{u}_{ss}"/></p>
</div><p>with <img class="math" src="_images/math/39f870222c3ea0ffecea2c7ff962e06126f8f5e6.png" alt="\mathbf{u}_{ss}"/> the control input at steady-state,
<img class="math" src="_images/math/ac5ec37d9e2b57b01ab0164a9763cc583d4b34a7.png" alt="\Delta \mathbf{v}_{ss}"/> the apical compartment voltage at
steady-state, <img class="math" src="_images/math/b43a42500991336b05f32537cf40ff7854066943.png" alt="\delta_L"/> the difference between the output target
and the output of the network at the feedforward sweep (without
feedback). For the other symbols, refer to the paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_target</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The output target of the network.</p></li>
<li><p><strong>retain_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating whether the autograd graph
should be retained for later use.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>An ordered tuple containing:</p>
<ul class="simple">
<li><p><strong>u_ndi</strong> (torch.Tensor): <img class="math" src="_images/math/39f870222c3ea0ffecea2c7ff962e06126f8f5e6.png" alt="\mathbf{u}_{ss}"/>, steady state
control input.</p></li>
<li><p><strong>v_ndi</strong> (list):
<img class="math" src="_images/math/36471e1ef305b3f7e6dd052325d23bcf15941efd.png" alt="\mathbf{v}_{ss} = \mathbf{v}^- + \
\Delta \mathbf{v}_{ss}"/>
The steady-state voltage activations of the somatic
compartments, split in a list that contains
<img class="math" src="_images/math/33ecb00f6df1be169e5f6d572a03a94f9c69db43.png" alt="\mathbf{v}_{ss}"/> for each layer.</p></li>
<li><p><strong>r_ndi</strong> (list): <img class="math" src="_images/math/c29fe7c9a0795ab68ad69e9948e19da27a63e35a.png" alt="\mathbf{r}_{ss} = \
\phi(\mathbf{v}_{ss})"/>. The steady-state firing rates of the
neurons, split in a list that contains <img class="math" src="_images/math/bf0669a4a55cb2318f57da05461de0bc5bed0623.png" alt="\mathbf{r}_{ss}"/>
for each layer.</p></li>
<li><p><strong>r_out_ndi</strong> (torch.Tensor): The steady state output activation
of the network.</p></li>
<li><p><strong>delta_v_ndi_split</strong> (list): A list containing
<img class="math" src="_images/math/ac5ec37d9e2b57b01ab0164a9763cc583d4b34a7.png" alt="\Delta \mathbf{v}_{ss}"/> for each layer.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(…)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.params">
<em class="property">property </em><code class="sig-name descname">params</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.params" title="Permalink to this definition">¶</a></dt>
<dd><p>Access all parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The list of all parameters.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>params (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.proactive_controller">
<em class="property">property </em><code class="sig-name descname">proactive_controller</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.proactive_controller" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.proactive_controller" title="networks.dfc_network.DFCNetwork.proactive_controller"><code class="xref py py-attr docutils literal notranslate"><span class="pre">proactive_controller</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.r">
<em class="property">property </em><code class="sig-name descname">r</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.r" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for attribute targets.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.save_H_angles">
<code class="sig-name descname">save_H_angles</code><span class="sig-paren">(</span><em class="sig-param">writer</em>, <em class="sig-param">step</em>, <em class="sig-param">loss</em>, <em class="sig-param">save_tensorboard=True</em>, <em class="sig-param">save_dataframe=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.save_H_angles"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.save_H_angles" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the angles of the current forward parameter updates
with the update driven from the Lu loss for those parameters</p>
<p>Save the angle in the tensorboard writer (if <code class="docutils literal notranslate"><span class="pre">save_tensorboard=True</span></code>)
and in the corresponding dataframe (if <code class="docutils literal notranslate"><span class="pre">save_dataframe=True</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <a class="reference internal" href="#networks.dfc_network.DFCNetwork.save_ndi_angles" title="networks.dfc_network.DFCNetwork.save_ndi_angles"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_ndi_angles()</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.save_bp_angles">
<code class="sig-name descname">save_bp_angles</code><span class="sig-paren">(</span><em class="sig-param">writer</em>, <em class="sig-param">step</em>, <em class="sig-param">loss</em>, <em class="sig-param">retain_graph=False</em>, <em class="sig-param">save_tensorboard=True</em>, <em class="sig-param">save_dataframe=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.save_bp_angles"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.save_bp_angles" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the angles of the current forward parameter updates
with the backprop update.</p>
<p>Save the angle in the tensorboard writer (if <code class="docutils literal notranslate"><span class="pre">save_tensorboard=True</span></code>)
and in the corresponding dataframe (if <code class="docutils literal notranslate"><span class="pre">save_dataframe=True</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <a class="reference internal" href="#networks.dfc_network.DFCNetwork.save_ndi_angles" title="networks.dfc_network.DFCNetwork.save_ndi_angles"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_ndi_angles()</span></code></a>.</p></li>
<li><p><strong>loss</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – Output loss of the network.</p></li>
<li><p><strong>retain_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating whether the graph of the
network should be retained after computing the gradients or
Jacobians. If the graph will not be used anymore for the current
minibatch afterwards, <cite>retain_graph</cite> should be <cite>False</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.save_feedback_batch_logs">
<code class="sig-name descname">save_feedback_batch_logs</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">writer</em>, <em class="sig-param">step</em>, <em class="sig-param">init=False</em>, <em class="sig-param">save_tensorboard=True</em>, <em class="sig-param">save_dataframe=True</em>, <em class="sig-param">save_statistics=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.save_feedback_batch_logs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.save_feedback_batch_logs" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the logs for the current minibatch on tensorboardX.</p>
<p>Args:</p>
<p>Save the angle in the tensorboard writer (if <code class="docutils literal notranslate"><span class="pre">save_tensorboard=True</span></code>)
and in the corresponding dataframe (if <code class="docutils literal notranslate"><span class="pre">save_dataframe=True</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <a class="reference internal" href="#networks.dfc_network.DFCNetwork.save_ndi_angles" title="networks.dfc_network.DFCNetwork.save_ndi_angles"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_ndi_angles()</span></code></a></p></li>
<li><p><strong>init</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating that the training is in the
initialization phase (only training the feedback weights).</p></li>
<li><p><strong>save_statistics</strong> – Flag indicating whether the statistics of the
feedback weights should be saved (e.g. gradient norms).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.save_logs">
<code class="sig-name descname">save_logs</code><span class="sig-paren">(</span><em class="sig-param">writer</em>, <em class="sig-param">step</em>, <em class="sig-param">log_weights='both'</em>, <em class="sig-param">prefix=''</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.save_logs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.save_logs" title="Permalink to this definition">¶</a></dt>
<dd><p>Log the norm of the weights and the gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>writer</strong> – The tensorboard writer.</p></li>
<li><p><strong>step</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The writer iteration.</p></li>
<li><p><strong>log_weights</strong> (<em>True</em>) – Which weights to log.</p></li>
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The naming prefix.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.save_ndi_angles">
<code class="sig-name descname">save_ndi_angles</code><span class="sig-paren">(</span><em class="sig-param">writer</em>, <em class="sig-param">step</em>, <em class="sig-param">save_dataframe=True</em>, <em class="sig-param">save_tensorboard=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.save_ndi_angles"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.save_ndi_angles" title="Permalink to this definition">¶</a></dt>
<dd><p>Save angle between dynamical and analytical inversion results.</p>
<p>The analytical results have been stored during training in
<cite>self.layers[i].ndi_update_weights</cite>.</p>
<p>Save the angle in the tensorboard writer (if <code class="docutils literal notranslate"><span class="pre">save_tensorboard=True</span></code>)
and in the corresponding dataframe (if <code class="docutils literal notranslate"><span class="pre">save_dataframe=True</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>writer</strong> – Tensorboard writer.</p></li>
<li><p><strong>step</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The number of forward training mini-batches.</p></li>
<li><p><strong>save_dataframe</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating whether a dataframe of the
angles should be saved in the network object.</p></li>
<li><p><strong>save_tensorboard</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating whether the angles should
be saved in Tensorboard.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.save_ratio_ff_fb">
<code class="sig-name descname">save_ratio_ff_fb</code><span class="sig-paren">(</span><em class="sig-param">writer</em>, <em class="sig-param">step</em>, <em class="sig-param">loss</em>, <em class="sig-param">save_tensorboard=True</em>, <em class="sig-param">save_dataframe=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.save_ratio_ff_fb"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.save_ratio_ff_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the ratio of the current feedforward and feedback stimulus.</p>
<p>Save the angle in the tensorboard writer (if <code class="docutils literal notranslate"><span class="pre">save_tensorboard=True</span></code>)
and in the corresponding dataframe (if <code class="docutils literal notranslate"><span class="pre">save_dataframe=True</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <a class="reference internal" href="#networks.dfc_network.DFCNetwork.save_ndi_angles" title="networks.dfc_network.DFCNetwork.save_ndi_angles"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_ndi_angles()</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.scaling_fb_updates">
<em class="property">property </em><code class="sig-name descname">scaling_fb_updates</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.scaling_fb_updates" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.scaling_fb_updates" title="networks.dfc_network.DFCNetwork.scaling_fb_updates"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scaling_fb_updates</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.set_grads_to_bp">
<code class="sig-name descname">set_grads_to_bp</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">retain_graph=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.set_grads_to_bp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.set_grads_to_bp" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the gradients to correspond to those obtained with backprop.</p>
<p>This function replaces the <code class="docutils literal notranslate"><span class="pre">grad</span></code> attributes of the forward weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The current loss.</p></li>
<li><p><strong>retain_graph</strong> (<em>boolean</em>) – Whether autograd graph should be retained.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.sigma">
<em class="property">property </em><code class="sig-name descname">sigma</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.sigma" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.sigma" title="networks.dfc_network.DFCNetwork.sigma"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sigma</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.sigma_fb">
<em class="property">property </em><code class="sig-name descname">sigma_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.sigma_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.sigma_fb" title="networks.dfc_network.DFCNetwork.sigma_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sigma_fb</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.sigma_init">
<em class="property">property </em><code class="sig-name descname">sigma_init</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.sigma_init" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">_sigma_init</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.sigma_output">
<em class="property">property </em><code class="sig-name descname">sigma_output</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.sigma_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.sigma_output" title="networks.dfc_network.DFCNetwork.sigma_output"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sigma_output</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.sigma_output_fb">
<em class="property">property </em><code class="sig-name descname">sigma_output_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.sigma_output_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.sigma_output_fb" title="networks.dfc_network.DFCNetwork.sigma_output_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sigma_output_fb</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.state_dict">
<em class="property">property </em><code class="sig-name descname">state_dict</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing the current state of the network,
incliding forward and backward weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The forward and feedback weights.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)">dict</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.strong_feedback">
<em class="property">property </em><code class="sig-name descname">strong_feedback</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.strong_feedback" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.strong_feedback" title="networks.dfc_network.DFCNetwork.strong_feedback"><code class="xref py py-attr docutils literal notranslate"><span class="pre">strong_feedback</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.target_stepsize">
<em class="property">property </em><code class="sig-name descname">target_stepsize</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.target_stepsize" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.target_stepsize" title="networks.dfc_network.DFCNetwork.target_stepsize"><code class="xref py py-attr docutils literal notranslate"><span class="pre">target_stepsize</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.tau_f">
<em class="property">property </em><code class="sig-name descname">tau_f</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.tau_f" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.tau_f" title="networks.dfc_network.DFCNetwork.tau_f"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tau_f</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.tau_noise">
<em class="property">property </em><code class="sig-name descname">tau_noise</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.tau_noise" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.tau_noise" title="networks.dfc_network.DFCNetwork.tau_noise"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tau_noise</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.time_constant_ratio">
<em class="property">property </em><code class="sig-name descname">time_constant_ratio</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.time_constant_ratio" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.time_constant_ratio" title="networks.dfc_network.DFCNetwork.time_constant_ratio"><code class="xref py py-attr docutils literal notranslate"><span class="pre">time_constant_ratio</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.time_constant_ratio_fb">
<em class="property">property </em><code class="sig-name descname">time_constant_ratio_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.time_constant_ratio_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.time_constant_ratio_fb" title="networks.dfc_network.DFCNetwork.time_constant_ratio_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">time_constant_ratio_fb</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.tmax_di">
<em class="property">property </em><code class="sig-name descname">tmax_di</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.tmax_di" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.tmax_di" title="networks.dfc_network.DFCNetwork.tmax_di"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tmax_di</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.tmax_di_fb">
<em class="property">property </em><code class="sig-name descname">tmax_di_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.tmax_di_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.tmax_di_fb" title="networks.dfc_network.DFCNetwork.tmax_di_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tmax_di_fb</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Override <cite>to</cite> method to also move the backward weights.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.u">
<em class="property">property </em><code class="sig-name descname">u</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.u" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.u" title="networks.dfc_network.DFCNetwork.u"><code class="xref py py-attr docutils literal notranslate"><span class="pre">u</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.unflatten_params">
<code class="sig-name descname">unflatten_params</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">params_type='forward_params'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network.html#DFCNetwork.unflatten_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network.DFCNetwork.unflatten_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Unflatten the parameters.</p>
<p>This function assumes a certain structure in the parameters to unflatten
a list into the same structure as <cite>self.forward_params</cite> or
<cite>self.feedback_params</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – The flat list.</p></li>
<li><p><strong>params_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The type of params we are dealing with.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The unflattened list.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)">list</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network.DFCNetwork.use_jacobian_as_fb">
<em class="property">property </em><code class="sig-name descname">use_jacobian_as_fb</code><a class="headerlink" href="#networks.dfc_network.DFCNetwork.use_jacobian_as_fb" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network.DFCNetwork.use_jacobian_as_fb" title="networks.dfc_network.DFCNetwork.use_jacobian_as_fb"><code class="xref py py-attr docutils literal notranslate"><span class="pre">use_jacobian_as_fb</span></code></a></p>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-networks.dfc_layer"></span><div class="section" id="implementation-of-a-layer-for-deep-feedback-control">
<h3><a class="toc-backref" href="#id10">Implementation of a layer for Deep Feedback Control</a><a class="headerlink" href="#implementation-of-a-layer-for-deep-feedback-control" title="Permalink to this headline">¶</a></h3>
<p>A layer that is prepared to be trained with DFC.</p>
<dl class="class">
<dt id="networks.dfc_layer.DFCLayer">
<em class="property">class </em><code class="sig-prename descclassname">networks.dfc_layer.</code><code class="sig-name descname">DFCLayer</code><span class="sig-paren">(</span><em class="sig-param">in_features</em>, <em class="sig-param">out_features</em>, <em class="sig-param">last_layer_features</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">requires_grad=False</em>, <em class="sig-param">forward_activation='tanh'</em>, <em class="sig-param">initialization='orthogonal'</em>, <em class="sig-param">initialization_fb='weight_product'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_layer.html#DFCLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_layer.DFCLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#networks.layer_interface.LayerInterface" title="networks.layer_interface.LayerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">networks.layer_interface.LayerInterface</span></code></a></p>
<p>Implementation of a Deep Feedback Control layer.</p>
<p>It contains the following important functions:</p>
<ul class="simple">
<li><p>forward: which computes the linear activation based on the previous layer
as well as the post non-linearity activation. It stores these in the
attributes “_linear_activations” and “_activa.tions”.</p></li>
<li><p>compute_forward_gradients: computes the forward parameter updates and
stores them under “grad”, by using the pre-synaptic activations and the
controller feedback. The ule is based on a voltage difference rule.</p></li>
<li><p>compute_forward_gradients_continuous: same as “compute_forward_gradients”
but it performs an integration over time.</p></li>
<li><p>compute_feedback_gradients: compute the feedback gradients.</p></li>
<li><p>compute_feedback_gradients_continuous: same as
“compute_feedback_gradients” but it performs an integration over time.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of class <code class="xref py py-class docutils literal notranslate"><span class="pre">layer_interface.LayerInterface</span></code>.</p></li>
<li><p><strong>last_layer_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The size of the output layer.</p></li>
</ul>
</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="networks.dfc_layer.DFCLayer.activations">
<em class="property">property </em><code class="sig-name descname">activations</code><a class="headerlink" href="#networks.dfc_layer.DFCLayer.activations" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_layer.DFCLayer.activations" title="networks.dfc_layer.DFCLayer.activations"><code class="xref py py-attr docutils literal notranslate"><span class="pre">activations</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_layer.DFCLayer.compute_feedback_gradients_continuous">
<code class="sig-name descname">compute_feedback_gradients_continuous</code><span class="sig-paren">(</span><em class="sig-param">v_fb_time</em>, <em class="sig-param">u_time</em>, <em class="sig-param">t_start=None</em>, <em class="sig-param">t_end=None</em>, <em class="sig-param">sigma=1.0</em>, <em class="sig-param">beta=0.0</em>, <em class="sig-param">scaling=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_layer.html#DFCLayer.compute_feedback_gradients_continuous"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_layer.DFCLayer.compute_feedback_gradients_continuous" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes feedback gradients using an integration (sum) of voltage.</p>
<p>This weight update is identical to <code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_feedback_gradients()</span></code>
except that it allows to integrate over more than one timestep.</p>
<p>It follows the differential equation:</p>
<div class="math">
<p><img src="_images/math/5835c03ee19fe15ea45bff225204b315cba6dde9.png" alt="\frac{dQ_i}{dt} = -\mathbf{v}_i^\text{fb} \mathbf{u}(t)^T - \
    \beta Q_i"/></p>
</div><p>Refer to <code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_feedback_gradients()</span></code> for variable details.</p>
<p>Note that pytorch saves the positive gradient, hence we should save
<img class="math" src="_images/math/02e00ef86a7297f7669185aea84a2fa02d5e21f4.png" alt="-\Delta Q_i"/>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>v_fb_time</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The apical compartment voltages over
a certain time period.</p></li>
<li><p><strong>u_time</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The control inputs over  certain time period.</p></li>
<li><p><strong>t_start</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The start index from which the summation
over time should start.</p></li>
<li><p><strong>t_end</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The stop index at which the summation over
time should stop.</p></li>
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The standard deviation of the noise in the network
dynamics. This is used to scale the fb weight update, such that
its magnitude is independent of the noise variance.</p></li>
<li><p><strong>beta</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – The homeostatic weight decay parameter.</p></li>
<li><p><strong>scaling</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – In the theory for the feedback weight updates, the
update for each layer should be scaled with
<img class="math" src="_images/math/951a29793bdfaffa9dfb9f60d18d3990dd610636.png" alt="(1+\tau_{v}/\tau_{\epsilon})^{L-i}"/>, with L the amount of
layers and i the layer index. <code class="docutils literal notranslate"><span class="pre">scaling</span></code> should be the factor
<img class="math" src="_images/math/951a29793bdfaffa9dfb9f60d18d3990dd610636.png" alt="(1+\tau_{v}/\tau_{\epsilon})^{L-i}"/> for this layer.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_layer.DFCLayer.compute_forward_gradients">
<code class="sig-name descname">compute_forward_gradients</code><span class="sig-paren">(</span><em class="sig-param">delta_v</em>, <em class="sig-param">r_previous</em>, <em class="sig-param">scale=1.0</em>, <em class="sig-param">saving_ndi_updates=False</em>, <em class="sig-param">learning_rule='nonlinear_difference'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_layer.html#DFCLayer.compute_forward_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_layer.DFCLayer.compute_forward_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes forward gradients using a local-in-time learning rule.</p>
<p>This function applies a non-linear difference learning rule as described
in Eq. (5) in the paper. Specifically, it compues the difference between
the non-linear transformation of basal and somatic voltages.</p>
<p>Depending on the option <code class="docutils literal notranslate"><span class="pre">saving_ndi_updates</span></code> these updates will be
stored in different locations (see argument docstring).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>delta_v</strong> – The feedback teaching signal from the controller.</p></li>
<li><p><strong>r_previous</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The activations of the previous layer.</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a>) – Scaling factor for the gradients.</p></li>
<li><p><strong>saving_ndi_updates</strong> (<em>boolean</em>) – Whether to save the non-dynamical
inversion updates. When <code class="docutils literal notranslate"><span class="pre">True</span></code>, computed updates are added to
<code class="docutils literal notranslate"><span class="pre">ndi_updates_weights</span></code> (and bias) to later compare with the
steady-state/continuous updates. When <code class="docutils literal notranslate"><span class="pre">False</span></code>, computed
updates are added to <code class="docutils literal notranslate"><span class="pre">weights.grad</span></code> (and bias), to be later
updated.</p></li>
<li><p><strong>learning_rule</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The type of learning rule.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_layer.DFCLayer.compute_forward_gradients_continuous">
<code class="sig-name descname">compute_forward_gradients_continuous</code><span class="sig-paren">(</span><em class="sig-param">v_time</em>, <em class="sig-param">v_ff_time</em>, <em class="sig-param">r_previous_time</em>, <em class="sig-param">t_start=None</em>, <em class="sig-param">t_end=None</em>, <em class="sig-param">learning_rule='nonlinear_difference'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_layer.html#DFCLayer.compute_forward_gradients_continuous"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_layer.DFCLayer.compute_forward_gradients_continuous" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes forward gradients using an integration (sum) of voltage
differences across comparments.</p>
<p>This weight update is identical to <code class="docutils literal notranslate"><span class="pre">compute_forward_gradients</span></code>
except that it allows to integrate over more than one timestep.
However, here the somatic and basal voltages are assumed to have been
computed outside and provided as an input argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>v_time</strong> – The somatic voltages at different timesteps.</p></li>
<li><p><strong>v_ff_time</strong> – The basal voltages at different timesteps.</p></li>
<li><p><strong>r_previous_time</strong> – The activations of the previous layer at different
timesteps.</p></li>
<li><p><strong>t_start</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The initial time index for the integration.</p></li>
<li><p><strong>t_end</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The final time index for the integration.</p></li>
<li><p><strong>learning_rule</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The type of learning rule.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_layer.DFCLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_layer.html#DFCLayer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_layer.DFCLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the output of the layer.</p>
<p>This method applies first a linear mapping with the parameters
<code class="docutils literal notranslate"><span class="pre">weights</span></code> and <code class="docutils literal notranslate"><span class="pre">bias</span></code>, after which it applies the forward activation
function.</p>
<p>In the forward pass there is no noise, and thus the normal activations
and the low-pass filtered activations are identical.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – Mini-batch of size <cite>[B, in_features]</cite> with input
activations from the previous layer or input.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The mini-batch of output activations of the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_layer.DFCLayer.linear_activations">
<em class="property">property </em><code class="sig-name descname">linear_activations</code><a class="headerlink" href="#networks.dfc_layer.DFCLayer.linear_activations" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_layer.DFCLayer.linear_activations" title="networks.dfc_layer.DFCLayer.linear_activations"><code class="xref py py-attr docutils literal notranslate"><span class="pre">linear_activations</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_layer.DFCLayer.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#networks.dfc_layer.DFCLayer.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="networks.dfc_layer.DFCLayer.save_feedback_batch_logs">
<code class="sig-name descname">save_feedback_batch_logs</code><span class="sig-paren">(</span><em class="sig-param">writer</em>, <em class="sig-param">step</em>, <em class="sig-param">name</em>, <em class="sig-param">no_gradient=False</em>, <em class="sig-param">pretraining=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_layer.html#DFCLayer.save_feedback_batch_logs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_layer.DFCLayer.save_feedback_batch_logs" title="Permalink to this definition">¶</a></dt>
<dd><p>Save feedback weight stats for the latest mini-batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>writer</strong> (<em>SummaryWriter</em>) – Summary writer from tensorboardX.</p></li>
<li><p><strong>step</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The global step used for the x-axis of the plots.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The name of the layer.</p></li>
<li><p><strong>no_gradient</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating whether we should skip saving
the gradients of the feedback weights.</p></li>
<li><p><strong>pretraining</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Flag indicating that the training is in the
initialization phase (only training the feedback weights).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_layer.DFCLayer.set_direct_feedback_layer">
<code class="sig-name descname">set_direct_feedback_layer</code><span class="sig-paren">(</span><em class="sig-param">last_features</em>, <em class="sig-param">out_features</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_layer.html#DFCLayer.set_direct_feedback_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_layer.DFCLayer.set_direct_feedback_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the network backward parameters.</p>
<p>This layer connects the output layer to a hidden layer. No biases are
used in direct feedback layers. These backward parameters have no
gradient as they are fixed.</p>
<p>Note that as opposed to DFA, here the backwards weights are not
Parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method
<code class="xref py py-meth docutils literal notranslate"><span class="pre">layer_interface.LayerInterface.set_layer()</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_layer.DFCLayer.weights_backward">
<em class="property">property </em><code class="sig-name descname">weights_backward</code><a class="headerlink" href="#networks.dfc_layer.DFCLayer.weights_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">_weights_backward</span></code>.</p>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-networks.dfc_network_single_phase"></span><div class="section" id="implementation-of-a-network-for-deep-feedback-control-that-uses-a-single-phase">
<h3><a class="toc-backref" href="#id11">Implementation of a network for Deep Feedback Control that uses a single phase</a><a class="headerlink" href="#implementation-of-a-network-for-deep-feedback-control-that-uses-a-single-phase" title="Permalink to this headline">¶</a></h3>
<p>A network that is prepared to be trained with DFC but using a single phase for
training both the forward and the feedback weights.</p>
<dl class="class">
<dt id="networks.dfc_network_single_phase.DFCNetworkSinglePhase">
<em class="property">class </em><code class="sig-prename descclassname">networks.dfc_network_single_phase.</code><code class="sig-name descname">DFCNetworkSinglePhase</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">pretrain_without_controller=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network_single_phase.html#DFCNetworkSinglePhase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#networks.dfc_network.DFCNetwork" title="networks.dfc_network.DFCNetwork"><code class="xref py py-class docutils literal notranslate"><span class="pre">networks.dfc_network.DFCNetwork</span></code></a></p>
<p>Implementation of a network for Deep Feedback Control with single phase.</p>
<p>Network that always udpates the feedfoward and feedback weights
simultaneously in one single phase.</p>
<p>In this single-phase DFC setting, the following options exist for defining
the target activations. For forward weight learning, the target outputs are
either feedforward activations nudged towards lower loss (default) or set as
the actual supervised targets (if the option <cite>strong_feedback</cite> is active),
just like in two-phase DFC. For feedback weight learning, the target
outputs are either nudged or set to the supervised targets (if
<cite>strong_feedback</cite> is active). However, in the pre-training stage, if the
option <cite>pretrain_without_controller</cite> is active, the targets are set to the
forward activations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of class <code class="xref py py-class docutils literal notranslate"><span class="pre">dfc_network.DFCNetwork</span></code>.</p></li>
<li><p><strong>pretrain_without_controller</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether pretraining should be done
without the controller being on.</p></li>
</ul>
</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="networks.dfc_network_single_phase.DFCNetworkSinglePhase.alpha_r">
<em class="property">property </em><code class="sig-name descname">alpha_r</code><a class="headerlink" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase.alpha_r" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase.alpha_r" title="networks.dfc_network_single_phase.DFCNetworkSinglePhase.alpha_r"><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha_r</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network_single_phase.DFCNetworkSinglePhase.alpha_u">
<em class="property">property </em><code class="sig-name descname">alpha_u</code><a class="headerlink" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase.alpha_u" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute <a class="reference internal" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase.alpha_u" title="networks.dfc_network_single_phase.DFCNetworkSinglePhase.alpha_u"><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha_u</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network_single_phase.DFCNetworkSinglePhase.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">targets=None</em>, <em class="sig-param">verbose=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network_single_phase.html#DFCNetworkSinglePhase.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the feedback phase of the network.</p>
<p>Here, the network is pushed to the output target by the controller and
used to compute update of the forward and backward weights.</p>
<p>This function simply constitutes a wrapper around the base <cite>backward</cite>
function, where forward updates are computed, and just adds the
feedback weight update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – Mean output loss for current mini-batch.</p></li>
<li><p><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The dataset targets. This will usually be
ignored, as the targets will be taken to be the activations
nudged towards lower loss, unless we use strong feedback.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether to display warnings.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network_single_phase.DFCNetworkSinglePhase.compute_feedback_gradients">
<code class="sig-name descname">compute_feedback_gradients</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">targets</em>, <em class="sig-param">u_time=None</em>, <em class="sig-param">v_fb_time=None</em>, <em class="sig-param">init=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network_single_phase.html#DFCNetworkSinglePhase.compute_feedback_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase.compute_feedback_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the gradients of the feedback weights for each layer.</p>
<p>This function is called in two different situations:</p>
<p>1. During pre-training of the feedback weights, there has not yet been a
simulation, so this function calls a simulation (with special values for
<img class="math" src="_images/math/2f5aa019312e1bbc969deab8dca8b00f76025404.png" alt="\alpha"/> and <img class="math" src="_images/math/9630132210b904754c9ab272b61cb527d12263ca.png" alt="k"/> to ensure stability during pre-training) and
uses the results of the simulation to update the feedback weights.
In this case, the inputs <img class="math" src="_images/math/ab3e69b5976e455e35ecc039531e1178711ddcc2.png" alt="\mathbf{v}^\text{fb}"/> and
<img class="math" src="_images/math/1fca50f37c5fdcaaf7e0c5fc6e2746d369bd564b.png" alt="\mathbf{u}^\text{hp}"/> will be <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>2. During the simultaneous training of feedforward and feedback weights,
the backward method already simulates the dynamics, and the results are
passed through <img class="math" src="_images/math/ab3e69b5976e455e35ecc039531e1178711ddcc2.png" alt="\mathbf{v}^\text{fb}"/> and
<img class="math" src="_images/math/1fca50f37c5fdcaaf7e0c5fc6e2746d369bd564b.png" alt="\mathbf{u}^\text{hp}"/>. In this case, we directly use these
simulation results to compute the updates without running a new
simulation.</p>
<p>The feedback weight updates are computed according to the following rule:</p>
<div class="math">
<p><img src="_images/math/9d76ff3f5199f2a88a4497c50b02f849b008b8d0.png" alt="\Delta Q = -(1+\frac{\tau_v}{\tau_{\epsilon}})^{L-i}\
    \frac{1}{K\sigma^2} \sum_k \mathbf{v}^\text{fb}_i[k] \
    \mathbf{u}^{\text{hp}T}[k]"/></p>
</div><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <a class="reference internal" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase.backward" title="networks.dfc_network_single_phase.DFCNetworkSinglePhase.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a>.</p></li>
<li><p><strong>u_time</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – A torch.Tensor of dimension
<img class="math" src="_images/math/c8dd7f3a82e3e23f79698e3c99bd6f44ef34cabb.png" alt="t_{max}\times B \times n_L"/> containing the high-pass
filtered controller inputs. If None (by default), a new
simulation will be run to calculate v_fb_time and u_time.</p></li>
<li><p><strong>v_fb_time</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – A list with at index <code class="docutils literal notranslate"><span class="pre">i</span></code> a torch.Tensor
of dimension <img class="math" src="_images/math/71c582acd65f6994657d391562ae32e51d59e479.png" alt="t_{max}\times B \times n_i"/> containing the
voltage levels of the apical (feedback) compartment of layer
<cite>i</cite>. If <code class="docutils literal notranslate"><span class="pre">None</span></code> (by default), a new simulation will be run to
calculate <img class="math" src="_images/math/ab3e69b5976e455e35ecc039531e1178711ddcc2.png" alt="\mathbf{v}^\text{fb}"/> and
<img class="math" src="_images/math/1fca50f37c5fdcaaf7e0c5fc6e2746d369bd564b.png" alt="\mathbf{u}^\text{hp}"/>.</p></li>
<li><p><strong>init</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether this is a pre-training stage. If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
dynamics values specific for the feedback path will be used.
Else, the same as the forward pass will be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network_single_phase.DFCNetworkSinglePhase.dynamical_inversion">
<code class="sig-name descname">dynamical_inversion</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network_single_phase.html#DFCNetworkSinglePhase.dynamical_inversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase.dynamical_inversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the dynamical (simulated) inversion of the targets.</p>
<p>Applies the same function as in the base DFC class, but adds a low-pass
filter to the target activations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_target</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The output targets.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.dfc_network_single_phase.DFCNetworkSinglePhase.pretrain_without_controller">
<em class="property">property </em><code class="sig-name descname">pretrain_without_controller</code><a class="headerlink" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase.pretrain_without_controller" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for read-only attribute
<a class="reference internal" href="#networks.dfc_network_single_phase.DFCNetworkSinglePhase.pretrain_without_controller" title="networks.dfc_network_single_phase.DFCNetworkSinglePhase.pretrain_without_controller"><code class="xref py py-attr docutils literal notranslate"><span class="pre">pretrain_without_controller</span></code></a></p>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-networks.credit_assignment_functions"></span><div class="section" id="adding-custom-functions-to-pytorch-s-autograd">
<h3><a class="toc-backref" href="#id12">Adding custom functions to PyTorch’s autograd</a><a class="headerlink" href="#adding-custom-functions-to-pytorch-s-autograd" title="Permalink to this headline">¶</a></h3>
<p>The module <code class="xref py py-mod docutils literal notranslate"><span class="pre">lib.backprop_functions</span></code> contains custom implementations of
neural network components (layers, activation functions, loss functions, …),
that are compatible with PyTorch its <a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">autograd</a> package.</p>
<p>A new functionality can be added to <a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">autograd</a> by creating a subclass of class
<a class="reference external" href="https://pytorch.org/docs/master/autograd.html#torch.autograd.Function" title="(in PyTorch v1.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>. In particular, we have to implement the
<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="(in PyTorch v1.12)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward()</span></code></a> method (which computes the output of a
differentiable function) and the <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward" title="(in PyTorch v1.12)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.backward()</span></code></a>
method (which computes the partial derivatives of the output of the implemented
<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="(in PyTorch v1.12)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward()</span></code></a> method with respect to all input tensors
that are flagged to require gradients).</p>
<dl class="class">
<dt id="networks.credit_assignment_functions.DFANonlinearFunction">
<em class="property">class </em><code class="sig-prename descclassname">networks.credit_assignment_functions.</code><code class="sig-name descname">DFANonlinearFunction</code><a class="reference internal" href="_modules/networks/credit_assignment_functions.html#DFANonlinearFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.credit_assignment_functions.DFANonlinearFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>Implementation of a fully-connected layer with activation function.</p>
<p>This class is very similar to <code class="docutils literal notranslate"><span class="pre">NonlinearFunction</span></code> but it provides layers
to be used with Direct Feedback Alignment, i.e. it projects gradients of
the last layer directly to all upstream layers, while keeping the forward
pass unchanged.</p>
<dl class="method">
<dt id="networks.credit_assignment_functions.DFANonlinearFunction.backward">
<em class="property">static </em><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">grad_Z</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/credit_assignment_functions.html#DFANonlinearFunction.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.credit_assignment_functions.DFANonlinearFunction.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Directly project the output gradients to this layer.</p>
<p>The matrix <code class="docutils literal notranslate"><span class="pre">grad_Z</span></code>, which we denote by
<img class="math" src="_images/math/4b23c2ee60e00adaa1d48db1cddfdfc2ca2b3c0d.png" alt="\delta_Z \in \mathbb{R}^{B \times N}"/>, contains the partial
derivatives of the scalar loss function with respect to each element
from the <a class="reference internal" href="#networks.credit_assignment_functions.DFANonlinearFunction.forward" title="networks.credit_assignment_functions.DFANonlinearFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> output matrix <img class="math" src="_images/math/95f028ab2b20b895fa12d986e0d9f40f7b6e52d3.png" alt="Z"/>. It is ignored for all
layers, except for the last layer.</p>
<p>For the linear component, the partial derivatives of the weights can be
computed as follows:</p>
<div class="math">
<p><img src="_images/math/0752f45e92d8d31115c4effac539ab85d59905f8.png" alt="\delta_W &amp;= B \delta_{Z_N}^T A \\"/></p>
</div><p>Where <img class="math" src="_images/math/4bc3e94a67870b41b7c20179693e889251e2c136.png" alt="B"/> is the feedback matrix that allows projecting from the
output layer, and <img class="math" src="_images/math/dbda6fccf9e6df650eaf2bd7818edf407bf9b0e3.png" alt="\delta_{Z_N}"/> is the gradient of the loss in
the last layer.</p>
<p>These need to be multiplied by the derivative of the post-nonlinearity
with respect to its input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> – See description of argument <code class="docutils literal notranslate"><span class="pre">ctx</span></code> of method <a class="reference internal" href="#networks.credit_assignment_functions.DFANonlinearFunction.forward" title="networks.credit_assignment_functions.DFANonlinearFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>.</p></li>
<li><p><strong>grad_Z</strong> – The backpropagated error <img class="math" src="_images/math/df6d8b87a81df9c87eec08cb28c85b7fd9516d61.png" alt="\delta_Z"/>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>See docstring of method <a class="reference internal" href="#networks.credit_assignment_functions.NonlinearFunction.backward" title="networks.credit_assignment_functions.NonlinearFunction.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">NonlinearFunction.backward()</span></code></a>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(…)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.credit_assignment_functions.DFANonlinearFunction.forward">
<em class="property">static </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">A</em>, <em class="sig-param">W</em>, <em class="sig-param">B</em>, <em class="sig-param">grad_out</em>, <em class="sig-param">nonlinearity</em>, <em class="sig-param">is_last_layer</em>, <em class="sig-param">b=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/credit_assignment_functions.html#DFANonlinearFunction.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.credit_assignment_functions.DFANonlinearFunction.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the output of a non-linear layer.</p>
<p>Same as in <cite>NonlinearFunction.forward</cite> except that here we also store
in the context the direct feedback matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <a class="reference internal" href="#networks.credit_assignment_functions.NonlinearFunction.forward" title="networks.credit_assignment_functions.NonlinearFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">NonlinearFunction.forward()</span></code></a>.</p></li>
<li><p><strong>B</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The feedback connection matrices.</p></li>
<li><p><strong>grad_out</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The gradient of the loss with respect to
the last layer activation. Required to be directly projected
to all upstream layers.</p></li>
<li><p><strong>is_last_layer</strong> (<em>boolean</em>) – Whether this is the last layer of the net.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>See docstring of method <a class="reference internal" href="#networks.credit_assignment_functions.NonlinearFunction.forward" title="networks.credit_assignment_functions.NonlinearFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">NonlinearFunction.forward()</span></code></a>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(…)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="networks.credit_assignment_functions.NonlinearFunction">
<em class="property">class </em><code class="sig-prename descclassname">networks.credit_assignment_functions.</code><code class="sig-name descname">NonlinearFunction</code><a class="reference internal" href="_modules/networks/credit_assignment_functions.html#NonlinearFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.credit_assignment_functions.NonlinearFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<p>Implementation of a fully-connected layer with activation function.</p>
<p>This class is a <code class="docutils literal notranslate"><span class="pre">Function</span></code> that behaves similar to PyTorch’s class
<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Linear.html#torch.nn.Linear" title="(in PyTorch v1.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code></a>, but it has a different backward function that
includes the non-linearity already. Sincethis class implements the interface
<a class="reference external" href="https://pytorch.org/docs/master/autograd.html#torch.autograd.Function" title="(in PyTorch v1.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>, we can use it to specify a custom
backpropagation behavior.</p>
<p>Assuming column vectors: layer input <img class="math" src="_images/math/76a7ef556c29ca69bfa914686b1019d12a09f3a8.png" alt="\mathbf{a} \in \mathbb{R}^M"/>,
bias vector <img class="math" src="_images/math/3ecb2f0a9ce0de89e62716e6e0253e040bacda0d.png" alt="\mathbf{b} \in \mathbb{R}^N"/> and a weight matrix
<img class="math" src="_images/math/53cac5cfba90a3a4521c5900779d18bca32a3017.png" alt="W \in \mathbb{R}^{N \times M}"/>, this layer simply computes</p>
<div class="math" id="equation-eq-single-sample">
<p><span class="eqno">(1)<a class="headerlink" href="#equation-eq-single-sample" title="Permalink to this equation">¶</a></span><img src="_images/math/4a9ac7236f1bd2e1a74d2e0564e26755e1a6b9f7.png" alt="\mathbf{z} = \sigma(W \mathbf{a} + \mathbf{b})"/></p>
</div><p>(or <img class="math" src="_images/math/0465fe35317932ebc0a348efddbd287983477b57.png" alt="\mathbf{z} = \sigma W \mathbf{a})"/> if <img class="math" src="_images/math/8a01cab30456eeff111a6985d01f7935c84d2731.png" alt="\mathbf{b}"/> is
<code class="docutils literal notranslate"><span class="pre">None</span></code>), where <img class="math" src="_images/math/b52df27bfb0b1e3af0c2c68a7b9da459178c2a7d.png" alt="\sigma"/> is the nonlinearity..</p>
<p>The mathematical operation described for single samples in eq.
<a class="reference internal" href="#equation-eq-single-sample">(1)</a>, is stated for the case of mini-batches below</p>
<div class="math" id="equation-eq-mini-batch">
<p><span class="eqno">(2)<a class="headerlink" href="#equation-eq-mini-batch" title="Permalink to this equation">¶</a></span><img src="_images/math/6bc5b814d6431dbd5970791244ecf8b6914ba45e.png" alt="Z = \sigma (A W^T + \tilde{B})"/></p>
</div><p>where <img class="math" src="_images/math/04fe8b412b9f8aa0722605a53f7ae69e7ea94e38.png" alt="Z \in \mathbb{R}^{B \times N}"/> is the output matrix.</p>
<dl class="method">
<dt id="networks.credit_assignment_functions.NonlinearFunction.backward">
<em class="property">static </em><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">grad_Z</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/credit_assignment_functions.html#NonlinearFunction.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.credit_assignment_functions.NonlinearFunction.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backpropagate the gradients of <img class="math" src="_images/math/95f028ab2b20b895fa12d986e0d9f40f7b6e52d3.png" alt="Z"/> through this layer.</p>
<p>The matrix <code class="docutils literal notranslate"><span class="pre">grad_Z</span></code>, which we denote by
<img class="math" src="_images/math/4b23c2ee60e00adaa1d48db1cddfdfc2ca2b3c0d.png" alt="\delta_Z \in \mathbb{R}^{B \times N}"/>, contains the partial
derivatives of the scalar loss function with respect to each element
from the <a class="reference internal" href="#networks.credit_assignment_functions.NonlinearFunction.forward" title="networks.credit_assignment_functions.NonlinearFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> output matrix <img class="math" src="_images/math/95f028ab2b20b895fa12d986e0d9f40f7b6e52d3.png" alt="Z"/>.</p>
<p>This method backpropagates the global error (encoded in
<img class="math" src="_images/math/df6d8b87a81df9c87eec08cb28c85b7fd9516d61.png" alt="\delta_Z"/>) to all input tensors of the <a class="reference internal" href="#networks.credit_assignment_functions.NonlinearFunction.forward" title="networks.credit_assignment_functions.NonlinearFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method,
essentially computing <img class="math" src="_images/math/e181daac991331622fc92f73422aad2fa21228a8.png" alt="\delta_A"/>, <img class="math" src="_images/math/9b05767c67d84c42c030011d1a3cb2870ec8e99d.png" alt="\delta_W"/>,
<img class="math" src="_images/math/7bb9e6000c409f7455fd0d9b0f8732673985a53e.png" alt="\delta_\mathbf{b}"/>.</p>
<p>For the linear component, these partial derivatives can be computed as
follows:</p>
<div class="math">
<p><img src="_images/math/8f293b635b3d6b000b0ceabd50235f39bc568523.png" alt="\delta_A &amp;= \delta_Z W \\
\delta_W &amp;= \delta_Z^T A \\
\delta_\mathbf{b} &amp;= \sum_{b=1}^B \delta_{Z_{b,:}}"/></p>
</div><p>where <img class="math" src="_images/math/960faea30dd56ccf9a18b07a58e8a81f7cd60ac4.png" alt="\delta_{Z_{b,:}}"/> denotes the vector retrieved from the
<img class="math" src="_images/math/68c7c8c65602677ab56cf7fd88002023f0edc575.png" alt="b"/>-th row of <img class="math" src="_images/math/df6d8b87a81df9c87eec08cb28c85b7fd9516d61.png" alt="\delta_Z"/>.</p>
<p>These need to be multiplied by the derivative of the post-nonlinearity
with respect to its input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> – See description of argument <code class="docutils literal notranslate"><span class="pre">ctx</span></code> of method <a class="reference internal" href="#networks.credit_assignment_functions.NonlinearFunction.forward" title="networks.credit_assignment_functions.NonlinearFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>.</p></li>
<li><p><strong>grad_Z</strong> – The backpropagated error <img class="math" src="_images/math/df6d8b87a81df9c87eec08cb28c85b7fd9516d61.png" alt="\delta_Z"/>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Tuple containing:</p>
<ul class="simple">
<li><p><strong>grad_A</strong>: The derivative of the loss with respect to the input
activations, i.e., <img class="math" src="_images/math/e181daac991331622fc92f73422aad2fa21228a8.png" alt="\delta_A"/>.</p></li>
<li><p><strong>grad_W</strong>: The derivative of the loss with respect to the weight
matrix, i.e., <img class="math" src="_images/math/9b05767c67d84c42c030011d1a3cb2870ec8e99d.png" alt="\delta_W"/>.</p></li>
<li><p><strong>grad_nonlinearity</strong>: Which is always <cite>None</cite>.</p></li>
<li><p><strong>grad_b</strong>: The derivative of the loss with respect to the bias
vector, i.e., <img class="math" src="_images/math/7bb9e6000c409f7455fd0d9b0f8732673985a53e.png" alt="\delta_\mathbf{b}"/>; or <code class="docutils literal notranslate"><span class="pre">None</span></code> if <code class="docutils literal notranslate"><span class="pre">b</span></code> was
passed as <code class="docutils literal notranslate"><span class="pre">None</span></code> to the <a class="reference internal" href="#networks.credit_assignment_functions.NonlinearFunction.forward" title="networks.credit_assignment_functions.NonlinearFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Gradients for input tensors are only computed if their keyword
<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">None</span></code> is
returned for the corresponding Tensor.</p>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)">tuple</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="networks.credit_assignment_functions.NonlinearFunction.forward">
<em class="property">static </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">A</em>, <em class="sig-param">W</em>, <em class="sig-param">nonlinearity</em>, <em class="sig-param">b=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/credit_assignment_functions.html#NonlinearFunction.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.credit_assignment_functions.NonlinearFunction.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the output of a non-linear layer.</p>
<p>This method implements eq. <a class="reference internal" href="#equation-eq-mini-batch">(2)</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> – A context. Should be used to store activations which are needed
in the backward pass.</p></li>
<li><p><strong>A</strong> – A mini-batch of input activations <img class="math" src="_images/math/211284f68205c3e66773eaf026f32a0acdd3dfb3.png" alt="A"/>.</p></li>
<li><p><strong>W</strong> – The weight matrix <img class="math" src="_images/math/1fbee781f84569077719a167b64e12064360fac1.png" alt="W"/>.</p></li>
<li><p><strong>nonlinearity</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The name of the nonlinearity to be used.</p></li>
<li><p><strong>b</strong> (<em>optional</em>) – The bias vector <img class="math" src="_images/math/8a01cab30456eeff111a6985d01f7935c84d2731.png" alt="\mathbf{b}"/>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The output activations <img class="math" src="_images/math/95f028ab2b20b895fa12d986e0d9f40f7b6e52d3.png" alt="Z"/> as defined by eq.
<a class="reference internal" href="#equation-eq-mini-batch">(2)</a>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="networks.credit_assignment_functions.non_linear_dfa_function">
<code class="sig-prename descclassname">networks.credit_assignment_functions.</code><code class="sig-name descname">non_linear_dfa_function</code><span class="sig-paren">(</span><em class="sig-param">A</em>, <em class="sig-param">W</em>, <em class="sig-param">B</em>, <em class="sig-param">grad_out</em>, <em class="sig-param">is_last_layer=False</em>, <em class="sig-param">b=None</em>, <em class="sig-param">nonlinearity='linear'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/credit_assignment_functions.html#non_linear_dfa_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.credit_assignment_functions.non_linear_dfa_function" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for using class <a class="reference internal" href="#networks.credit_assignment_functions.DFANonlinearFunction" title="networks.credit_assignment_functions.DFANonlinearFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">DFANonlinearFunction</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <a class="reference internal" href="#networks.credit_assignment_functions.NonlinearFunction.forward" title="networks.credit_assignment_functions.NonlinearFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">NonlinearFunction.forward()</span></code></a>.</p></li>
<li><p><strong>grad_out</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – The loss gradient of the last layer.</p></li>
<li><p><strong>is_last_layer</strong> (<em>boolean</em>) – Whether this is the last layer.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="networks.credit_assignment_functions.non_linear_function">
<code class="sig-prename descclassname">networks.credit_assignment_functions.</code><code class="sig-name descname">non_linear_function</code><span class="sig-paren">(</span><em class="sig-param">A</em>, <em class="sig-param">W</em>, <em class="sig-param">b=None</em>, <em class="sig-param">nonlinearity='linear'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/credit_assignment_functions.html#non_linear_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.credit_assignment_functions.non_linear_function" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for using class <a class="reference internal" href="#networks.credit_assignment_functions.NonlinearFunction" title="networks.credit_assignment_functions.NonlinearFunction"><code class="xref py py-class docutils literal notranslate"><span class="pre">NonlinearFunction</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(</strong><strong>...</strong><strong>)</strong> – See docstring of method <a class="reference internal" href="#networks.credit_assignment_functions.NonlinearFunction.forward" title="networks.credit_assignment_functions.NonlinearFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">NonlinearFunction.forward()</span></code></a>.</p></li>
<li><p><strong>nonlinearity</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The nonlinearity to be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<span class="target" id="module-networks.net_utils"></span><div class="section" id="helper-functions-for-generating-different-networks">
<h3><a class="toc-backref" href="#id13">Helper functions for generating different networks</a><a class="headerlink" href="#helper-functions-for-generating-different-networks" title="Permalink to this headline">¶</a></h3>
<p>A collection of helper functions for generating networks to keep other scripts
clean.</p>
<dl class="function">
<dt id="networks.net_utils.generate_network">
<code class="sig-prename descclassname">networks.net_utils.</code><code class="sig-name descname">generate_network</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">dataset</em>, <em class="sig-param">device</em>, <em class="sig-param">network_type='BP'</em>, <em class="sig-param">classification=True</em>, <em class="sig-param">logger=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/net_utils.html#generate_network"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.net_utils.generate_network" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the network based on the provided command line arguments.</p>
<dl class="simple">
<dt>config:</dt><dd><p>config: Command-line arguments.
dataset: The dataset being used.
device: The cuda device.
network_type (str): The type of network.
classification (boolean): Whether the task is a classification task.
logger: The logger. If <cite>None</cite> nothing will be logged.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The network.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="networks.net_utils.log_net_details">
<code class="sig-prename descclassname">networks.net_utils.</code><code class="sig-name descname">log_net_details</code><span class="sig-paren">(</span><em class="sig-param">logger</em>, <em class="sig-param">net</em>, <em class="sig-param">network_type=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/net_utils.html#log_net_details"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.net_utils.log_net_details" title="Permalink to this definition">¶</a></dt>
<dd><p>Log the architecture of the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logger</strong> – The logger.</p></li>
<li><p><strong>net</strong> – The network.</p></li>
<li><p><strong>network_type</strong> – The type of network.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<span class="target" id="module-networks.dfc_network_utils"></span><div class="section" id="script-with-helper-functions-for-deep-feedback-control-computations">
<h3><a class="toc-backref" href="#id14">Script with helper functions for Deep Feedback Control computations</a><a class="headerlink" href="#script-with-helper-functions-for-deep-feedback-control-computations" title="Permalink to this headline">¶</a></h3>
<p>This module contains several helper functions for training with DFC.</p>
<dl class="function">
<dt id="networks.dfc_network_utils.loss_function_H">
<code class="sig-prename descclassname">networks.dfc_network_utils.</code><code class="sig-name descname">loss_function_H</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">net</em>, <em class="sig-param">shared</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network_utils.html#loss_function_H"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network_utils.loss_function_H" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute surrogate <img class="math" src="_images/math/f6debfa5617fe793770015b967c5d5ab70ad9182.png" alt="\mathcal{H}"/> loss on the last batch.</p>
<p>This loss corresponds to the norm of the total amount of help, computed
as <img class="math" src="_images/math/bba82ebbda9be9e8e488468a9e57a442a0058bc3.png" alt="||Q\mathbf{u}||^2"/>, normalized by the batch size and the
number of neurons.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> – The config.</p></li>
<li><p><strong>net</strong> – The network.</p></li>
<li><p><strong>shared</strong> – The shared subspace.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The normalized <img class="math" src="_images/math/f6debfa5617fe793770015b967c5d5ab70ad9182.png" alt="\mathcal{H}"/> loss.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)">float</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="networks.dfc_network_utils.save_angles">
<code class="sig-prename descclassname">networks.dfc_network_utils.</code><code class="sig-name descname">save_angles</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">writer</em>, <em class="sig-param">step</em>, <em class="sig-param">net</em>, <em class="sig-param">loss</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network_utils.html#save_angles"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network_utils.save_angles" title="Permalink to this definition">¶</a></dt>
<dd><p>Save logs and plots for the current mini-batch on tensorboard.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<em>Namespace</em>) – The config.</p></li>
<li><p><strong>writer</strong> (<em>SummaryWriter</em>) – TensorboardX summary writer</p></li>
<li><p><strong>step</strong> – global step</p></li>
<li><p><strong>net</strong> (<em>networks.DTPNetwork</em>) – network</p></li>
<li><p><strong>loss</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – loss of the current minibatch.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="networks.dfc_network_utils.train_epoch_feedback">
<code class="sig-prename descclassname">networks.dfc_network_utils.</code><code class="sig-name descname">train_epoch_feedback</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">logger</em>, <em class="sig-param">writer</em>, <em class="sig-param">dloader</em>, <em class="sig-param">optimizer</em>, <em class="sig-param">net</em>, <em class="sig-param">shared</em>, <em class="sig-param">loss_function</em>, <em class="sig-param">epoch=None</em>, <em class="sig-param">pretraining=False</em>, <em class="sig-param">compute_gn_condition=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network_utils.html#train_epoch_feedback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network_utils.train_epoch_feedback" title="Permalink to this definition">¶</a></dt>
<dd><p>Train the feedback parameters for one epoch.</p>
<p>For each mini-batch in the training set, this function:</p>
<ul class="simple">
<li><p>computes the forward pass</p></li>
<li><p>sets the feedback gradients to zero and computes the gradients</p></li>
<li><p>clips the feedback gradients if necessary</p></li>
<li><p>updates the feedback weights</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> – The command-line config.</p></li>
<li><p><strong>logger</strong> – The logger.</p></li>
<li><p><strong>writer</strong> – The writer.</p></li>
<li><p><strong>dloader</strong> – The data loader.</p></li>
<li><p><strong>optimizer</strong> – The feedback optimizer.</p></li>
<li><p><strong>net</strong> – The network.</p></li>
<li><p><strong>shared</strong> – The Namespace containing important training information.</p></li>
<li><p><strong>loss_function</strong> – The loss function.</p></li>
<li><p><strong>epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The current epoch.</p></li>
<li><p><strong>pretraining</strong> (<em>boolean</em>) – Whether the call is for pretraining or not.</p></li>
<li><p><strong>compute_gn_condition</strong> (<em>boolean</em>) – Whether to compute the gn condition
during this epoch or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="networks.dfc_network_utils.train_feedback_parameters">
<code class="sig-prename descclassname">networks.dfc_network_utils.</code><code class="sig-name descname">train_feedback_parameters</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">logger</em>, <em class="sig-param">writer</em>, <em class="sig-param">device</em>, <em class="sig-param">dloader</em>, <em class="sig-param">net</em>, <em class="sig-param">optimizers</em>, <em class="sig-param">shared</em>, <em class="sig-param">loss_function</em>, <em class="sig-param">pretraining=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/networks/dfc_network_utils.html#train_feedback_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#networks.dfc_network_utils.train_feedback_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Train the feedback weights.</p>
<p>This function is called either to perform further training of feedback
weights after each epoch of forward parameter training, or as a pre-training
to initialize the network in a ‘pseudo-inverse’ condition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<em>Namespace</em>) – The command-line arguments.</p></li>
<li><p><strong>logger</strong> – The logger.</p></li>
<li><p><strong>writer</strong> – The writer.</p></li>
<li><p><strong>dloader</strong> – The dataset.</p></li>
<li><p><strong>net</strong> – The neural network.</p></li>
<li><p><strong>optimizers</strong> – The optimizers.</p></li>
<li><p><strong>shared</strong> – The Namespace containing important training information.</p></li>
<li><p><strong>loss_function</strong> – The loss function.</p></li>
<li><p><strong>pretraining</strong> (<em>boolean</em>) – Whether the call is for pretraining or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">dfc</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents of the repository:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="main.html">Main script to run experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="main.html#reproducibility">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="datahandlers.html">Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#general-notes">General Notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-networks.network_interface">API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Utilities</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="datahandlers.html" title="previous chapter">Data</a></li>
      <li>Next: <a href="utils.html" title="next chapter">Utilities</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Alexander Meulemans, Matilde Tristany Farinha, Maria R. Cervera.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="_sources/networks.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>