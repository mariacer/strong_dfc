
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>networks.dfc_network &#8212; dfc 0.1 documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for networks.dfc_network</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="c1"># Copyright 2021 Alexander Meulemans, Matilde Tristany, Maria Cervera</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="c1"># @title          :networks/dfc_network.py</span>
<span class="c1"># @author         :mc</span>
<span class="c1"># @contact        :mariacer@ethz.ch</span>
<span class="c1"># @created        :28/11/2021</span>
<span class="c1"># @version        :1.0</span>
<span class="c1"># @python_version :3.7.4</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Implementation of a network for Deep Feedback Control</span>
<span class="sd">-----------------------------------------------------</span>

<span class="sd">A network that is prepared to be trained with DFC.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">networks.network_interface</span> <span class="k">import</span> <span class="n">NetworkInterface</span>
<span class="kn">from</span> <span class="nn">networks.dfc_layer</span> <span class="k">import</span> <span class="n">DFCLayer</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="k">import</span> <span class="n">math_utils</span> <span class="k">as</span> <span class="n">mutils</span>

<div class="viewcode-block" id="DFCNetwork"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork">[docs]</a><span class="k">class</span> <span class="nc">DFCNetwork</span><span class="p">(</span><span class="n">NetworkInterface</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implementation of a network for Deep Feedback Control.</span>

<span class="sd">    Note that forward and feedback weights are learned in two different phases</span>
<span class="sd">    that we call the wake and sleep phases, respectively.</span>

<span class="sd">    It contains the following important functions:</span>

<span class="sd">    * ``forward``: compute the forward pass across all layers.</span>
<span class="sd">    * ``backward``: compute the backward phase with controller on, and compute</span>
<span class="sd">      the gradients of the forward weights.</span>
<span class="sd">    * ``compute_feedback_gradients``: compute the gradients of feedback weights.</span>
<span class="sd">    * ``controller``: a proportional integral controller.</span>

<span class="sd">    In this two-phase DFC setting, the following options exist for defining the</span>
<span class="sd">    target activations. For forward weight learning, the target outputs are</span>
<span class="sd">    either feedforward activations nudged towards lower loss (default) or set as</span>
<span class="sd">    the actual supervised targets (if the option ``strong_feedback`` is active).</span>
<span class="sd">    For feedback weight learning, the target outputs are either the feedforward</span>
<span class="sd">    activations (default) or the supervised targets (if the option</span>
<span class="sd">    ``strong_feedback`` is active).</span>

<span class="sd">    Args:</span>
<span class="sd">        (....): See docstring of class</span>
<span class="sd">            :class:`network_interface.NetworkInterface`.</span>
<span class="sd">        initialization_fb (str): The initialization for feedback weights.</span>
<span class="sd">        ndi (boolean): Whether to compute the non-dynamical inversion, i.e. the</span>
<span class="sd">            analytical solution at steady-state instead of simulating the</span>
<span class="sd">            dynamics.</span>
<span class="sd">        cont_updates (boolean): Whether the forward weights are updated at</span>
<span class="sd">            steady-state or not. Feedback weights are always computed</span>
<span class="sd">            continuously.</span>
<span class="sd">        sigma: Std of Gaussian noise to corrupt activations during controller</span>
<span class="sd">            dynamics.</span>
<span class="sd">        sigma_output: Same as `sigma` but for output layer.</span>
<span class="sd">        sigma_fb: Same as &quot;sigma&quot; but for the sleep phase.</span>
<span class="sd">        sigma_output_fb: Same as `sigma_fb` but for output layer.</span>
<span class="sd">        sigma_init (float): Std for assing Gaussian noise to feedback weight</span>
<span class="sd">            initialization for &quot;weight_product&quot; inits.</span>
<span class="sd">        dt_di (float): The timestep to be used in the differential equations.</span>
<span class="sd">        dt_di_fb (float): Same as &quot;dt_di&quot; but for the sleep phase.</span>
<span class="sd">        alpha_di (float): Leakage gain of the feedback controller.</span>
<span class="sd">        alpha_di_fb (float): Same as &quot;alpha&quot; but for the sleep phase.</span>
<span class="sd">        tmax_di (float): Maximum number of iterations (timesteps) performed in</span>
<span class="sd">            the dynamical inversion of targets.</span>
<span class="sd">        tmax_di_fb (float): Same as &quot;tmax_di&quot; but for the sleep phase.</span>
<span class="sd">        epsilon_di (float): Constant to check for convergence.</span>
<span class="sd">        k_p (float): The gain factor of the proportional control.</span>
<span class="sd">        k_p_fb (float): Same as &quot;k_p&quot; but for the sleep phase.</span>
<span class="sd">        time_constant_ratio (float): Ratio of the time constant of the voltage</span>
<span class="sd">            dynamics w.r.t. the controller dynamics.</span>
<span class="sd">        time_constant_ratio_fb (float): Same as &quot;time_constant_ratio&quot; but for</span>
<span class="sd">            the sleep phase.</span>
<span class="sd">        inst_transmission (bool): Whether to assume an instantaneous</span>
<span class="sd">            transmission between layers.</span>
<span class="sd">        inst_transmission_fb (bool): Same as &quot;inst_transmission&quot; but for the</span>
<span class="sd">            sleep phase.</span>
<span class="sd">        apical_time_constant (float): Time constant of the apical compartment.</span>
<span class="sd">        apical_time_constant_fb (float): Same as &quot;apical_time_constant&quot; but for</span>
<span class="sd">            the sleep phase.</span>
<span class="sd">        inst_system_dynamics (bool): Whether the dynamics of the somatic</span>
<span class="sd">            compartments, should be approximated by their instantaneous</span>
<span class="sd">            counterparts.</span>
<span class="sd">        inst_apical_dynamics (bool): Whether the dyamics of the apical</span>
<span class="sd">            compartiment should be instantneous.</span>
<span class="sd">        proactive_controller (bool): Whether to use the teaching signal of the</span>
<span class="sd">            next time step for simulating the controller dynamics. </span>
<span class="sd">        noisy_dynamics (bool): Whether dynamics should be corrupted with noise</span>
<span class="sd">            with std &quot;sigma&quot; or &quot;sigma_fb&quot;.</span>
<span class="sd">        target_stepsize (float): Step size for computing the output target based</span>
<span class="sd">            on the output gradient.</span>
<span class="sd">        tau_f (float): The time constant for filtering the dynamics and the</span>
<span class="sd">            control signal.</span>
<span class="sd">        tau_noise (float): The time constant to filter the noise.</span>
<span class="sd">        forward_requires_grad (bool): Whether the forward pass requires autograd</span>
<span class="sd">            to compute gradients.</span>
<span class="sd">        include_non_converged_samples (bool): Whether samples that have not</span>
<span class="sd">            converged should be excluded.</span>
<span class="sd">        compare_with_ndi (bool): Whether the dynamical inversion results should</span>
<span class="sd">            be compared to the analytical solution. Should only ever be active</span>
<span class="sd">            if `ndi` is `False`.</span>
<span class="sd">        low_pass_filter_u (bool): Whether the control signal should be low-pass</span>
<span class="sd">            filtered.</span>
<span class="sd">        low_pass_filter_noise (bool): Whether the noise should be low-pass</span>
<span class="sd">            filtered. Only relevant if `noisy_dynamics==True`.</span>
<span class="sd">        use_jacobian_as_fb (bool): Whether to use the Jacobian for the</span>
<span class="sd">            feedback weights.</span>
<span class="sd">        save_ndi_updates (bool): Whether angle with the analytical updates</span>
<span class="sd">            should be computed. Causes a minor increase in computational load.</span>
<span class="sd">        save_df (bool): Whether angles should be stored in a dataframe.</span>
<span class="sd">        strong_feedback (bool): Whether the feedback should be strong. In this</span>
<span class="sd">            case, the outputs are not simply nudged but clamped to the desired</span>
<span class="sd">            values. In this setting, the linearization becomes very inaccurate,</span>
<span class="sd">            so it does not make sense to use an analytical solution, so</span>
<span class="sd">            `ndi` should be `False`.</span>
<span class="sd">        compute_jacobian_at (str): How to compute the Jacobian.</span>
<span class="sd">        scaling_fb_updates (bool): Whether to scale the feedback updates</span>
<span class="sd">            differently for different layers.</span>
<span class="sd">        learning_rule (str): The type of learning rule to use for the forward</span>
<span class="sd">            weights.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">initialization</span><span class="o">=</span><span class="s1">&#39;xavier_normal&#39;</span><span class="p">,</span> <span class="n">cont_updates</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">initialization_fb</span><span class="o">=</span><span class="s1">&#39;weight_product&#39;</span><span class="p">,</span> <span class="n">ndi</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">sigma</span><span class="o">=</span><span class="mf">0.36</span><span class="p">,</span> <span class="n">sigma_fb</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">sigma_output</span><span class="o">=</span><span class="mf">0.36</span><span class="p">,</span>
                 <span class="n">sigma_output_fb</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">sigma_init</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">dt_di</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">dt_di_fb</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">alpha_di</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">alpha_di_fb</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">tmax_di</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">tmax_di_fb</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">k_p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">k_p_fb</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">epsilon_di</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                 <span class="n">time_constant_ratio</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">time_constant_ratio_fb</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span>
                 <span class="n">inst_transmission</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inst_transmission_fb</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">apical_time_constant</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">apical_time_constant_fb</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                 <span class="n">inst_system_dynamics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inst_apical_dynamics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">proactive_controller</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">noisy_dynamics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">target_stepsize</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">tau_f</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">tau_noise</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                 <span class="n">forward_requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">include_non_converged_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">compare_with_ndi</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">low_pass_filter_u</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">low_pass_filter_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">use_jacobian_as_fb</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">save_ndi_updates</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">save_df</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strong_feedback</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">compute_jacobian_at</span><span class="o">=</span><span class="s1">&#39;full_trajectory&#39;</span><span class="p">,</span>
                 <span class="n">freeze_fb_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">scaling_fb_updates</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">learning_rule</span><span class="o">=</span><span class="s1">&#39;nonlinear_difference&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                         <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">initialization</span><span class="o">=</span><span class="n">initialization</span><span class="p">,</span>
                         <span class="n">initialization_fb</span><span class="o">=</span><span class="n">initialization_fb</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_input</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialization_fb</span> <span class="o">=</span> <span class="n">initialization_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ndi</span> <span class="o">=</span> <span class="n">ndi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cont_updates</span> <span class="o">=</span> <span class="n">cont_updates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_di</span> <span class="o">=</span> <span class="n">epsilon_di</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sigma_output</span> <span class="o">=</span> <span class="n">sigma_output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sigma_fb</span> <span class="o">=</span> <span class="n">sigma_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sigma_output_fb</span> <span class="o">=</span> <span class="n">sigma_output_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sigma_init</span> <span class="o">=</span> <span class="n">sigma_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dt_di</span> <span class="o">=</span> <span class="n">dt_di</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dt_di_fb</span> <span class="o">=</span> <span class="n">dt_di_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha_di</span> <span class="o">=</span> <span class="n">alpha_di</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha_di_fb</span> <span class="o">=</span> <span class="n">alpha_di_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tmax_di</span> <span class="o">=</span> <span class="n">tmax_di</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tmax_di_fb</span> <span class="o">=</span> <span class="n">tmax_di_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_k_p</span> <span class="o">=</span> <span class="n">k_p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_k_p_fb</span> <span class="o">=</span> <span class="n">k_p_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_time_constant_ratio</span> <span class="o">=</span> <span class="n">time_constant_ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_time_constant_ratio_fb</span> <span class="o">=</span> <span class="n">time_constant_ratio_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_inst_transmission</span> <span class="o">=</span> <span class="n">inst_transmission</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_inst_transmission_fb</span> <span class="o">=</span> <span class="n">inst_transmission_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_apical_time_constant</span> <span class="o">=</span> <span class="n">apical_time_constant</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_apical_time_constant_fb</span> <span class="o">=</span> <span class="n">apical_time_constant_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_inst_system_dynamics</span> <span class="o">=</span> <span class="n">inst_system_dynamics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_inst_apical_dynamics</span> <span class="o">=</span> <span class="n">inst_apical_dynamics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_proactive_controller</span> <span class="o">=</span> <span class="n">proactive_controller</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_noisy_dynamics</span> <span class="o">=</span> <span class="n">noisy_dynamics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_stepsize</span> <span class="o">=</span> <span class="n">target_stepsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tau_f</span> <span class="o">=</span> <span class="n">tau_f</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tau_noise</span> <span class="o">=</span> <span class="n">tau_noise</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forward_requires_grad</span> <span class="o">=</span> <span class="n">forward_requires_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_include_non_converged_samples</span> <span class="o">=</span> <span class="n">include_non_converged_samples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compare_with_ndi</span> <span class="o">=</span> <span class="n">compare_with_ndi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_low_pass_filter_u</span> <span class="o">=</span> <span class="n">low_pass_filter_u</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_low_pass_filter_noise</span> <span class="o">=</span> <span class="n">low_pass_filter_noise</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_jacobian_as_fb</span> <span class="o">=</span> <span class="n">use_jacobian_as_fb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_strong_feedback</span> <span class="o">=</span> <span class="n">strong_feedback</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compute_jacobian_at</span> <span class="o">=</span> <span class="n">compute_jacobian_at</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_freeze_fb_weights</span> <span class="o">=</span> <span class="n">freeze_fb_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scaling_fb_updates</span> <span class="o">=</span> <span class="n">scaling_fb_updates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learning_rule</span> <span class="o">=</span> <span class="n">learning_rule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_ndi_updates</span> <span class="o">=</span> <span class="n">save_ndi_updates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_df</span> <span class="o">=</span> <span class="n">save_df</span>
        <span class="k">if</span> <span class="n">compare_with_ndi</span><span class="p">:</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">ndi</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rel_dist_to_ndi</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">strong_feedback</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndi</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The analytical inversion is not applicable to &#39;</span>
                             <span class="s1">&#39;settings with strong feedback as the &#39;</span>
                             <span class="s1">&#39;linearization becomes inaccurate.&#39;</span><span class="p">)</span>

        <span class="c1"># This option is always initialized by default as MSE, and might be</span>
        <span class="c1"># overwritten to cross-entropy after the loss function has been defined.</span>
        <span class="c1"># This is used to compute the error based on the gradient of the loss.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_function_name</span> <span class="o">=</span> <span class="s1">&#39;mse&#39;</span>

        <span class="c1"># Overwrite feedback weight initialization if necessary (up to output).</span>
        <span class="k">if</span> <span class="n">initialization_fb</span> <span class="o">==</span> <span class="s1">&#39;weight_product&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_feedback_layers_weight_product</span><span class="p">()</span>
        <span class="c1"># Overwrite last feedback layer to be the identity.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weights_backward</span> <span class="o">=</span> \
            <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weights_backward</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> 

        <span class="c1"># If the homeostatic constant is -1, set it to the square of the</span>
        <span class="c1"># frobenius norm of the initialized feedback weights.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_homeostatic_const</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">full_Q</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">ndi</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">converged_samples_per_epoch</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">diverged_samples_per_epoch</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">not_converged_samples_per_epoch</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">save_df</span><span class="p">:</span>
            <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_depth</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ndi_angles_network</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lu_angles_network</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ratio_ff_fb_network</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">condition_gn</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">condition_gn_init</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ndi_angles</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d</span><span class="p">)])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bp_angles</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d</span><span class="p">)])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lu_angles</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d</span><span class="p">)])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ratio_ff_fb</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d</span><span class="p">)])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;DFCNetwork&#39;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">layer_class</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Define the layer type to be used.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">DFCLayer</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">r</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for attribute targets.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_r</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for attribute input.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input</span>

    <span class="nd">@input</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Setter for attribute input.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ndi</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`ndi`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ndi</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cont_updates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`cont_updates`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cont_updates</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">epsilon_di</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`epsilon_di`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_di</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sigma</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`sigma`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigma</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sigma_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`sigma_fb`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigma_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sigma_init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`_sigma_init`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigma_init</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sigma_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`sigma_output`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigma_output</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sigma_output_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`sigma_output_fb`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigma_output_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">initialization_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`initialization_fb`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialization_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">alpha_di</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`alpha_di`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha_di</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">alpha_di_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`alpha_di_fb`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha_di_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dt_di</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`dt_di`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dt_di</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dt_di_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`dt_di_fb`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dt_di_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tmax_di</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`tmax_di`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tmax_di</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tmax_di_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`tmax_di_fb`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tmax_di_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">k_p</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`k_p`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_p</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">k_p_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`k_p_fb`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k_p_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">inst_system_dynamics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`inst_system_dynamics`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inst_system_dynamics</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">inst_apical_dynamics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`inst_apical_dynamics`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inst_apical_dynamics</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">noisy_dynamics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Getter for read-only attribute :attr:`noisy_dynamics`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_noisy_dynamics</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">inst_transmission</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`inst_transmission`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inst_transmission</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">inst_transmission_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`inst_transmission_fb`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inst_transmission_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">time_constant_ratio</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`time_constant_ratio`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_time_constant_ratio</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">time_constant_ratio_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`time_constant_ratio_fb`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_time_constant_ratio_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">apical_time_constant</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`apical_time_constant`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apical_time_constant</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">apical_time_constant_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`apical_time_constant_fb`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apical_time_constant_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">proactive_controller</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`proactive_controller`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proactive_controller</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">target_stepsize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`target_stepsize`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_stepsize</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">forward_requires_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute forward_requires_grad&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_requires_grad</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">include_non_converged_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute </span>
<span class="sd">        :attr:`include_non_converged_samples`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_include_non_converged_samples</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">low_pass_filter_u</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`low_pass_filter_u`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_low_pass_filter_u</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">low_pass_filter_noise</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`_low_pass_filter_noise`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_low_pass_filter_noise</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tau_f</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`tau_f`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tau_f</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tau_noise</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`tau_noise`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tau_noise</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">use_jacobian_as_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`use_jacobian_as_fb`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_jacobian_as_fb</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">strong_feedback</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`strong_feedback`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strong_feedback</span>

    <span class="nd">@property</span>

    <span class="k">def</span> <span class="nf">compute_jacobian_at</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`compute_jacobian_at`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_jacobian_at</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">freeze_fb_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`freeze_fb_weights`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_freeze_fb_weights</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">scaling_fb_updates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`scaling_fb_updates`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scaling_fb_updates</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">learning_rule</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`learning_rule`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learning_rule</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">loss_function_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`loss_function_name`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_function_name</span>

    <span class="nd">@loss_function_name</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">loss_function_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Setter for loss_function_name&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_function_name</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">u</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Getter for read-only attribute :attr:`u`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_u</span>

    <span class="nd">@u</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">u</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Setter for u&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_u</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">full_Q</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Getter for matrix :math:`\bar{Q}` containing the concatenated</span>
<span class="sd">        feedback weights.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">l</span><span class="o">.</span><span class="n">weights_backward</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A dictionary containing the current state of the network,</span>
<span class="sd">        incliding forward and backward weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (dict): The forward and feedback weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">forward_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>
        <span class="n">feedback_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">weights_backward</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>

        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;forward_weights&#39;</span><span class="p">:</span> <span class="n">forward_weights</span><span class="p">,</span>
                      <span class="s1">&#39;feedback_weights&#39;</span><span class="p">:</span> <span class="n">feedback_weights</span><span class="p">}</span>

        <span class="k">return</span> <span class="n">state_dict</span>

<div class="viewcode-block" id="DFCNetwork.load_state_dict"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.load_state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Load a state into the network.</span>

<span class="sd">        This function sets the forward and backward weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): The state with forward and backward weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">):</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;forward_weights&#39;</span><span class="p">][</span><span class="n">l</span><span class="p">]</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">weights_backward</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;feedback_weights&#39;</span><span class="p">][</span><span class="n">l</span><span class="p">]</span></div>

<div class="viewcode-block" id="DFCNetwork.init_feedback_layers_weight_product"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.init_feedback_layers_weight_product">[docs]</a>    <span class="k">def</span> <span class="nf">init_feedback_layers_weight_product</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the feedback weights to the inversion matrices.</span>

<span class="sd">        The feedback weights will be initializaed to the product of the forward</span>
<span class="sd">        weights (transposed) of subsequent layers.</span>
<span class="sd">        Note that this function can&#39;t be called at the level of inidividual</span>
<span class="sd">        layers since it needs information about the entire network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>

            <span class="c1"># Compute product of forward weights of subsequent layers.</span>
            <span class="c1"># Need to clone the tensor to not point to the same object.</span>
            <span class="n">K</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
                <span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>\
                             <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

            <span class="c1"># Add noise if necessary.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_init</span> <span class="o">&gt;</span> <span class="mf">0.</span><span class="p">:</span>
                <span class="n">K</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma_init</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights_backward</span> <span class="o">=</span> <span class="n">K</span></div>

<div class="viewcode-block" id="DFCNetwork.forward"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the output :math:`y` of this network given the input</span>
<span class="sd">        :math:`x`.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): The input to the network.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The output of the network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span> <span class="c1"># TODO check this</span>
            <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="n">y</span></div>

<div class="viewcode-block" id="DFCNetwork.backward"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_for_fb</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run the feedback phase of the network.</span>

<span class="sd">        In this phase, the network is pushed to the output target by the</span>
<span class="sd">        controller. Compute the update of the forward weights of the network</span>
<span class="sd">        accordingly and save it in ``self.layers[i].weights.grad`` for each</span>
<span class="sd">        layer ``i``.</span>
<span class="sd">        </span>
<span class="sd">        Note: ``backward`` is implemented at the network-level, as it performs</span>
<span class="sd">        simultaneous inversion and control of all targets</span>

<span class="sd">        Args:</span>
<span class="sd">            loss (torch.Tensor): Mean output loss for current mini-batch.</span>
<span class="sd">            targets (torch.Tensor): The dataset targets. This will usually be</span>
<span class="sd">                ignored, as the targets will be taken to be the activations</span>
<span class="sd">                nudged towards lower loss, unless we use strong feedback.</span>
<span class="sd">            return_for_fb (boolean): Whether to return the control signal and</span>
<span class="sd">                the aplical voltages. This is useful when implementing DFC in</span>
<span class="sd">                a single phase, as these values will be needed to compute the</span>
<span class="sd">                feedback gradients.</span>
<span class="sd">            verbose (bool): Whether to display warnings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (torch.Tensor): (Optionally) the controller signal.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_output_target</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

        <span class="c1"># Get all pre- and post-nonlinearity activations.</span>
        <span class="n">v_feedforward</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">linear_activations</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>
        <span class="n">r_feedforward</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">activations</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>

        <span class="c1"># Compute the target activations.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndi</span><span class="p">:</span>
            <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_out</span><span class="p">,</span> <span class="n">delta_v</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">non_dynamical_inversion</span><span class="p">(</span><span class="n">output_target</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_out</span><span class="p">,</span> <span class="n">delta_v</span><span class="p">,</span> \
                <span class="p">(</span><span class="n">u_time</span><span class="p">,</span> <span class="n">v_fb_time</span><span class="p">,</span> <span class="n">v_time</span><span class="p">,</span> <span class="n">v_ff_time</span><span class="p">,</span> <span class="n">r_time</span><span class="p">)</span> <span class="o">=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">dynamical_inversion</span><span class="p">(</span><span class="n">output_target</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compare_with_ndi</span><span class="p">:</span>
                <span class="c1"># Compare analytical solution and compute relative distance.</span>
                <span class="n">u_ndi</span><span class="p">,</span> <span class="n">v_ndi</span><span class="p">,</span> <span class="n">r_ndi</span><span class="p">,</span> <span class="n">r_out_ndi</span><span class="p">,</span> \
                    <span class="n">delta_v_ndi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_dynamical_inversion</span><span class="p">(</span><span class="n">output_target</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rel_dist_to_ndi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>\
                    <span class="n">mutils</span><span class="o">.</span><span class="n">euclidean_dist</span><span class="p">(</span><span class="n">r_out</span><span class="p">,</span> <span class="n">r_out_ndi</span><span class="p">)</span><span class="o">/</span>
                    <span class="n">mutils</span><span class="o">.</span><span class="n">euclidean_dist</span><span class="p">(</span><span class="n">r_feedforward</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">r_out_ndi</span><span class="p">))</span><span class="o">.</span>\
                    <span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

        <span class="c1"># Iterate across all layers.</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
            <span class="n">delta_v_i</span> <span class="o">=</span> <span class="n">delta_v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

            <span class="c1"># Get the activations of the previous layer.</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">r_previous</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">r_previous</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

            <span class="c1"># Compute the forward gradients.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_updates</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">r_previous_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
                        <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tmax_di</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">r_previous_time</span> <span class="o">=</span> <span class="n">r_time</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">compute_forward_gradients_continuous</span><span class="p">(</span><span class="n">v_time</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                        <span class="n">v_ff_time</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">r_previous_time</span><span class="p">,</span>
                        <span class="n">learning_rule</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rule</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">compute_forward_gradients</span><span class="p">(</span><span class="n">delta_v_i</span><span class="p">,</span> <span class="n">r_previous</span><span class="p">,</span>
                            <span class="n">learning_rule</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rule</span><span class="p">)</span>

            <span class="c1"># Store target values.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">u</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_ndi_updates</span><span class="p">:</span>
                <span class="c1"># Compute and save the analytical updates.</span>
                <span class="n">u_ndi</span><span class="p">,</span> <span class="n">v_ndi</span><span class="p">,</span> <span class="n">r_ndi</span><span class="p">,</span> <span class="n">r_out_ndi</span><span class="p">,</span> <span class="n">delta_v_ndi</span> <span class="o">=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">non_dynamical_inversion</span><span class="p">(</span><span class="n">output_target</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">compute_forward_gradients</span><span class="p">(</span><span class="n">delta_v_ndi</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                             <span class="n">r_previous</span><span class="p">,</span>
                                             <span class="n">saving_ndi_updates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                             <span class="n">learning_rule</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rule</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">u</span>

        <span class="k">if</span> <span class="n">return_for_fb</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndi</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">u_time</span><span class="p">,</span> <span class="n">v_fb_time</span></div>

<div class="viewcode-block" id="DFCNetwork.compute_output_target"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.compute_output_target">[docs]</a>    <span class="k">def</span> <span class="nf">compute_output_target</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the output target.</span>

<span class="sd">        The target can be computed in one of two ways:</span>

<span class="sd">        * Nudged activations towards lower loss:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathbf{r}_L^* = \mathbf{r}_L^- - \lambda \frac{\partial \</span>
<span class="sd">                \mathcal{L}}{\partial \mathbf{r}_L} \</span>
<span class="sd">                \bigg\rvert^T_{\mathbf{r}_L = \mathbf{r}_L^-}</span>

<span class="sd">        * As the actual targets (if ``strong_feedback==True``)</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathbf{r}_L^* = \mathbf{r}^\text{true}</span>

<span class="sd">        We assume the loss is averaged across the mini-batch, so here we need</span>
<span class="sd">        to multiply by the batch size to use the total loss over the mini-batch.</span>

<span class="sd">        Args:</span>
<span class="sd">            loss (torch.Tensor): Mean output loss for current mini-batch.</span>
<span class="sd">            targets (torch.Tensor): The dataset targets. This will usually be</span>
<span class="sd">                ignored, as the targets will be taken to be the activations</span>
<span class="sd">                nudged towards lower loss, unless we use strong feedback.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (torch.Tensor): Mini-batch of output targets</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">strong_feedback</span><span class="p">:</span>
            <span class="c1"># Return the targets.</span>
            <span class="n">output_targets</span> <span class="o">=</span> <span class="n">targets</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Return the nudged activations.</span>
            <span class="n">target_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_stepsize</span>
            <span class="n">output_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span>

            <span class="c1"># We multiply loss by batch size to have sum of losses.</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">output_activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># Compute gradient.</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="o">*</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">output_activations</span><span class="p">,</span>
                           <span class="n">retain_graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_requires_grad</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">output_targets</span> <span class="o">=</span> <span class="n">output_activations</span> <span class="o">-</span> <span class="n">target_lr</span> <span class="o">*</span> <span class="n">gradient</span>

        <span class="k">return</span> <span class="n">output_targets</span></div>

<div class="viewcode-block" id="DFCNetwork.dynamical_inversion"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.dynamical_inversion">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">dynamical_inversion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_target</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the dynamical (simulated) inversion of the targets.</span>

<span class="sd">        It does the inversion in real time, controlling all hidden layers</span>
<span class="sd">        simultaneously.</span>

<span class="sd">        This function calls ``self.controller()`` as a subroutine, which</span>
<span class="sd">        returns values for :math:`\mathbf{u}`, :math:`\mathbf{v}^\text{fb}`,</span>
<span class="sd">        :math:`\mathbf{v}`, :math:`\mathbf{v}^\text{ff}` and :math:`\mathbf{r}`</span>
<span class="sd">        for every simulated time step. The last values of these arrays are taken</span>
<span class="sd">        to represent the steady state. However, convergence is not guaranteed.</span>
<span class="sd">        If ``self.include_non_converged_samples`` is set to  ``False``,</span>
<span class="sd">        the values of batch elements that did not converge are set to</span>
<span class="sd">        their feedforward mode values, which includes :math:`\mathbf{u}` and</span>
<span class="sd">        :math:`\mathbf{v}^\text{fb}` being set to 0. </span>
<span class="sd">        If ``self.include_non_converged_samples`` is set to ``True``, some of </span>
<span class="sd">        the returned values with ``_ss`` suffix may in fact not represent the </span>
<span class="sd">        steady state.</span>

<span class="sd">        Args:</span>
<span class="sd">            output_target (torch.Tensor): The output targets.</span>
<span class="sd">            verbose (bool): Whether to display warnings.</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            (....): An ordered tuple containing:</span>

<span class="sd">            - **u_ss** (torch.Tensor): :math:`\mathbf{u}`, the final control</span>
<span class="sd">              input.</span>
<span class="sd">            - **v_ss** (list):</span>
<span class="sd">              :math:`\mathbf{v}_{ss} = \mathbf{v}^- + \</span>
<span class="sd">              \Delta_{\mathbf{v}}`</span>
<span class="sd">              The final voltage activations of the somatic</span>
<span class="sd">              compartments, split in a list that contains</span>
<span class="sd">              :math:`\mathbf{v}_{ss}` for each layer.</span>
<span class="sd">            - **r_ss** (list): :math:`\mathbf{r}_{ss} = \</span>
<span class="sd">              \phi(\mathbf{v}_{ss})`. The final firing rates of the</span>
<span class="sd">              neurons, split in a list that contains :math:`\mathbf{r}_{ss}`</span>
<span class="sd">              for each layer.</span>
<span class="sd">            - **r_out_ss** (torch.Tensor): The finaloutput activation</span>
<span class="sd">              of the network.</span>
<span class="sd">            - **delta_v_ss** (list): A list containing the final</span>
<span class="sd">              :math:`\Delta \mathbf{v}_i` for each layer.</span>
<span class="sd">            - **(u, v_fb, v, v_ff, r)** (tuple): A tuple with 5 elements.</span>
<span class="sd">              :math:`\mathbf{u}` represents a tensor of dimension</span>
<span class="sd">              :math:`t_{max}\times B \times n_L` containing the control</span>
<span class="sd">              input for each timestep.</span>
<span class="sd">              :math:`\mathbf{v}^\text{fb}`, :math:`\mathbf{v}`,</span>
<span class="sd">              :math:`\mathbf{v}^\text{ff}` each contain a list with at</span>
<span class="sd">              index ``i`` a ``torch.Tensor`` of dimension</span>
<span class="sd">              :math:`t_{max}\times B \times n_i`.</span>
<span class="sd">              :math:`\mathbf{r}` is a list with at index ``i`` a</span>
<span class="sd">              ``torch.Tensor`` of dimension :math:`t_{max}\times B \times n_i`</span>
<span class="sd">              containing the firing rates of layer ``i`` for each timestep.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Get the post- and pre-nonlinearity activations.</span>
        <span class="n">r_feedforward</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">activations</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>
        <span class="n">v_feedforward</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">linear_activations</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>

        <span class="c1"># Compute the target activations.</span>
        <span class="n">r</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="p">(</span><span class="n">v_fb</span><span class="p">,</span> <span class="n">v_ff</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">sample_error</span> <span class="o">=</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">controller</span><span class="p">(</span><span class="n">output_target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_di</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt_di</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">tmax_di</span><span class="p">,</span>
                            <span class="n">k_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_p</span><span class="p">,</span>
                            <span class="n">noisy_dynamics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noisy_dynamics</span><span class="p">,</span>
                            <span class="n">inst_transmission</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inst_transmission</span><span class="p">,</span>
                            <span class="n">time_constant_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">time_constant_ratio</span><span class="p">,</span>
                            <span class="n">apical_time_constant</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">apical_time_constant</span><span class="p">,</span>
                            <span class="n">proactive_controller</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proactive_controller</span><span class="p">,</span>
                            <span class="n">sigma</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">,</span>
                            <span class="n">sigma_output</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma_output</span><span class="p">)</span>

        <span class="n">converged</span><span class="p">,</span> <span class="n">diverged</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_convergence</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r_feedforward</span><span class="p">,</span>
                                                     <span class="n">output_target</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> 
                                                     <span class="n">sample_error</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># Select only samples that have converged.</span>
        <span class="n">non_conv_idxs</span> <span class="o">=</span> <span class="n">converged</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">non_conv_idxs</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">bool_to_indices</span><span class="p">(</span><span class="n">non_conv_idxs</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_non_converged_samples</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
                <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][:,</span> <span class="n">non_conv_idxs</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v_feedforward</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">non_conv_idxs</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">v_ff</span><span class="p">[</span><span class="n">i</span><span class="p">][:,</span> <span class="n">non_conv_idxs</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v_feedforward</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">non_conv_idxs</span><span class="p">,:]</span>
                <span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][:,</span> <span class="n">non_conv_idxs</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.</span>
                <span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">][:,</span> <span class="n">non_conv_idxs</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">r_feedforward</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">non_conv_idxs</span><span class="p">]</span>
            <span class="n">u</span><span class="p">[:,</span> <span class="n">non_conv_idxs</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;There are </span><span class="si">%s</span><span class="s1"> non-converged &#39;</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">non_conv_idxs</span><span class="p">)</span><span class="o">+</span>\
                              <span class="s1">&#39;samples that are discarded.&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">non_conv_idxs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;There are </span><span class="si">%s</span><span class="s1"> non-converged &#39;</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">non_conv_idxs</span><span class="p">)</span><span class="o">+</span>\
                              <span class="s1">&#39;samples in the mini-batch.&#39;</span><span class="p">)</span>

        <span class="c1"># Get the steady-state target values (i.e. at last timestep).</span>
        <span class="n">r_ss</span> <span class="o">=</span> <span class="p">[</span><span class="n">val</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">r</span><span class="p">]</span>
        <span class="n">v_fb_ss</span> <span class="o">=</span> <span class="p">[</span><span class="n">val</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">v_fb</span><span class="p">]</span>
        <span class="n">v_ff_ss</span> <span class="o">=</span> <span class="p">[</span><span class="n">val</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">v_ff</span><span class="p">]</span>
        <span class="n">v_ss</span> <span class="o">=</span> <span class="p">[</span><span class="n">val</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">v</span><span class="p">]</span>
        <span class="n">r_out_ss</span> <span class="o">=</span> <span class="n">r_ss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">u_ss</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Compute the difference in somatic and basal voltages at steady-state.</span>
        <span class="n">delta_v_ss</span> <span class="o">=</span> <span class="p">[</span><span class="n">v_ss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">v_ff_ss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v_ss</span><span class="p">))]</span>

        <span class="k">return</span> <span class="n">u_ss</span><span class="p">,</span> <span class="n">v_ss</span><span class="p">,</span> <span class="n">r_ss</span><span class="p">,</span> <span class="n">r_out_ss</span><span class="p">,</span> <span class="n">delta_v_ss</span><span class="p">,</span> \
                <span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v_fb</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">v_ff</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="DFCNetwork.non_dynamical_inversion"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.non_dynamical_inversion">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">non_dynamical_inversion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_target</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the analytical solution for the network activations in the</span>
<span class="sd">        feedback phase, when the controller pushes the network to reach the</span>
<span class="sd">        output target. The following formulas are used:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathbf{u}_{ss} &amp;= (JQ + \alpha I)^{-1} \delta_L \\</span>
<span class="sd">            \Delta \mathbf{v}_{ss} &amp;= Q \mathbf{u}_{ss}</span>

<span class="sd">        with :math:`\mathbf{u}_{ss}` the control input at steady-state,</span>
<span class="sd">        :math:`\Delta \mathbf{v}_{ss}` the apical compartment voltage at</span>
<span class="sd">        steady-state, :math:`\delta_L` the difference between the output target</span>
<span class="sd">        and the output of the network at the feedforward sweep (without</span>
<span class="sd">        feedback). For the other symbols, refer to the paper.</span>

<span class="sd">        Args:</span>
<span class="sd">            output_target (torch.Tensor): The output target of the network.</span>
<span class="sd">            retain_graph (bool): Flag indicating whether the autograd graph</span>
<span class="sd">                should be retained for later use.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (....): An ordered tuple containing:</span>

<span class="sd">            - **u_ndi** (torch.Tensor): :math:`\mathbf{u}_{ss}`, steady state</span>
<span class="sd">              control input.</span>
<span class="sd">            - **v_ndi** (list):</span>
<span class="sd">              :math:`\mathbf{v}_{ss} = \mathbf{v}^- + \</span>
<span class="sd">              \Delta \mathbf{v}_{ss}`</span>
<span class="sd">              The steady-state voltage activations of the somatic</span>
<span class="sd">              compartments, split in a list that contains</span>
<span class="sd">              :math:`\mathbf{v}_{ss}` for each layer.</span>
<span class="sd">            - **r_ndi** (list): :math:`\mathbf{r}_{ss} = \</span>
<span class="sd">              \phi(\mathbf{v}_{ss})`. The steady-state firing rates of the</span>
<span class="sd">              neurons, split in a list that contains :math:`\mathbf{r}_{ss}`</span>
<span class="sd">              for each layer.</span>
<span class="sd">            - **r_out_ndi** (torch.Tensor): The steady state output activation</span>
<span class="sd">              of the network.</span>
<span class="sd">            - **delta_v_ndi_split** (list): A list containing</span>
<span class="sd">              :math:`\Delta \mathbf{v}_{ss}` for each layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Compute the Jacobian.</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_Q</span>
        <span class="n">J</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_full_jacobian</span><span class="p">()</span>

        <span class="c1"># Compute the error.</span>
        <span class="n">deltaL</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_error</span><span class="p">(</span><span class="n">output_target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span><span class="p">)</span>

        <span class="c1"># Analytically compute the steady-state control signal.</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">output_target</span><span class="o">.</span><span class="n">device</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_jacobian_as_fb</span><span class="p">:</span>
            <span class="n">u_ndi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">deltaL</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> \
                        <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="n">J</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">+</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_di</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">J</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>\
                        <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">delta_v_ndi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">J</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> \
                                <span class="n">u_ndi</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">u_ndi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">deltaL</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="n">Q</span><span class="p">)</span> <span class="o">+</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_di</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">J</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>\
                        <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">delta_v_ndi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">u_ndi</span><span class="p">,</span> <span class="n">Q</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
        <span class="n">delta_v_ndi_split</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">split_in_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta_v_ndi</span><span class="p">)</span>

        <span class="c1"># Compute the targets across layers.</span>
        <span class="n">r_previous</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">]</span> 
        <span class="n">v_ndi</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)):</span>
            <span class="n">v_ndi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta_v_ndi_split</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> \
                <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">r_previous</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">t</span><span class="p">()))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
                <span class="n">v_ndi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">v_ndi</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">r_previous</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>\
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">forward_activation_function</span><span class="p">(</span><span class="n">v_ndi</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

        <span class="n">r_ndi</span> <span class="o">=</span> <span class="n">r_previous</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">r_out_ndi</span> <span class="o">=</span> <span class="n">r_ndi</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">u_ndi</span><span class="p">,</span> <span class="n">v_ndi</span><span class="p">,</span> <span class="n">r_ndi</span><span class="p">,</span> <span class="n">r_out_ndi</span><span class="p">,</span> <span class="n">delta_v_ndi_split</span></div>

<div class="viewcode-block" id="DFCNetwork.compute_feedback_gradients"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.compute_feedback_gradients">[docs]</a>    <span class="k">def</span> <span class="nf">compute_feedback_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the gradients of the feedback weights for each layer.</span>

<span class="sd">        The updates are computed according to the following update rule:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \frac{d Q_i}{dt} = - k \frac{1}{\sigma^2} \</span>
<span class="sd">                \mathbf{v}^{\text{fb}}_i(t) \mathbf{u}(t)^T - \beta Q_i</span>

<span class="sd">        where the output target is equal to the feedforward output of the</span>
<span class="sd">        network (i.e. without feedback) and where noise is applied to the</span>
<span class="sd">        network dynamics. Hence, the controller will try to &#39;counter&#39; the</span>
<span class="sd">        noisy dynamics, such that the output is equal to the unnoisy output.</span>

<span class="sd">        The scaling :math:`k` is either 1 or, if the option</span>
<span class="sd">        ``scaling_fb_updates`` is active,</span>
<span class="sd">        :math:`(1 + \frac{\tau_v}{\tau_{\epsilon}})^{L-i}`.</span>

<span class="sd">        No inputs are required to this function since all activations are</span>
<span class="sd">        expected to be saved within the object.</span>

<span class="sd">        Args:</span>
<span class="sd">            loss (float): The loss.</span>
<span class="sd">            targets (torch.Tensor): The dataset targets. This will usually be</span>
<span class="sd">                ignored, as the targets will be taken to be the activations</span>
<span class="sd">                nudged towards lower loss, unless we use strong feedback.</span>
<span class="sd">            init (boolean): Indicates that this is a pre-train of the weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">data</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">strong_feedback</span><span class="p">:</span>
            <span class="n">output_target</span> <span class="o">=</span> <span class="n">targets</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">output_target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Compute the controller signal.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="p">(</span><span class="n">v_fb</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">_</span> <span class="o">=</span>  \
            <span class="bp">self</span><span class="o">.</span><span class="n">controller</span><span class="p">(</span><span class="n">output_target</span><span class="o">=</span><span class="n">output_target</span><span class="p">,</span>
                            <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_di_fb</span><span class="p">,</span>
                            <span class="n">dt</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dt_di_fb</span><span class="p">,</span>
                            <span class="n">tmax</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tmax_di_fb</span><span class="p">,</span>
                            <span class="n">k_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_p_fb</span><span class="p">,</span>
                            <span class="n">noisy_dynamics</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">inst_transmission</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inst_transmission_fb</span><span class="p">,</span>
                            <span class="n">time_constant_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">time_constant_ratio_fb</span><span class="p">,</span>
                            <span class="n">apical_time_constant</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">apical_time_constant_fb</span><span class="p">,</span>
                            <span class="n">proactive_controller</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proactive_controller</span><span class="p">,</span>
                            <span class="n">sigma</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma_fb</span><span class="p">,</span>
                            <span class="n">sigma_output</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma_output_fb</span><span class="p">)</span>

        <span class="c1"># Compute gradient for each layer.</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="c1">#  ignore the first timestep</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="n">v_fb_i</span> <span class="o">=</span> <span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>

            <span class="c1"># compute a layerwise scaling for the feedback weights</span>
            <span class="n">scaling</span> <span class="o">=</span> <span class="mf">1.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_fb_updates</span><span class="p">:</span>
                <span class="n">scaling</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_constant_ratio_fb</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau_noise</span><span class="p">)</span> \
                     <span class="o">**</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># get the amount of noise used</span>
            <span class="n">sigma_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_fb</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">sigma_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_output_fb</span>

            <span class="n">layer</span><span class="o">.</span><span class="n">compute_feedback_gradients_continuous</span><span class="p">(</span><span class="n">v_fb_i</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span>
                                                        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma_i</span><span class="p">,</span>
                                                        <span class="n">scaling</span><span class="o">=</span><span class="n">scaling</span><span class="p">)</span></div>

<div class="viewcode-block" id="DFCNetwork.compute_full_jacobian"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.compute_full_jacobian">[docs]</a>    <span class="k">def</span> <span class="nf">compute_full_jacobian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">noisy_dynamics</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the Jacobian of the network output.</span>

<span class="sd">        Compute the Jacobian of the network output (post-nonlinearity)</span>
<span class="sd">        with respect to either the concatenated pre-nonlinearity activations of</span>
<span class="sd">        all layers (including the output layer!)</span>
<span class="sd">        (i.e. ``self.layers[i].linear_activations``) if ``linear=True`` or the</span>
<span class="sd">        concatenated post-nonlinearity activations of all layers if</span>
<span class="sd">        ``linear=False`` (i.e. ``self.layers[i].activations``).</span>

<span class="sd">        If there is noise being used in the dynamics, then a low-passed version</span>
<span class="sd">        of the activations will be used to compute the Jacobian.</span>

<span class="sd">        Note that this implementation does not use autograd.</span>

<span class="sd">        Args:</span>
<span class="sd">            linear (bool): Flag indicating whether the Jacobian with respect to</span>
<span class="sd">                pre-nonlinearity activations (``linear=True``) should be taken</span>
<span class="sd">                or with respect to the post-nonlinearity activations</span>
<span class="sd">                (``linear=False``).</span>
<span class="sd">            noisy_dynamics (bool): Whether the dynamics are noisy.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (torch.Tensor): A :math:`B \times n_L \times \sum_{l=1}^L n_l`</span>
<span class="sd">                dimensional tensor, with :math:`B` the minibatch size and</span>
<span class="sd">                :math:`n_l` the dimension of layer :math:`l`,</span>
<span class="sd">                containing the Jacobian of the network output w.r.t. the</span>
<span class="sd">                concatenated activations (pre or post-nonlinearity) of all</span>
<span class="sd">                layers, for each minibatch sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">L</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span>

        <span class="c1"># Compute the derivatives w.r.t. activation function.</span>
        <span class="k">if</span> <span class="n">noisy_dynamics</span><span class="p">:</span>
            <span class="n">vectorized_nonlinearity_derivative</span> <span class="o">=</span> \
                <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">compute_vectorized_jacobian</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">linear_activations_lp</span><span class="p">)</span> \
                 <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vectorized_nonlinearity_derivative</span> <span class="o">=</span> \
                <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">compute_vectorized_jacobian</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">linear_activations</span><span class="p">)</span> \
                 <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>
        <span class="n">output_activation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span>
        
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">output_activation</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_activation</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">output_activation</span><span class="o">.</span><span class="n">device</span>

        <span class="n">J</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                        <span class="nb">sum</span><span class="p">([</span><span class="n">l</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]),</span>
                        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Compute Jacobian of the last layer.</span>
        <span class="n">idx_start</span><span class="p">,</span> <span class="n">idx_end</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">get_jacobian_slice</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">J</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">idx_start</span><span class="p">:</span><span class="n">idx_end</span><span class="p">]</span> <span class="o">=</span> \
            <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">linear</span><span class="p">:</span>
            <span class="n">J</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">idx_start</span><span class="p">:</span><span class="n">idx_end</span><span class="p">]</span> <span class="o">=</span> \
                    <span class="n">vectorized_nonlinearity_derivative</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>\
                    <span class="n">batch_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> \
                    <span class="n">J</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">idx_start</span><span class="p">:</span><span class="n">idx_end</span><span class="p">]</span>

        <span class="c1"># Compute Jacobian of previous layers.</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="c1"># Get indices of the current layer.</span>
            <span class="n">idx_start_1</span><span class="p">,</span> <span class="n">idx_end_1</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">get_jacobian_slice</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
            <span class="c1"># Get indices of the layer downstream to it.</span>
            <span class="n">idx_start_2</span><span class="p">,</span> <span class="n">idx_end_2</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">get_jacobian_slice</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    
            <span class="k">if</span> <span class="n">linear</span><span class="p">:</span>
                <span class="n">J</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">idx_start_1</span><span class="p">:</span><span class="n">idx_end_1</span><span class="p">]</span> <span class="o">=</span> \
                        <span class="n">vectorized_nonlinearity_derivative</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> \
                        <span class="n">J</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">idx_start_2</span><span class="p">:</span><span class="n">idx_end_2</span><span class="p">]</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>\
                        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">J</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">idx_start_1</span><span class="p">:</span><span class="n">idx_end_1</span><span class="p">]</span> <span class="o">=</span> \
                        <span class="p">(</span><span class="n">J</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">idx_start_2</span><span class="p">:</span><span class="n">idx_end_2</span><span class="p">]</span> <span class="o">*</span> \
                        <span class="n">vectorized_nonlinearity_derivative</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span>\
                        <span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>

            <span class="k">del</span> <span class="n">vectorized_nonlinearity_derivative</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">J</span></div>

<div class="viewcode-block" id="DFCNetwork.controller"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.controller">[docs]</a>    <span class="k">def</span> <span class="nf">controller</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_target</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">tmax</span><span class="p">,</span> <span class="n">k_p</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                   <span class="n">noisy_dynamics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inst_transmission</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">time_constant_ratio</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">apical_time_constant</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                   <span class="n">proactive_controller</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">sigma_output</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Simulate the feedback control loop for several timesteps. </span>

<span class="sd">        The following continuous time ODEs are simulated with time interval</span>
<span class="sd">        ``dt``. The following equation is used for the voltage:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \frac{\tau_v}{\tau_u}\frac{d \mathbf{v}_i(t)}{dt} = \</span>
<span class="sd">                -\mathbf{v}_i(t) + W_i \mathbf{r}_{i-1}(t) + b_i + \</span>
<span class="sd">                Q_i \mathbf{u}(t)</span>
<span class="sd">            </span>
<span class="sd">        And the following for the control signal:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathbf{u}(t) = \mathbf{u}^{\text{int}}(t) + k \mathbf{e}(t)</span>

<span class="sd">        .. math::</span>

<span class="sd">            \tau_u \frac{d \mathbf{u}^{\text{int}}(t)}{dt} = \mathbf{e}(t) - \</span>
<span class="sd">                \alpha \mathbf{u}^{\text{int}}(t)</span>

<span class="sd">        Note that we use a ratio :math:`\frac{\tau_v}{\tau_u}` instead of two</span>
<span class="sd">        separate time constants for :math:`\mathbf{v}` and :math:`\mathbf{u}`,</span>
<span class="sd">        as a scaling of both time constants can be absorbed in the simulation</span>
<span class="sd">        timestep ``dt``.</span>
<span class="sd">        IMPORTANT: ``time_constant_ratio`` should never be taken smaller than</span>
<span class="sd">        ``dt``, as then the forward Euler method will become unstable by</span>
<span class="sd">        default (the simulation steps will start to &#39;overshoot&#39;).</span>

<span class="sd">        If ``inst_transmission=False``, the forward Euler method is used to</span>
<span class="sd">        simulate the differential equation. If ``inst_transmission=True``, a</span>
<span class="sd">        slight modification is made to the forward Euler method, assuming that</span>
<span class="sd">        we have instant transmission from one layer to the next: the basal</span>
<span class="sd">        voltage of layer ``i`` at timestep ``t`` will already be based on the</span>
<span class="sd">        forward propagation of the somatic voltage of layer ``i-1`` at timestep</span>
<span class="sd">        ``t``, hence including the feedback of layer ``i-1`` at timestep ``t``.</span>
<span class="sd">        It is recommended to put ``inst_transmission=True`` when the</span>
<span class="sd">        ``time_constant_ratio`` is approaching ``dt``, as then we are</span>
<span class="sd">        approaching the limit of instantaneous system dynamics in the simulation</span>
<span class="sd">        where ``inst_transmission`` is always used (See below).</span>

<span class="sd">        If ``inst_system_dynamics=True``, we assume that the time constant of</span>
<span class="sd">        the system (i.e. the network) is much smaller than that of the</span>
<span class="sd">        controller and we approximate this by replacing the dynamical equations</span>
<span class="sd">        for :math:`\mathbf{v}_i` by their instantaneous equivalents:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathbf{v}_i(t) = W_i \mathbf{r}_{i-1}(t) + b_i + Q_i \mathbf{u}(t)</span>

<span class="sd">        Note that ``inst_transmission`` will always be put on ``True`` </span>
<span class="sd">        (overridden) in combination with ``inst_system_dynamics``.</span>

<span class="sd">        If ``proactive_controller=True``, the control input ``u[k+1]`` will be</span>
<span class="sd">        used to compute the apical voltages ``v^\text{fb}[k+1]``, instead of the</span>
<span class="sd">        control input ``u[k]``. This is a slight variation on the forward Euler</span>
<span class="sd">        method and corresponds to the conventional discretized control schemes.</span>

<span class="sd">        If ``noisy_dynamics=True``, noise is added to the apical compartment of</span>
<span class="sd">        the neurons. We now simulate the apical compartment with its own</span>
<span class="sd">        dynamics, as the feedback learning rule needs access to the noisy apical</span>
<span class="sd">        compartment. We use the following stochastic differential equation for</span>
<span class="sd">        the apical compartment:</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">        </span>
<span class="sd">            \tau_{\text{fb}} d \mathbf{v}_i^{\text{fb}}(t) = \</span>
<span class="sd">                (-\mathbf{v}_i^{\text{fb}}(t) + Q_i \mathbf{u}(t))dt + \sigma \</span>
<span class="sd">                \bm{\epsilon}_i(t)</span>
<span class="sd">        </span>
<span class="sd">        with :math:`\bm{\epsilon}` the Wiener process (Brownian motion) with</span>
<span class="sd">        covariance matrix :math:`I`.</span>

<span class="sd">        This is simulated with the Euler-Maruyama method:</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">        </span>
<span class="sd">            v_i^\text{fb}[k+1] = v_i^\text{fb}[k] + \Delta t / \tau_\text{fb} \</span>
<span class="sd">                (-v_i^\text{fb}[k] + Q_i u[k]) + \sigma / \sqrt{\Delta t / \</span>
<span class="sd">                \tau_\text{fb}} \Delta \beta</span>

<span class="sd">        with :math:`\Delta \beta` drawn from the zero-mean Gaussian distribution</span>
<span class="sd">        with covariance :math:`I`. The other dynamical equations in the system</span>
<span class="sd">        remain the same, except that :math:`Q_i \mathbf{u}` is replaced by</span>
<span class="sd">        :math:`\mathbf{v}_i^\text{fb}`:</span>

<span class="sd">        .. math::</span>
<span class="sd">        </span>
<span class="sd">            \tau_v \frac{d \mathbf{v}_i(t)}{dt} = -\mathbf{v}_i(t) + W_i \</span>
<span class="sd">                \mathbf{r}_{i-1}(t) + b_i + \mathbf{v}_i^\text{fb}</span>

<span class="sd">        One can opt for instantaneous apical compartment dynamics by putting</span>
<span class="sd">        its time constant :math:`\tau_\text{fb}` (``apical_time_constant``) equal</span>
<span class="sd">        to ``dt``. This is not encouraged for training the feedback weights, but</span>
<span class="sd">        can be used for simulating noisy system dynamics for training the</span>
<span class="sd">        forward weights, resulting in:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \tau_v d \mathbf{v}_i(t) = (-\mathbf{v}_i(t) + W_i \</span>
<span class="sd">                \mathbf{r}_{i-1}(t) + b_i + Q_i \mathbf{u}(t) )dt + \</span>
<span class="sd">                \sigma \bm{\epsilon}_i(t)</span>

<span class="sd">        which can again be similarly discretized with the Euler-Maruyama method.</span>

<span class="sd">        Note that for training the feedback weights, it is recommended to put</span>
<span class="sd">        ``inst_transmission=True``, such that the noise of all layers can</span>
<span class="sd">        influence the output at the current timestep, instead of having to wait</span>
<span class="sd">        for a couple of timesteps, depending on the layer depth.</span>

<span class="sd">        Note that in the current implementation, we interpret that the noise is</span>
<span class="sd">        added in the apical compartment, and that the basal and somatic</span>
<span class="sd">        compartments are not noisy. At some point we might want to also add</span>
<span class="sd">        noise in the somatic and basal compartments for physical realism.</span>

<span class="sd">        Args:</span>
<span class="sd">            output_target (torch.Tensor): The output target</span>
<span class="sd">                :math:`\mathbf{r}_L^*` that is used by the controller to compute</span>
<span class="sd">                the control error :math:`\mathbf{e}(t)`.</span>
<span class="sd">            alpha (float): The leakage term of the controller.</span>
<span class="sd">            dt (float): The time interval used in the forward Euler method.</span>
<span class="sd">            tmax (int): The maximum number of timesteps.</span>
<span class="sd">            k_p (float): The positive gain parameter for the proportional part</span>
<span class="sd">                of the controller. If it is equal to zero (by default),</span>
<span class="sd">                no proportional control will be used, only integral control.</span>
<span class="sd">            noisy_dynamics (bool): Flag indicating whether noise should be</span>
<span class="sd">                added to the dynamics.</span>
<span class="sd">            inst_transmission (bool): Flag indicating whether the modified</span>
<span class="sd">                version of the forward Euler method should be used, where it is</span>
<span class="sd">                assumed that there is instant transmission between layers (but</span>
<span class="sd">                not necessarily instant voltage dynamics). See the docstring</span>
<span class="sd">                above for more information.</span>
<span class="sd">            time_constant_ratio (float): Ratio of the time constant of the</span>
<span class="sd">                voltage dynamics w.r.t. the controller dynamics.</span>
<span class="sd">            apical_time_constant (float): Time constant of the apical</span>
<span class="sd">                compartment. If ``-1``, we assume that the user does not want</span>
<span class="sd">                to model the apical compartment dynamics, but assumes instant</span>
<span class="sd">                transmission to the somatic compartment instead (i.e. apical</span>
<span class="sd">                time constant of zero).</span>

<span class="sd">        Returns:</span>

<span class="sd">            (....): Tuple containing:</span>

<span class="sd">            - **r** (list): A list with at index ``i`` a ``torch.Tensor``</span>
<span class="sd">                of dimension :math:`t_{max}\times B \times n_i` containing the</span>
<span class="sd">                firing rates of layer ``i`` for each timestep.</span>
<span class="sd">            - **u** (torch.Tensor): A tensor of dimension</span>
<span class="sd">                :math:`t_{max}\times B \times n_L` containing the control input</span>
<span class="sd">                for each timestep.</span>
<span class="sd">            - **(v_fb, v_ff, v)** (tuple): A tuple with 3 elements, each</span>
<span class="sd">                containing a list with at index ``i`` a ``torch.Tensor`` of</span>
<span class="sd">                dimension :math:`t_{max}\times B \times n_i` containing the</span>
<span class="sd">                voltage levels of the apical, basal or somatic compartments</span>
<span class="sd">                respectively.</span>
<span class="sd">            - **sample_error** (torch.Tensor): A tensor of dimension</span>
<span class="sd">                :math:`t_{max} \times B` containing the L2 norm of the error</span>
<span class="sd">                :math:`\mathbf{e}(t)` at each timestep.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">k_p</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only positive values for &quot;k_p&quot; are allowed&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inst_system_dynamics</span><span class="p">:</span>
            <span class="n">inst_transmission</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">apical_time_constant</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="n">apical_time_constant</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">apical_time_constant</span> <span class="o">=</span> <span class="n">dt</span>
        <span class="k">assert</span> <span class="n">apical_time_constant</span> <span class="o">&gt;</span> <span class="mi">0</span>

        <span class="c1"># Extract important variables and shapes.</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">output_target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> 
        <span class="n">lod</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span> <span class="c1"># layer out dims</span>
        <span class="n">size_output</span> <span class="o">=</span> <span class="n">output_target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">tmax</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">tmax</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">output_target</span><span class="o">.</span><span class="n">device</span>

        <span class="c1"># Create empty containers for desired variables:</span>
        <span class="c1"># - v_fb: apical voltage (Ki u)</span>
        <span class="c1"># - v_ff: basal voltage (Wi h_target_i-1)</span>
        <span class="c1"># - v: somatic voltage</span>
        <span class="c1"># - r: v after non-linearlity</span>
        <span class="c1"># - u: control signal</span>
        <span class="c1"># - u_int: intermediate control signal if proportional part is active</span>
        <span class="n">v_fb</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tmax</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lod</span><span class="p">]</span>
        <span class="n">v_ff</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tmax</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lod</span><span class="p">]</span>
        <span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tmax</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lod</span><span class="p">]</span>
        <span class="n">r</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tmax</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lod</span><span class="p">]</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tmax</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">size_output</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">k_p</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">u_int</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tmax</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">size_output</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">u_lp</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">v_lp</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">low_pass_filter_u</span><span class="p">:</span>
            <span class="n">u_lp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">low_pass_filter_noise</span><span class="p">:</span>
            <span class="n">noise_filtered</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> \
                              <span class="n">l</span> <span class="ow">in</span> <span class="n">lod</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">noisy_dynamics</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_jacobian_as_fb</span><span class="p">:</span>
            <span class="n">v_lp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tmax</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>\
                   <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lod</span><span class="p">]</span>
        <span class="n">sample_error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">tmax</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
        
        <span class="c1"># Fill the values at the initial timestep.</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
            <span class="n">v_ff</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">linear_activations</span>
            <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">linear_activations</span>
            <span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span>    
            <span class="k">if</span> <span class="n">v_lp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">v_lp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">linear_activations</span>      
        <span class="n">sample_error</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">output_target</span><span class="p">,</span> <span class="n">r</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>

        <span class="c1"># Save initial zero targets for computation of Jacobian if needed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_r</span> <span class="o">=</span> <span class="p">[</span><span class="n">r_l</span><span class="p">[:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="k">for</span> <span class="n">r_l</span> <span class="ow">in</span> <span class="n">r</span><span class="p">]</span>

        <span class="c1"># If hidden activations are linear, then J doen&#39;t depend on the samples</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_jacobian_as_fb</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
            <span class="n">J</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_full_jacobian</span><span class="p">(</span><span class="n">noisy_dynamics</span><span class="o">=</span><span class="n">noisy_dynamics</span><span class="p">)</span>

        <span class="c1"># Iterate over all the timesteps.</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tmax</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>

            <span class="c1"># Compute the error.</span>
            <span class="n">e</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_error</span><span class="p">(</span><span class="n">output_target</span><span class="p">,</span> <span class="n">r</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">t</span><span class="p">])</span>

            <span class="c1"># If hidden activations are nonlinear, then J does depend on the</span>
            <span class="c1"># samples (derivative of their activations).</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_jacobian_as_fb</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">!=</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
                <span class="n">J</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_full_jacobian</span><span class="p">(</span><span class="n">noisy_dynamics</span><span class="o">=</span><span class="n">noisy_dynamics</span><span class="p">)</span>
            
            <span class="c1"># Compute the control signal ``u``.</span>
            <span class="k">if</span> <span class="n">k_p</span> <span class="o">&gt;</span> <span class="mf">0.</span><span class="p">:</span>
                <span class="c1"># Proportional and integral control.</span>
                <span class="n">u_int</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">u_int</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">*</span> <span class="p">(</span><span class="n">e</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
                <span class="n">u</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">u_int</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">k_p</span> <span class="o">*</span> <span class="n">e</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Only integral control.</span>
                <span class="n">u</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">*</span> <span class="p">(</span><span class="n">e</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            <span class="c1"># Exponential low-pass filter if necessary.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">low_pass_filter_u</span><span class="p">:</span>
                <span class="c1"># We need to keep track both of the unfiltered u and the</span>
                <span class="c1"># low-pass filtered u, as we might need the high-frequency parts</span>
                <span class="c1"># of u for the single-phase feedback weight updates.</span>
                <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># start the low-pass filtering at the same value of u,</span>
                    <span class="c1"># as otherwise it takes a long time to recover from zero</span>
                    <span class="n">u_lp</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">u_lp</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">dt</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau_f</span><span class="p">)</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                                  <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">dt</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau_f</span><span class="p">))</span> <span class="o">*</span> <span class="n">u_lp</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

            <span class="k">def</span> <span class="nf">layer_iteration</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
                <span class="sd">&quot;&quot;&quot;Compute the controlled activations of layer ``i``.&quot;&quot;&quot;</span>
                <span class="c1"># Get the activities of previous layer.</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">r_previous</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">inst_transmission</span><span class="p">:</span>
                        <span class="n">r_previous</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">r_previous</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="n">t</span><span class="p">]</span>

                <span class="c1"># Get basal voltage of current layer (based on ff input).</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">r_previous</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">a</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
                <span class="n">v_ff</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">a</span>

                <span class="k">def</span> <span class="nf">get_control_signal</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">u_aux</span><span class="p">):</span>
                    <span class="sd">&quot;&quot;&quot;Get the control signal Qu for the given timestep.</span>

<span class="sd">                    By default, this computes :math:`Qu` but in case the option</span>
<span class="sd">                    `use_jacobian_as_fb``is active, this computes :math:`Ju`.</span>

<span class="sd">                    Args:</span>
<span class="sd">                        t (int): The timestep.</span>
<span class="sd">                        u_aux (torch.Tensor): The control u to use. Can be</span>
<span class="sd">                            low-pass filtered or not, depending on</span>
<span class="sd">                            `low_pass_filter_u`.</span>

<span class="sd">                    Returns:</span>
<span class="sd">                        (torch.Tensor): The control signal.</span>
<span class="sd">                    &quot;&quot;&quot;</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_jacobian_as_fb</span><span class="p">:</span>
                        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">u_aux</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                        <span class="n">n_out</span> <span class="o">=</span> <span class="n">u_aux</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

                        <span class="c1"># Select the correct Jacobian block.</span>
                        <span class="n">J_sq</span> <span class="o">=</span> <span class="n">J</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">J</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                        <span class="n">Ji</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">split_in_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">J_sq</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span>
                        <span class="n">Ji</span> <span class="o">=</span> <span class="n">Ji</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">Ji</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

                        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">u_aux</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">Ji</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">u_aux</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> \
                                        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights_backward</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>

                <span class="c1"># Get the control signal.</span>
                <span class="n">control_signal</span> <span class="o">=</span> <span class="n">get_control_signal</span><span class="p">(</span>\
                                    <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">proactive_controller</span> <span class="k">else</span> <span class="n">t</span><span class="p">,</span>
                                    <span class="n">u_lp</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">low_pass_filter_u</span> <span class="k">else</span> <span class="n">u</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">control_signal</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">shape</span>

                <span class="c1"># Get apical voltage of current layer (based on fb input).</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inst_apical_dynamics</span><span class="p">:</span>
                    <span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">control_signal</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">/</span> <span class="n">apical_time_constant</span> <span class="o">*</span>\
                                      <span class="p">(</span><span class="o">-</span> <span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">control_signal</span><span class="p">)</span>

                <span class="c1"># Add noise to the apical voltage if necessary.</span>
                <span class="k">if</span> <span class="n">noisy_dynamics</span><span class="p">:</span>
                    <span class="n">sigma_copy</span> <span class="o">=</span> <span class="n">sigma</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">sigma_copy</span> <span class="o">=</span> <span class="n">sigma_output</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">low_pass_filter_noise</span><span class="p">:</span>
                        <span class="c1"># Warning: for very small dt, we might need to change</span>
                        <span class="c1"># the implementation for numerical stability and work</span>
                        <span class="c1"># with tau_noise * sqrt(dt) instead of </span>
                        <span class="c1"># alpha_noise / sqrt(dt).</span>
                        <span class="n">alpha_noise</span> <span class="o">=</span> <span class="n">dt</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau_noise</span>
                        <span class="n">noise_filtered</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> \
                                <span class="p">(</span><span class="n">alpha_noise</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dt</span><span class="p">))</span> <span class="o">*</span> \
                                <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>\
                                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">+</span>\
                                <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_noise</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise_filtered</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                        <span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span>  <span class="n">sigma_copy</span> <span class="o">*</span> <span class="n">noise_filtered</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>   
                        <span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span>  <span class="n">sigma_copy</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">/</span> \
                            <span class="n">apical_time_constant</span> <span class="o">*</span> \
                            <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

                <span class="c1"># Get somatic voltage as function of basal and apical voltages.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inst_system_dynamics</span><span class="p">:</span>
                    <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">v_ff</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                <span class="k">else</span><span class="p">:</span> 
                    <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">/</span> <span class="n">time_constant_ratio</span> \
                                      <span class="o">*</span> <span class="p">(</span><span class="n">v_fb</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">v_ff</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span>
                                         <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span><span class="p">,</span> <span class="p">:])</span>

                <span class="c1"># Compute the post-nonlinearity activations of the layer.</span>
                <span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">forward_activation_function</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

                <span class="c1"># Update activations in layer objects to enable steady-state </span>
                <span class="c1"># jacobian computation in `compute_full_jacobian()`</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_jacobian_as_fb</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">!=</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">linear_activations</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                    <span class="k">if</span> <span class="n">noisy_dynamics</span><span class="p">:</span>
                        <span class="n">alpha_r</span> <span class="o">=</span> <span class="n">dt</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau_f</span>
                        <span class="n">v_lp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">alpha_r</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                                            <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_r</span><span class="p">)</span> <span class="o">*</span> <span class="n">v_lp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">t</span><span class="p">]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">linear_activations_lp</span> <span class="o">=</span> \
                                        <span class="p">[</span><span class="n">v_lp_l</span><span class="p">[:</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="k">for</span> <span class="n">v_lp_l</span> <span class="ow">in</span> <span class="n">v_lp</span><span class="p">]</span>

            <span class="c1"># Computed the controlled activations of all layers in current ts.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">inst_transmission</span><span class="p">:</span>
                <span class="c1"># Compute backwards to have already the influence of the</span>
                <span class="c1"># controller being propagated through network across time</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
                    <span class="n">layer_iteration</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
                    <span class="n">layer_iteration</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

            <span class="c1"># Compute the loss.</span>
            <span class="n">sample_error</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">output_target</span><span class="p">,</span>
                                                    <span class="n">r</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

            <span class="c1"># Save targets for computation of Jacobian if needed, only up to</span>
            <span class="c1"># the current timestep.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_r</span> <span class="o">=</span> <span class="p">[</span><span class="n">r_l</span><span class="p">[:</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="k">for</span> <span class="n">r_l</span> <span class="ow">in</span> <span class="n">r</span><span class="p">]</span>

        <span class="c1"># Store the control signal.</span>
        <span class="k">if</span> <span class="n">noisy_dynamics</span><span class="p">:</span>
            <span class="c1"># With noisy dynamics, the last value of u will be noisy, and we</span>
            <span class="c1"># should average over u to cancel out the noise. I assume that in</span>
            <span class="c1"># the last quarter of the simulation, u has converged, so we can</span>
            <span class="c1"># average over that interval.</span>
            <span class="n">interval_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">tmax</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="o">-</span><span class="n">interval_length</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,:,:],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>\
                    <span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">interval_length</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">r</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="p">(</span><span class="n">v_fb</span><span class="p">,</span> <span class="n">v_ff</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">sample_error</span></div>

<div class="viewcode-block" id="DFCNetwork.check_convergence"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.check_convergence">[docs]</a>    <span class="k">def</span> <span class="nf">check_convergence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_feedforward</span><span class="p">,</span> <span class="n">output_target</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span>
                          <span class="n">sample_error</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check whether the dynamics of the network have converged.</span>

<span class="sd">        This function computes whether individual samples have converged to a</span>
<span class="sd">        small output error. Like this, the ones that have not converged can be</span>
<span class="sd">        exclulded from the mini-batch update.</span>

<span class="sd">        Args:</span>
<span class="sd">            r (torch.Tensor): The target activations across layers.</span>
<span class="sd">            r_feedforward (torch.Tensor): The forward activations across layers.</span>
<span class="sd">            output_target (torch.Tensor): The output target.</span>
<span class="sd">            u (torch.Tensor): The control signal.</span>
<span class="sd">            sample_error (torch.Tensor): The L2 norm of the error :math:`e(t)`</span>
<span class="sd">                at each timestep, computed by the controller.</span>
<span class="sd">            batch_size (int): The batch-size.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (....): Tuple containing:</span>

<span class="sd">            - **converged**: List indicating if individual samples converged.</span>
<span class="sd">            - **diverged**: List indicating if individual samples diverged.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Define the thresholds for convergence and divergence.</span>
        <span class="n">threshold_convergence</span> <span class="o">=</span> <span class="mf">1e-5</span>
        <span class="n">threshold_divergence</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># Get the target activations in the last layer and timestep.</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Compute the loss.</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">((</span><span class="n">output_target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_di</span> <span class="o">*</span> <span class="n">u</span><span class="p">),</span> <span class="n">r</span><span class="p">)</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">r_feedforward</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="c1"># Determine whether individual samples converged or diverged.</span>
        <span class="n">converged</span> <span class="o">=</span> <span class="p">((</span><span class="n">diff</span> <span class="o">/</span> <span class="n">norm</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">threshold_convergence</span><span class="p">)</span> <span class="o">*</span> \
                    <span class="p">(</span><span class="n">sample_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_di</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sample_error</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">diverged</span> <span class="o">=</span> <span class="p">(</span><span class="n">diff</span> <span class="o">/</span> <span class="n">norm</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">threshold_divergence</span>

        <span class="c1"># Update the count of converged/diverged samples per epoch.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">converged_samples_per_epoch</span> <span class="o">+=</span> \
            <span class="nb">sum</span><span class="p">(</span><span class="n">converged</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">diverged_samples_per_epoch</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">diverged</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">not_converged_samples_per_epoch</span> <span class="o">+=</span> \
            <span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">converged</span><span class="p">)</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">diverged</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">converged</span><span class="p">,</span> <span class="n">diverged</span></div>

<div class="viewcode-block" id="DFCNetwork.compute_error"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.compute_error">[docs]</a>    <span class="k">def</span> <span class="nf">compute_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_target</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the error :math:`\mathbf{e}(t)` in the predictions.</span>

<span class="sd">        By default this error is computed as in the DFC paper according to:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathbf{e}(t) = \mathbf{r}_L^* - \mathbf{r}_L(t)</span>

<span class="sd">        For a mean-squared error (MSE) loss</span>
<span class="sd">        :math:`\mathcal{L} = \frac{1}{2}\lVert {\mathbf{r}_L^* - \</span>
<span class="sd">        \mathbf{r}_L(t)} \rVert_{2}^{2}`, this can be seen as the gradient of</span>
<span class="sd">        the loss with respect to the output activations :math:`\mathbf{r}_L(t)`.</span>
<span class="sd">        This notion can be generalized to other losses, and we can instead</span>
<span class="sd">        write the error as:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathbf{e}(t) = - \frac{\partial \mathcal{L}}{\partial \</span>
<span class="sd">                \mathbf{r}_L} \biggr\rvert^T_{\mathbf{r}_L=\mathbf{r}_L(t)}</span>

<span class="sd">        In this function we hard-code the solution of this equation for the</span>
<span class="sd">        MSE loss mentioned above as well as for the cross-entropy loss. Which</span>
<span class="sd">        one of these is used will be determined by the attribute</span>
<span class="sd">        ``loss_function_name``, which by default is the MSE loss.</span>

<span class="sd">        So for cross-entropy loss, we return the following error:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathbf{e}(t) = \mathbf{r}_L^* - \text{softmax}(\mathbf{r}_L(t))</span>

<span class="sd">        Args:</span>
<span class="sd">            output_target (torch.Tensor): The desired output</span>
<span class="sd">                :math:`\mathbf{r}_L^*`.</span>
<span class="sd">            r (torch.Tensor): The current output :math:`\mathbf{r}_L(t)`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (torch.Tensor): The error :math:`\mathbf{e}(t)`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">output_target</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">r</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function_name</span> <span class="o">==</span> <span class="s1">&#39;mse&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output_target</span> <span class="o">-</span> <span class="n">r</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function_name</span> <span class="o">==</span> <span class="s1">&#39;cross_entropy&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output_target</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Loss function </span><span class="si">%s</span><span class="s1"> &#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function_name</span> <span class="o">+</span> \
                             <span class="s1">&#39;not recognized.&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="DFCNetwork.compute_loss"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.compute_loss">[docs]</a>    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_target</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the loss in the predictions.</span>

<span class="sd">        This function is mostly used to check for convergence.</span>
<span class="sd">        By default this error is computed as in the DFC paper for each sample</span>
<span class="sd">        according to:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathcal{L} = \frac{1}{2}\ \lVert {\mathbf{r}_L^* - \</span>
<span class="sd">                \mathbf{r}_L(t)} \rVert_{2}^{2}</span>

<span class="sd">        However, if ``loss_function_name==cross_entropy`` we compute the</span>
<span class="sd">        following:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathcal{L} = - (\mathbf{r}_L^* * \log \</span>
<span class="sd">                \text{softmax}(\mathbf{r}_L(t))</span>

<span class="sd">        Args:</span>
<span class="sd">            output_target (torch.Tensor): The desired output</span>
<span class="sd">                :math:`\mathbf{r}_L^*`.</span>
<span class="sd">            r (torch.Tensor): The current output :math:`\mathbf{r}_L(t)`.</span>
<span class="sd">            axis (int): The axis across which to compute the norm.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (torch.Tensor): The list of loss values in the mini-batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">output_target</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">r</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function_name</span> <span class="o">==</span> <span class="s1">&#39;mse&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">output_target</span> <span class="o">-</span> <span class="n">r</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function_name</span> <span class="o">==</span> <span class="s1">&#39;cross_entropy&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mutils</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">output_target</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Loss function </span><span class="si">%s</span><span class="s1"> &#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function_name</span> <span class="o">+</span> \
                             <span class="s1">&#39;not recognized.&#39;</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">forward_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Access forward parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            params (list): The list of forward parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">])</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">])</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">feedback_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Access feedback parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            params (list): The list of feedback parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">layer</span><span class="o">.</span><span class="n">weights_backward</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Access all parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            params (list): The list of all parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_params</span>
        <span class="n">params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feedback_params</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">params</span>

<div class="viewcode-block" id="DFCNetwork.get_max_grad"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.get_max_grad">[docs]</a>    <span class="k">def</span> <span class="nf">get_max_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params_type</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">):</span>  
        <span class="sd">&quot;&quot;&quot;Return the maximum gradient across the parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            params_type (str): Whether to compute across the entire set of</span>
<span class="sd">                parameters or only forward or only feedback.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (float): The maximum gradient encountered.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">params_type</span> <span class="o">==</span> <span class="s1">&#39;both&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_max_grad</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">params_type</span> <span class="o">==</span> <span class="s1">&#39;forward&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_max_grad</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_params</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">params_type</span> <span class="o">==</span> <span class="s1">&#39;feedback&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_max_grad</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">feedback_params</span><span class="p">)</span></div>

<div class="viewcode-block" id="DFCNetwork.save_logs"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.save_logs">[docs]</a>    <span class="k">def</span> <span class="nf">save_logs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">writer</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">log_weights</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Log the norm of the weights and the gradients.</span>

<span class="sd">        Args:</span>
<span class="sd">            writer: The tensorboard writer.</span>
<span class="sd">            step (int): The writer iteration.</span>
<span class="sd">            log_weights (True): Which weights to log.</span>
<span class="sd">            prefix (str): The naming prefix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">log_param</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Log norm and gradient of one set of parameters.&quot;&quot;&quot;</span>
            <span class="c1"># Log the weights.</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="s1">&#39;feedback&#39;</span> <span class="k">if</span> <span class="n">feedback</span> <span class="k">else</span> <span class="s1">&#39;forward&#39;</span>
            <span class="n">weights_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">layer_</span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">_norm&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span>
                                    <span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">weights</span><span class="p">),</span>
                              <span class="n">scalar_value</span><span class="o">=</span><span class="n">weights_norm</span><span class="p">,</span>
                              <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
            
            <span class="c1"># Log the norms.</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">gradients_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span>
                    <span class="n">tag</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">layer_</span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">_gradients_norm&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span>
                                <span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">weights</span><span class="p">),</span>
                    <span class="n">scalar_value</span><span class="o">=</span><span class="n">gradients_norm</span><span class="p">,</span>
                    <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">log_weights</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="s1">&#39;forward&#39;</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_params</span><span class="p">):</span>
                <span class="n">log_param</span><span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="n">log_param</span><span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">log_weights</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="s1">&#39;feedback&#39;</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feedback_params</span><span class="p">):</span>
                <span class="n">log_param</span><span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="n">log_param</span><span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

<div class="viewcode-block" id="DFCNetwork.to"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.to">[docs]</a>    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Override `to` method to also move the backward weights.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights_backward</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights_backward</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>

<div class="viewcode-block" id="DFCNetwork.set_grads_to_bp"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.set_grads_to_bp">[docs]</a>    <span class="k">def</span> <span class="nf">set_grads_to_bp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set the gradients to correspond to those obtained with backprop.</span>

<span class="sd">        This function replaces the ``grad`` attributes of the forward weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            loss (float): The current loss.</span>
<span class="sd">            retain_graph (boolean): Whether autograd graph should be retained.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Autograd requires a flat list of parameters.</span>
        <span class="n">flattened_params</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">flatten_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_params</span><span class="p">)</span>
        <span class="n">bp_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">flattened_params</span><span class="p">,</span>
                                      <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">)</span>
        <span class="n">unflattened_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unflatten_params</span><span class="p">(</span><span class="n">bp_grad</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_params</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">bpg</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">unflattened_grad</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
                    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">bpg</span><span class="o">.</span><span class="n">shape</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">bpg</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">unflattened_grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">unflattened_grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span></div>

<div class="viewcode-block" id="DFCNetwork.unflatten_params"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.unflatten_params">[docs]</a>    <span class="k">def</span> <span class="nf">unflatten_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">params_type</span><span class="o">=</span><span class="s1">&#39;forward_params&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Unflatten the parameters.</span>

<span class="sd">        This function assumes a certain structure in the parameters to unflatten</span>
<span class="sd">        a list into the same structure as `self.forward_params` or</span>
<span class="sd">        `self.feedback_params`.</span>

<span class="sd">        Args:</span>
<span class="sd">            params (list): The flat list.</span>
<span class="sd">            params_type (str): The type of params we are dealing with.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (list): The unflattened list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

        <span class="n">unflattened_params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">:</span>
                <span class="n">bias</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">unflattened_params</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">unflattened_params</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">weights</span><span class="p">])</span>

        <span class="c1"># Sanity checks. REMOVE</span>
        <span class="n">params_to_match</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params_type</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">unflattened_params</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_to_match</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">unf_params</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">unflattened_params</span><span class="p">,</span> <span class="n">params_to_match</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">unf_params</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">unf_params</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">unf_params</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">params</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">return</span> <span class="n">unflattened_params</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">forward_params_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return a structure identical to forward params but with gradients.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (list): The gradients.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">params</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_params</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span>
                <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grad</span><span class="p">])</span>    
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">params</span><span class="o">.</span><span class="n">grad</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">grads</span>
    
<div class="viewcode-block" id="DFCNetwork.save_ndi_angles"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.save_ndi_angles">[docs]</a>    <span class="k">def</span> <span class="nf">save_ndi_angles</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">writer</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">save_dataframe</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">save_tensorboard</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Save angle between dynamical and analytical inversion results.</span>

<span class="sd">        The analytical results have been stored during training in</span>
<span class="sd">        `self.layers[i].ndi_update_weights`.</span>

<span class="sd">        Save the angle in the tensorboard writer (if ``save_tensorboard=True``)</span>
<span class="sd">        and in the corresponding dataframe (if ``save_dataframe=True``).</span>

<span class="sd">        Args:</span>
<span class="sd">            writer: Tensorboard writer.</span>
<span class="sd">            step (int): The number of forward training mini-batches.</span>
<span class="sd">            save_dataframe (bool): Flag indicating whether a dataframe of the</span>
<span class="sd">                angles should be saved in the network object.</span>
<span class="sd">            save_tensorboard (bool): Flag indicating whether the angles should</span>
<span class="sd">                be saved in Tensorboard.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ndi_param_updates</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">net_param_updates</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">flatten_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_params_grad</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
            <span class="n">parameter_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_forward_gradients</span><span class="p">()</span>
            <span class="n">weights_angle</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">compute_angle</span><span class="p">(</span>\
                    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">ndi_updates_weights</span><span class="p">,</span> <span class="n">parameter_update</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">ndi_param_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">ndi_updates_weights</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">:</span>
                <span class="n">bias_angle</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">compute_angle</span><span class="p">(</span>\
                        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">ndi_updates_bias</span><span class="p">,</span> <span class="n">parameter_update</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">ndi_param_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">ndi_updates_bias</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">save_tensorboard</span><span class="p">:</span>
                <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;layer </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">/weight_ndi_angle&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
                                  <span class="n">scalar_value</span><span class="o">=</span><span class="n">weights_angle</span><span class="p">,</span>
                                  <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">:</span>
                    <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">/bias_ndi_angle&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
                                      <span class="n">scalar_value</span><span class="o">=</span><span class="n">bias_angle</span><span class="p">,</span>
                                      <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">save_dataframe</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ndi_angles</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">step</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights_angle</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Compute the total angle between the entire updates across all layers.</span>
        <span class="n">total_angle</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">compute_angle</span><span class="p">(</span>\
                            <span class="n">mutils</span><span class="o">.</span><span class="n">vectorize_tensor_list</span><span class="p">(</span><span class="n">ndi_param_updates</span><span class="p">),</span>
                            <span class="n">mutils</span><span class="o">.</span><span class="n">vectorize_tensor_list</span><span class="p">(</span><span class="n">net_param_updates</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">save_tensorboard</span><span class="p">:</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;total_alignment/ndi_angle&#39;</span><span class="p">,</span>
                              <span class="n">scalar_value</span><span class="o">=</span><span class="n">total_angle</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">save_dataframe</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ndi_angles_network</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">step</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_angle</span><span class="o">.</span><span class="n">item</span><span class="p">()</span></div>


<div class="viewcode-block" id="DFCNetwork.save_bp_angles"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.save_bp_angles">[docs]</a>    <span class="k">def</span> <span class="nf">save_bp_angles</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">writer</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">save_tensorboard</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_dataframe</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Save the angles of the current forward parameter updates</span>
<span class="sd">        with the backprop update.</span>

<span class="sd">        Save the angle in the tensorboard writer (if ``save_tensorboard=True``)</span>
<span class="sd">        and in the corresponding dataframe (if ``save_dataframe=True``).</span>

<span class="sd">        Args:</span>
<span class="sd">            (....): See docstring of method :meth:`save_ndi_angles`.</span>
<span class="sd">            loss (torch.Tensor): Output loss of the network.</span>
<span class="sd">            retain_graph (bool): Flag indicating whether the graph of the</span>
<span class="sd">                network should be retained after computing the gradients or</span>
<span class="sd">                Jacobians. If the graph will not be used anymore for the current</span>
<span class="sd">                minibatch afterwards, `retain_graph` should be `False`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">layer_indices</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">layer_indices</span><span class="p">:</span>
            <span class="n">retain_graph_flag</span> <span class="o">=</span> <span class="n">retain_graph</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">layer_indices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">retain_graph_flag</span> <span class="o">=</span> <span class="kc">True</span>
                
            <span class="n">angles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_bp_angles</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">retain_graph_flag</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">save_tensorboard</span><span class="p">:</span>
                <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;layer </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">/weight_bp_angle&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
                                  <span class="n">scalar_value</span><span class="o">=</span><span class="n">angles</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                  <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
                    <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">/bias_bp_angle&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
                                      <span class="n">scalar_value</span><span class="o">=</span><span class="n">angles</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                      <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">save_dataframe</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bp_angles</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">step</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">angles</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span></div>

<div class="viewcode-block" id="DFCNetwork.save_H_angles"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.save_H_angles">[docs]</a>    <span class="k">def</span> <span class="nf">save_H_angles</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">writer</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span>
                       <span class="n">save_tensorboard</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_dataframe</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Save the angles of the current forward parameter updates</span>
<span class="sd">        with the update driven from the Lu loss for those parameters </span>

<span class="sd">        Save the angle in the tensorboard writer (if ``save_tensorboard=True``)</span>
<span class="sd">        and in the corresponding dataframe (if ``save_dataframe=True``).</span>

<span class="sd">        Args:</span>
<span class="sd">            (....): See docstring of method :meth:`save_ndi_angles`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">lu_parameter_updates_W</span><span class="p">,</span> <span class="n">lu_parameter_updates_b</span> <span class="o">=</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">compute_H_update</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">u</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
            <span class="n">parameter_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_forward_gradients</span><span class="p">()</span>
            <span class="n">weights_angle</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">compute_angle</span><span class="p">(</span><span class="n">lu_parameter_updates_W</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">parameter_update</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
                <span class="n">bias_angle</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">compute_angle</span><span class="p">(</span><span class="n">lu_parameter_updates_b</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                  <span class="n">parameter_update</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

            <span class="k">if</span> <span class="n">save_tensorboard</span><span class="p">:</span>
                <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;layer </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">/weight_lu_angle&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
                                  <span class="n">scalar_value</span><span class="o">=</span><span class="n">weights_angle</span><span class="p">,</span>
                                  <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
                    <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">/bias_lu_angle&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
                                      <span class="n">scalar_value</span><span class="o">=</span><span class="n">bias_angle</span><span class="p">,</span>
                                      <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">save_dataframe</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lu_angles</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">step</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights_angle</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                
        <span class="c1"># Compute the total angle between the entire updates across all layers.</span>
        <span class="c1"># We only do this for the weights.</span>
        <span class="n">parameter_updates_concat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_vectorized_parameter_updates</span><span class="p">(</span>
                                                    <span class="n">with_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">lu_parameter_updates_concat</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">vectorize_tensor_list</span><span class="p">(</span>\
                                                    <span class="n">lu_parameter_updates_W</span><span class="p">)</span>
        <span class="n">total_angle</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">compute_angle</span><span class="p">(</span><span class="n">parameter_updates_concat</span><span class="p">,</span> 
                                           <span class="n">lu_parameter_updates_concat</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">save_tensorboard</span><span class="p">:</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;total_alignment/lu_angle&#39;</span><span class="p">,</span>
                              <span class="n">scalar_value</span><span class="o">=</span><span class="n">total_angle</span><span class="p">,</span>
                              <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">save_dataframe</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lu_angles_network</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">step</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_angle</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="c1">#/self.depth</span></div>

<div class="viewcode-block" id="DFCNetwork.save_ratio_ff_fb"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.save_ratio_ff_fb">[docs]</a>    <span class="k">def</span> <span class="nf">save_ratio_ff_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">writer</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span>
                               <span class="n">save_tensorboard</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_dataframe</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Save the ratio of the current feedforward and feedback stimulus.</span>

<span class="sd">        Save the angle in the tensorboard writer (if ``save_tensorboard=True``)</span>
<span class="sd">        and in the corresponding dataframe (if ``save_dataframe=True``).</span>

<span class="sd">        Args:</span>
<span class="sd">            (....): See docstring of method :meth:`save_ndi_angles`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_ratio_ff_fb</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">u</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">save_tensorboard</span><span class="p">:</span>
                <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;layer </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">/ratio_ff_fb&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
                                  <span class="n">scalar_value</span><span class="o">=</span><span class="n">ratio</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                  <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">save_dataframe</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ratio_angle_ff_fb</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">step</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ratio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Compute the total ratio across all layers.</span>
        <span class="n">total_ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ratio</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">save_tensorboard</span><span class="p">:</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;total_ratio_ff_fb&#39;</span><span class="p">,</span>
                              <span class="n">scalar_value</span><span class="o">=</span><span class="n">total_ratio</span><span class="p">,</span>
                              <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">save_dataframe</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ratio_angle_ff_fb_network</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">step</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_ratio</span><span class="o">.</span><span class="n">item</span><span class="p">()</span></div>


<div class="viewcode-block" id="DFCNetwork.save_feedback_batch_logs"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.save_feedback_batch_logs">[docs]</a>    <span class="k">def</span> <span class="nf">save_feedback_batch_logs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">writer</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">save_tensorboard</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_dataframe</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">save_statistics</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Save the logs for the current minibatch on tensorboardX.</span>

<span class="sd">        Args:</span>

<span class="sd">        Save the angle in the tensorboard writer (if ``save_tensorboard=True``)</span>
<span class="sd">        and in the corresponding dataframe (if ``save_dataframe=True``).</span>

<span class="sd">        Args:</span>
<span class="sd">            (....): See docstring of method :meth:`save_ndi_angles`</span>
<span class="sd">            init (bool): Flag indicating that the training is in the</span>
<span class="sd">                initialization phase (only training the feedback weights).</span>
<span class="sd">            save_statistics: Flag indicating whether the statistics of the</span>
<span class="sd">                feedback weights should be saved (e.g. gradient norms).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">save_statistics</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
                <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;layer_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">save_feedback_batch_logs</span><span class="p">(</span><span class="n">writer</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                                     <span class="n">no_gradient</span><span class="o">=</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">pretraining</span><span class="o">=</span><span class="n">init</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">save_condition_fb</span><span class="p">:</span>
            <span class="n">condition_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_condition_two</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">save_tensorboard</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">init</span><span class="p">:</span>
                    <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;feedback_training/condition_2_init&#39;</span><span class="p">,</span>
                                      <span class="n">scalar_value</span><span class="o">=</span><span class="n">condition_2</span><span class="p">,</span>
                                      <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tag</span><span class="o">=</span><span class="s1">&#39;feedback_training/condition_2&#39;</span><span class="p">,</span>
                                      <span class="n">scalar_value</span><span class="o">=</span><span class="n">condition_2</span><span class="p">,</span>
                                      <span class="n">global_step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">save_dataframe</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">init</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">condition_gn_init</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">step</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">condition_2</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">condition_gn</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">step</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">condition_2</span><span class="o">.</span><span class="n">item</span><span class="p">()</span></div>

<div class="viewcode-block" id="DFCNetwork.compute_bp_angles"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.compute_bp_angles">[docs]</a>    <span class="k">def</span> <span class="nf">compute_bp_angles</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the angles of the current forward parameter updates of layer</span>
<span class="sd">        `i` with the backprop update for those parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            loss (float): Output loss of the network.</span>
<span class="sd">            i (int): Layer index.</span>
<span class="sd">            retain_graph (bool): flag indicating whether the graph of the</span>
<span class="sd">                network should be retained after computing the gradients or</span>
<span class="sd">                jacobians. If the graph will not be used anymore for the current</span>
<span class="sd">                minibatch afterwards, retain_graph should be False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (....): Tuple containing:</span>

<span class="sd">            - **weights_angle**: The angle in degrees between the updates for</span>
<span class="sd">              the forward weights.</span>
<span class="sd">            - **bias_angle**: (Optionally) the angle in degrees for the bias.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bp_gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">compute_bp_update</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">)</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_forward_gradients</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">mutils</span><span class="o">.</span><span class="n">contains_nan</span><span class="p">(</span><span class="n">bp_gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;Backprop update contains NaN (layer </span><span class="si">{}</span><span class="s1">).&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">mutils</span><span class="o">.</span><span class="n">contains_nan</span><span class="p">(</span><span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;Weight update contains NaN (layer </span><span class="si">{}</span><span class="s1">).&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-14</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Norm updates approximately zero (layer </span><span class="si">{}</span><span class="s1">).&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Norm updates exactly zero (layer </span><span class="si">{}</span><span class="s1">).&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

        <span class="n">weights_angle</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">compute_angle</span><span class="p">(</span><span class="n">bp_gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span>
                                            <span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="n">bias_angle</span> <span class="o">=</span> <span class="n">mutils</span><span class="o">.</span><span class="n">compute_angle</span><span class="p">(</span><span class="n">bp_gradients</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span>
                                              <span class="n">gradients</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">weights_angle</span><span class="p">,</span> <span class="n">bias_angle</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">weights_angle</span><span class="p">,</span> <span class="p">)</span></div>


<div class="viewcode-block" id="DFCNetwork.compute_H_update"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.compute_H_update">[docs]</a>    <span class="k">def</span> <span class="nf">compute_H_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the negative weight updates from the :math:`\mathcal{H}`</span>
<span class="sd">        loss.</span>

<span class="sd">        This computes the gradients w.r.t. the post-nonlinearity activations</span>
<span class="sd">        using the full Jacobian of the network:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \frac{1}{2} \frac{d \lVert Q\mathbf{u}_{ss} \rVert^2_2}{d \</span>
<span class="sd">                \bm{\theta}} = -\mathbf{u}_{ss}^{T}Q^{T}Q(J_{ss}Q + \</span>
<span class="sd">                \alpha I)^{-1}J_{ss}R_{ss}^{T}</span>

<span class="sd">        Args:</span>
<span class="sd">            (....): See docstring of method :meth:`compute_bp_update`.</span>
<span class="sd">            u (torch.Tensor): The controller signal.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (....): Tuple containing:</span>

<span class="sd">            - **lu_updates_W** (list): The weight updates for each layer</span>
<span class="sd">              according to loss :math:`\mathcal{H}`.</span>
<span class="sd">            - **lu_updates_b** (list): The bias updates for each layer</span>
<span class="sd">              according to loss :math:`\mathcal{H}`. ``None`` if no biases</span>
<span class="sd">              exist.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">device</span>
        <span class="n">J_ss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_full_jacobian</span><span class="p">(</span><span class="n">noisy_dynamics</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">J_ss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_Q</span>
        <span class="n">u_ss</span> <span class="o">=</span> <span class="o">-</span><span class="n">u</span>
      
        <span class="c1"># Create empty variable to save the lu_updates</span>
        <span class="n">lu_updates_b</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">lu_updates_W</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>\
                        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_forward_gradients</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> 
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">)]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
            <span class="n">lu_updates_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>\
                            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_forward_gradients</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span>\
                            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">)]</span>

        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>

            <span class="n">aux_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">u_ss</span><span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="n">Q</span><span class="o">.</span><span class="n">t</span><span class="p">()),</span> <span class="n">Q</span><span class="p">)</span>
            <span class="n">aux_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">J_ss</span><span class="p">[</span><span class="n">b</span><span class="p">],</span><span class="n">Q</span><span class="p">)</span> <span class="o">+</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_di</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">J_ss</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>\
                        <span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
            <span class="n">aux_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">aux_0</span><span class="p">,</span><span class="n">aux_1</span><span class="p">),</span> <span class="n">J_ss</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>

            <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">aux_3</span> <span class="o">=</span> <span class="n">aux_2</span><span class="p">[</span><span class="mi">0</span> <span class="p">:</span> <span class="n">n</span><span class="p">]</span>
            <span class="n">aux_4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>
            <span class="n">lu_updates_W</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">mutils</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">aux_3</span><span class="p">,</span> <span class="n">aux_4</span><span class="p">)</span>
            <span class="n">lu_updates_b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">aux_3</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">n_new</span> <span class="o">=</span> <span class="n">n</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">aux_3</span> <span class="o">=</span> <span class="n">aux_2</span><span class="p">[</span><span class="n">n</span><span class="p">:</span><span class="n">n_new</span><span class="p">]</span>
                <span class="n">aux_4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>
                <span class="n">lu_updates_W</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">mutils</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">aux_3</span><span class="p">,</span> <span class="n">aux_4</span><span class="p">)</span>
                <span class="n">lu_updates_b</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">aux_3</span>
                <span class="n">n</span> <span class="o">=</span> <span class="n">n_new</span>
        
        <span class="k">return</span> <span class="n">lu_updates_W</span><span class="p">,</span> <span class="n">lu_updates_b</span></div>
    
<div class="viewcode-block" id="DFCNetwork.compute_ratio_ff_fb"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.compute_ratio_ff_fb">[docs]</a>    <span class="k">def</span> <span class="nf">compute_ratio_ff_fb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the ratio of the current feedforward and feedback stimulus.</span>

<span class="sd">        It is computed as:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \frac{||Q_{i}\mathbf{u}(t)||}{||W_{i}\mathbf{r}_{i-1}(t)||}</span>

<span class="sd">        Args:</span>
<span class="sd">            loss (torch.Tensor): Output loss of the network.</span>
<span class="sd">            u (torch.Tensor): Controller signal.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (list): A list with the ratios for each layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">device</span>
        <span class="n">u_ss</span> <span class="o">=</span> <span class="n">u</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_forward_parameter_list</span><span class="p">(</span><span class="n">with_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        <span class="c1"># empty variable to save the ratios per hidden layer</span>
        <span class="n">ratio_ff_fb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">apical</span> <span class="o">=</span> <span class="n">u_ss</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>
            <span class="n">aux_0</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">aux_1</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">Q_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weights_backward</span>
            <span class="n">W_0</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">basal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>
            <span class="n">aux_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">apical</span><span class="p">,</span> <span class="n">Q_0</span><span class="o">.</span><span class="n">t</span><span class="p">()),</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
            <span class="n">aux_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">basal</span><span class="p">,</span> <span class="n">W_0</span><span class="o">.</span><span class="n">t</span><span class="p">()),</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
            <span class="n">ratio_ff_fb</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">aux_0</span><span class="o">/</span><span class="n">aux_1</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>

                <span class="n">Q_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights_backward</span>
                <span class="n">W_i</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">basal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">activations</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>
                <span class="n">aux_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">apical</span><span class="p">,</span> <span class="n">Q_i</span><span class="o">.</span><span class="n">t</span><span class="p">()),</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
                <span class="n">aux_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">basal</span><span class="p">,</span> <span class="n">W_i</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
                <span class="n">ratio_ff_fb</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">aux_0</span><span class="o">/</span><span class="n">aux_1</span>

        <span class="k">return</span> <span class="n">ratio_ff_fb</span></div>

<div class="viewcode-block" id="DFCNetwork.compute_condition_two"><a class="viewcode-back" href="../../networks.html#networks.dfc_network.DFCNetwork.compute_condition_two">[docs]</a>    <span class="k">def</span> <span class="nf">compute_condition_two</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the Gauss-Newton condition on the feedback weights.</span>

<span class="sd">        .. math::</span>

<span class="sd">            \frac{\|\tilde{J}_2\|_F}{\|\tilde{J}\|_F}</span>

<span class="sd">        to keep track whether condition 2 is (approximately) satisfied.</span>
<span class="sd">        If the minibatch size is bigger than 1, the mean over the minibatch</span>
<span class="sd">        is returned.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The condition value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">jacobians</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_full_jacobian</span><span class="p">(</span>\
                                            <span class="n">noisy_dynamics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noisy_dynamics</span><span class="p">)</span>
        
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_Q</span>
        <span class="n">projected_Q_fro</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">jacobians</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">jac</span> <span class="o">=</span> <span class="n">jacobians</span><span class="p">[</span><span class="n">b</span><span class="p">,:,:]</span>
            <span class="n">projection_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">jac</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">jac</span><span class="p">,</span> <span class="n">jac</span><span class="o">.</span><span class="n">T</span><span class="p">)),</span> <span class="n">jac</span><span class="p">))</span>

            <span class="n">projected_Q_fro</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span>\
                    <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">projection_matrix</span><span class="p">,</span> <span class="n">Q</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">))</span>

        <span class="n">projected_Q_fro</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">projected_Q_fro</span><span class="p">)</span>
        <span class="n">Q_fro</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
        <span class="n">condition_two_ratio</span> <span class="o">=</span> <span class="n">projected_Q_fro</span><span class="o">/</span><span class="n">Q_fro</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">condition_two_ratio</span><span class="p">)</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">dfc</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents of the repository:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main.html">Main script to run experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main.html#reproducibility">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../datahandlers.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../networks.html">Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../utils.html">Utilities</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Alexander Meulemans, Matilde Tristany Farinha, Maria R. Cervera.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>